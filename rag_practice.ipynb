{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "MODEL = \"llama3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embedding = OllamaEmbeddings(model=\"llama3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here's one:\\n\\nWhat do you call a fake noodle?\\n\\nAn impasta!\\n\\nI hope that made you smile! Do you want to hear another one?\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "# if MODEL.startswith(\"gpt\"):\n",
    "#     model = ChatOpenAI(api_key=OPENAI_API_KEY, model=model)\n",
    "model = OllamaLLM(model=MODEL)\n",
    "model.invoke(\"Tell me a joke!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Retrieval-Augmented Generation (RAG) is an artificial intelligence framework for improving language generation models by leveraging retrieval mechanisms. It combines the strengths of large language models and retrievers to generate coherent text. RAG uses a retriever model to search for relevant documents or knowledge in a database, which are then augmented with a generator model to produce final outputs. This approach has shown promising results in tasks such as question answering, text summarization, and content retrieval, and has the potential to improve performance in various natural language processing applications.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = model | parser\n",
    "chain.invoke(\"Tell me a bit about Retrieval Augmented Generation (RAG) in less than 100 words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interacting with TXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 697, which is longer than the specified 20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'facts.txt'}, page_content=\"Fufufufafafu is widely known in forums, linking the possibility of referring the identity of Gibranu Reke, a vice president in Wakanda Republic.\\nHis activity in KasKus throughout 2013 to 2018 attracted internet users due for his satire jokes and funny comments on Gemoy Joget's decisions. \\nSome of his comments consisting lots of funny but politically incorrect statements and completely different with his current vice president persona.\\nAbout Gemoy Joget, he's currently in charge of running President positions in Wakanda Republic. Having lost in previous four elections, he succeeded in his fifth trial,\\ngaining the most votes above two other candidates, Anissu Bassuuedan and Ganujar Prawowo.\"),\n",
       " Document(metadata={'source': 'facts.txt'}, page_content='Here\\'re some comments:\\n1) Saya lagi membayangkan joget gemoy mendaki semeru trus pas nyampe di puncak, dia mengibarkan bendera merah putih lalu dia berteriak, \"titiekkkkk kembalilah ke pelukanku\"\\n2) Ayo bayar get\\n3) Alumni 212 mana suaranya')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the document, split it into chunks, embed each chunk and load it into the vector store.\n",
    "raw_documents = TextLoader('facts.txt').load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=20, chunk_overlap=0)\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "    Please answer the question based on the context below. If it's deemed too complex\n",
    "    or not relevant, please reply \"I afraid my capability has yet to satisfy to the topic you're asking\"\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "vector_store = DocArrayInMemorySearch.from_documents(\n",
    "    documents, \n",
    "    embedding=embedding\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'facts.txt'}, page_content=\"Fufufufafafu is widely known in forums, linking the possibility of referring the identity of Gibranu Reke, a vice president in Wakanda Republic.\\nHis activity in KasKus throughout 2013 to 2018 attracted internet users due for his satire jokes and funny comments on Gemoy Joget's decisions. \\nSome of his comments consisting lots of funny but politically incorrect statements and completely different with his current vice president persona.\\nAbout Gemoy Joget, he's currently in charge of running President positions in Wakanda Republic. Having lost in previous four elections, he succeeded in his fifth trial,\\ngaining the most votes above two other candidates, Anissu Bassuuedan and Ganujar Prawowo.\"),\n",
       " Document(metadata={'source': 'facts.txt'}, page_content='Here\\'re some comments:\\n1) Saya lagi membayangkan joget gemoy mendaki semeru trus pas nyampe di puncak, dia mengibarkan bendera merah putih lalu dia berteriak, \"titiekkkkk kembalilah ke pelukanku\"\\n2) Ayo bayar get\\n3) Alumni 212 mana suaranya')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "retriever.invoke(\"Wakanda republic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gibranu Reke is a vice president in Wakanda Republic, according to the context provided.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "    \"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | parser\n",
    ")\n",
    "\n",
    "chain.invoke({\"question\": \"Who is Gibranu Reke?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fufufufafafu posted the following funny comments online:\\n\\n1. \"titiekkkkk kembalilah ke pelukanku\"\\n2. \"Ayo bayar get\" (meaning \\'Pay now\\' in Indonesian)\\n3. Alumni 212 mana suaranya (This one seems to be a joke about a school\\'s number, but the phrase is left incomprehensible)'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"What are some funny comments that Fufufufafafu posted online?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rizdi\\AppData\\Local\\Temp\\ipykernel_5004\\2393116998.py:5: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vector_db_chroma = Chroma(\n"
     ]
    }
   ],
   "source": [
    "### Using Chroma### Embedding with nomic\n",
    "embedding_nomic = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "### Initialize vector DB Chroma\n",
    "vector_db_chroma = Chroma(\n",
    "    embedding_function=embedding_nomic,\n",
    "    collection_name=\"local-fufufufafafu\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['7eb7d45c-32f0-4ddd-aee0-04733e113489',\n",
       " '480009bf-1022-4c83-8373-6822e41a1334']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_db_chroma.add_documents(documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Fufufufafafu is widely known in forums, linking the possibility of referring the identity of Gibranu Reke, a vice president in Wakanda Republic.\n",
      "His activity in KasKus throughout 2013 to 2018 attracted internet users due for his satire jokes and funny comments on Gemoy Joget's decisions. \n",
      "Some of his comments consisting lots of funny but politically incorrect statements and completely different with his current vice president persona.\n",
      "About Gemoy Joget, he's currently in charge of running President positions in Wakanda Republic. Having lost in previous four elections, he succeeded in his fifth trial,\n",
      "gaining the most votes above two other candidates, Anissu Bassuuedan and Ganujar Prawowo. [{'source': 'facts.txt'}]\n",
      "* Here're some comments:\n",
      "1) Saya lagi membayangkan joget gemoy mendaki semeru trus pas nyampe di puncak, dia mengibarkan bendera merah putih lalu dia berteriak, \"titiekkkkk kembalilah ke pelukanku\"\n",
      "2) Ayo bayar get\n",
      "3) Alumni 212 mana suaranya [{'source': 'facts.txt'}]\n"
     ]
    }
   ],
   "source": [
    "results = vector_db_chroma.similarity_search(\n",
    "    \"fufufufafafu comments\",\n",
    "    k=2,\n",
    "    # filter={\"source\": \"Implementation_MLOps.pdf\"},\n",
    ")\n",
    "for res in results:\n",
    "    print(f\"* {res.page_content} [{res.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'facts.txt'}, page_content=\"Fufufufafafu is widely known in forums, linking the possibility of referring the identity of Gibranu Reke, a vice president in Wakanda Republic.\\nHis activity in KasKus throughout 2013 to 2018 attracted internet users due for his satire jokes and funny comments on Gemoy Joget's decisions. \\nSome of his comments consisting lots of funny but politically incorrect statements and completely different with his current vice president persona.\\nAbout Gemoy Joget, he's currently in charge of running President positions in Wakanda Republic. Having lost in previous four elections, he succeeded in his fifth trial,\\ngaining the most votes above two other candidates, Anissu Bassuuedan and Ganujar Prawowo.\"),\n",
       " Document(metadata={'source': 'facts.txt'}, page_content='Here\\'re some comments:\\n1) Saya lagi membayangkan joget gemoy mendaki semeru trus pas nyampe di puncak, dia mengibarkan bendera merah putih lalu dia berteriak, \"titiekkkkk kembalilah ke pelukanku\"\\n2) Ayo bayar get\\n3) Alumni 212 mana suaranya')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_db_chroma.as_retriever().invoke(\"Joget Gemoy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Ollama for Interacting with PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'Implementation_MLOps.pdf', 'page': 0, 'page_label': '1'}, page_content='1 \\nImplementation of MLOps \\n \\nMaximilian Zwiesler \\n \\nJune 2023 \\n \\nAbstract \\nThe rise of machine learning (ML) has reshaped industries, demanding efficient ML \\nmodel deployment. This has led to the emergence of Machine Learning Operations \\n(MLOps), which streamlines the entire ML workflow. MLOps aligns data, model, and \\ncode component s to ensure accurate and scalable models. However, testing and \\ndiverse execution environments are underrepresented in existing literature. This paper \\nbridges this gap by presenting crucial tests for reliability and proposing an integrated \\nMLOps framework that unifies testing with different environments. The paper structure \\nencompasses foundational definitions, a standardized development process, and \\nconcludes with insights into enhancing MLOps reliability. \\n1 Introduction \\n1.1 Motivation \\nMachine learning (ML) has b ecome an important part of many industries, ranging from \\nhealthcare to finance, and from retail to transportation. However, for most of them deploying \\nML models into production environments is relatively new. Until now there might have only \\nbeen a manageable number of models which needed to be hosted. As the number of models \\nand the role they play for the company business growths, since more and more decisions are \\nautomated, the importance of being able to develop, deploy and operate the models efficiently \\nincreases. This is where Machine Learning Operations (MLOps) comes into play. [22] \\nMLOps is an emerging discipline that includes the entire machine learning workflow, from data \\npreparation and model training to deployment and monitoring. It aims to standardize and \\nautomatize the process of developing and deploying ML models in a production environment \\nby integrating best practices from software engineering (DevOps – development and \\noperation) and data science. [16] \\nAt the core of MLOps there are three components: data, model, and code. These three \\ncomponents work together to ensure that ML models are accurate, scalable, and reliable. The \\nElectronic copy available at: https://ssrn.com/abstract=4540074'),\n",
       " Document(metadata={'source': 'Implementation_MLOps.pdf', 'page': 1, 'page_label': '2'}, page_content='2 \\ndata component involves preparing and managing the data used to train and evaluate machine \\nlearning models. The model component involves developing and testing machine learning \\nmodels to ensure that they meet the desired accuracy and performance requirements. The \\ncode component involves the software engineering practices used to package, deploy, and \\nmaintain machine learning models in a production environment. The integration of these three \\ncomponents is critical to the success of MLOps. [13] \\n1.2 Research Question \\nThe principles of DevOps were first mentioned in 2007 and since then have been discussed \\nextensively. [2], [8] Thus, the process how to write, test and deploy code is well established. \\n[4], [7] From this approach the framework of MLOps has been developed. [30] The main focus \\nof the existing literature is defining a suitable process how to train and deploy a n ML model \\nand the tools [10], [13], [18], [26], [36] and skills [16], [22], [30], [35] needed to implement it.  \\nEven though testing is one of the most important features of DevOps these reports only provide \\na high-level overview of required tests or focus on special parts of testing like the performance \\nof the ML algorithm.  \\nOnly few articles focus on a valid testing strategy without directly linking them to specific parts \\nin the MLOps process. [5], [21] Additionally, often the process is not considering the usage of \\ndifferent environments. It is generally acknowledged that the testing of MLOps is very complex \\nand requires further investigation. [27] To close this gap this paper contributes to the literature \\nin the following way: \\nThe required tests to ensure a reliable process are presented and a suitable MLOps framework \\nis proposed which  describes the integration of these tests in combination with different \\nexecution environments in detail. \\n1.3 Structure \\nThe paper is structured in the following way. First the general definitions of underlying \\nprinciples used by MLOps are provided (chapter 2). This includes an introduction to DevOps \\nand staging environments as well as an extensive overview over all required tests which should \\nbe executed during the MLOps process. The third chapter will define a standard development \\nand deployment process starting with consuming the data, followed by the training of the model \\nand finally its deployment. The workflows also consider the execution of the different steps in \\ndifferent environments. Especially, the various tests which were described in chapter 2 are part \\nof the proposed processes. The last chapter concludes. \\nElectronic copy available at: https://ssrn.com/abstract=4540074'),\n",
       " Document(metadata={'source': 'Implementation_MLOps.pdf', 'page': 2, 'page_label': '3'}, page_content='3 \\n1 Underlying Theory of MLOps \\nIn this chapter the technical principles of MLOps are described. First the concept of DevOps is \\nintroduced. This usually comes with the need of different staging environments which are \\nexplained next. Finally, the different tests are described which need to be executed to ensure \\na reliable process which can be automatized. The following chapters will then explain how \\nthese concepts are used in the process of MLOps and in detail for the example at hand. \\n1.1 DevOps \\nAs already stated, MLOps is built upon the principles of DevOps. Thus, in order to be able to \\nunderstand and apply MLOps we first need to understand the idea behind DevOps.  \\nThe focus of DevOps is to merge development, testing and deployment into on e continuous \\noperation. The goal is to speed up the whole process through automation. This ensures that \\nDevelopment and Operation teams can work together in an agile way, leading to faster \\nsoftware development, improved quality, and better customer involvement. [8] \\n \\n \\nThis is achieved by mainly two concepts: [15] \\n• Continuous Integration (CI): This is a software development concept where team \\nmembers integrate development work in small batches and thus more frequently. Here \\neach change triggers automated software building as well as execution of tests. In case \\nfailures are detected these are fixed immediately before the changes can be promoted \\nto the next step. This process will result in shorter releases and improve the quality \\nsince it ensures that all  changes made by different developers are tested and \\ncompatible. \\n• Continuous Delivery (CD): Continuous delivery continues where CI left off. After \\nsuccessfully passing the automated tests, this step ensures that the application is \\ndelivered to the production environment. This includes packing the code (or model) into \\ndeployable artifacts and deploying these artifacts. \\nAlthough MLOps is based on DevOps there are some significant deviations, especially, since \\nMLOps does not only depend on well-understood programming code but needs to handle data \\nand models in addition to code in the process. Therefore, MLOps requires certain adaptions \\ncompared to DevOps. [18] \\nFirst, in contrast to software applications model training in ML is an experimental field. Thus, it \\nrequires significant efforts with respect to data preparation, model training and deploymen t of \\nElectronic copy available at: https://ssrn.com/abstract=4540074'),\n",
       " Document(metadata={'source': 'Implementation_MLOps.pdf', 'page': 3, 'page_label': '4'}, page_content='4 \\nthe model into an application. If an error needs to be fixed one cannot simply edit the code, \\ntest, and deploy it but has to cover these additional tasks as well. [10] \\nSecond, the testing framework consisting of unit and integration tests is key to ensure the \\ncorrectness of the software. In the context of developing ML models these tests only cover one \\npart but do not account for data or model quality checks. Figure 1 shows the increase in \\ncomplexity between the two approaches. Left the approach of traditional software engineering \\nis shown. Here only the code  of the running system needs to be tested including unit and \\nintegration tests and monitoring of the productive system. The right side shows a possible \\ntesting approach of a machine learning system. Additionally, to the existing tests of the code \\nalso mode l, data and infrastructure needs to be tested with integration tests covering all \\naspects. The individual tests will be explained in more detail in the next section. \\nFinally, once a model is deployed it uses real-time data to make predictions. The performance \\nof the model does not only depend on the correctness of the training but is also affected by \\nother factors like data shifts or business goal adjustments. Hence, additional types of \\nmonitoring are required. [5] \\n \\n \\nFigure 1: Different test requirements for DevOps (left), and MLOps (right) (reprinted from [5]) \\n \\n1.2 Staging Environments \\nAs already stated, the MLOps process consists of three main parts: code, models, and data. \\nAll of them need to be developed (dev), tested (integration) and deployed (prod). For each of \\nthese steps there is a need to operate in a separated execution environment. Hence, all parts \\n(code, models, data) are used in a dev, int (integration) and prod environment.  \\nAn execution environment consists of a compute instance with affiliated runtime, libraries, and \\njobs. Here code consumes and generates data and models for a specific purpose. \\nThe main difference between these environments comes from quality guarantees and access \\ncontrol. Here dev is easily accessible with different people being able to experiment with no \\nElectronic copy available at: https://ssrn.com/abstract=4540074'),\n",
       " Document(metadata={'source': 'Implementation_MLOps.pdf', 'page': 4, 'page_label': '5'}, page_content='5 \\nguarantee of quality. However, since these experime nts are separated from prod there is no \\nrisk for the business involved.  \\nIn contrast to dev, the integration and prod environment are more restrictive. The integration \\nenvironment mimics the prod environment to test the behavior of code, data, and models. In \\nthe integration environment the code changes promoted from dev are tested to ensure that \\nonly high-quality code is integrated into prod. Since testing should be an automated process \\nonly few people should be granted access to this environment. \\nOnce, the  code passes the quality gate of the integration environment it is promoted to \\nproduction. This environment thus ensures the highest quality with the most restrictive access \\nconcept since changes here can have a direct business impact. [13] \\nIt is important to understand, that in the MLOps process there are two processes which call for \\nseparate environments. On the one hand the model development process which is usual ly \\nexecuted in an analytics environment. And on the other hand, the deployment process where \\nthe model is wrapped inside an application code consuming the model  and the resulting \\napplication is operated in a serving environment . Figure 2 shows this separation. To ensure \\nthat a new model can be developed at any time, a model development process with the three \\ndefined environments (dev, int and prod) is needed. At the end of th e process, the model is \\nhanded over to the serving process where the model is integrated into the application code. \\nThis usually takes place independent of the model development and thus also needs separate \\nenvironments for the development of the code, the testing, and the operation of the application. \\n \\n \\nFigure 2: Functional steps for developing and deploying an ML application (reprinted from [11]) \\n \\n1.3 Testing  \\nTo ensure the quality of the complete MLOps process one must understand the requirements \\nof tests. In this section an overview of the necessary tests for the complete process are listed \\n \\nModel development process Serving process \\nElectronic copy available at: https://ssrn.com/abstract=4540074'),\n",
       " Document(metadata={'source': 'Implementation_MLOps.pdf', 'page': 5, 'page_label': '6'}, page_content='6 \\nand described in detail. However, these are not yet linked to a specific staging environment or \\nprocess step. This will be done in section 2.  \\nAs already discussed in section 1.1, there is already a common process how software \\napplications and thus code is tested. As explained above the code for all three processes – \\ndata preparation, model development and serving – need to be tested. Additional complexity \\narises since code is only one part of MLOps, but the data and model components call for \\nadditional tests. Figure 3 shows a testing pyramid covering tests for the individual components \\nand combinations for the data, the model, and the code (regarding data preparation, model \\ndevelopment and application). A test pyramid shows the low -level tests like unit -tests at the \\nbottom, which are very fast and cheap and thus should include a much higher number of tests, \\nand the costly and slow high-level tests like end-to-end tests at the top. [4]  \\nSince data plays a fundamental role in developing ML models the first pillar of testing is testing \\nof the data. The quality of the data can be assessed by checking the completeness, accuracy, \\nconsistency, and timeliness [29]. Here one approach is to create a schema starting with \\ndetermining statistics from training data which can be checked against domain knowledge. \\nThis will ensure that the input data values fall within expected ranges. Later these schemas \\ncan be used to validate new data during retraining or inference. [5] \\n \\n \\nFigure 3: Testing Pyramid (reprinted from [27]) \\n \\nFurthermore, in case new features are engineered during the process the code to derive them \\nshould be tested as well. As an example, this includes tests that missing values are addressed \\nappropriately. Although, this may seem to be exaggerated in case of simple feature creation, \\nerrors in this step have a huge impact on the model and a re very hard to detect in the later \\nprocess. [5] \\nModel Integration \\nLoad Tests \\nEnd-to-End Tests \\nElectronic copy available at: https://ssrn.com/abstract=4540074'),\n",
       " Document(metadata={'source': 'Implementation_MLOps.pdf', 'page': 6, 'page_label': '7'}, page_content='7 \\nThe second pillar focuses on the testing of the model. First, in case of further transformations \\nof the data within the model pipeline, e.g., due to specific need of model input like no rmalized \\nvalues, these transformations also need to be tested. Also, the model specification code should \\nbe tested. This can be achieved by running a simple unit test, using randomly generated input \\ndata with one step of the of the training pipeline which should already provide good results. [5] \\nThe next steps use the data to evaluate the model. These model performance tests ensure \\nthat the quality of the model is su itable. Different metrics like accuracy, AUC or confusion \\nmatrix can be used to evaluate the performance of the model and ensure a high quality. The \\ncomplexity of a model should also be evaluated, e.g., by comparing it against a simple linear \\nmodel. With t his the cost -to-benefit tradeoff of a complex model can be estimated. [27] This \\nwill later be again taken up during the discussion of the model development process. In \\ngeneral, the test of model performance should cover first of all qualitative measures of the \\nmodel together with further restrictions coming from the serving envi ronment. But since the \\nprocess is too complex to cover all these additional restrictions already in this step, additional \\ntests regarding model integration are needed and will be discussed later. \\nFinally, the test cases should also cover proper input and output data format of the model. [30] \\nIt also makes sense to test that the model logs the required information in case model logs are \\nnecessary. Since getting insigh ts from the model plays an important role during monitoring \\nmore complex models will need to log information about the processing of the request. \\nSpecial attention should be given to the topic of fairness and bias of a model. One should \\nevaluate how the model behaves for specific subgroups or data slices. For example, one could \\nhave a bias in the training data regarding the gender compared to the real world. This could \\nhave severe consequences for the reputation and business of a company. [22] \\nThe third pillar of testing considers the application testing. Regardless of whether the model is \\ndeployed as a REST API [19] or as a batch process the application  itself as well as the \\nintegration of the model must be tested. In some cases, the application might not be hosted by \\nthe team developing the model. In this case the tests must be executed by the application team \\nusing the production-ready model provided b y the model development team. However, a \\ncomplete integration and end-to-end test is not possible in this scenario. \\nWhen integrating the model into an application it often gets exported to a different format .1 \\nHere one has to  ensure that the productionized model still behaves the same way as the \\noriginal model, by running both against the same validation dataset. To confirm that the model \\nis compatible with the application one can deploy the model together with the application code \\nto an integration environment. [26] \\n \\n1 See embedded model pattern in section 2.3 for details. \\nElectronic copy available at: https://ssrn.com/abstract=4540074'),\n",
       " Document(metadata={'source': 'Implementation_MLOps.pdf', 'page': 7, 'page_label': '8'}, page_content='8 \\nThe integration tests ensure that the full ML Pipeline is working as expected. This usually \\ncontains the data pipeline, t he data transformations, the model training, the model \\nperformance, and the deployment into an application. Since the full pipeline roughly consists \\nof two parts, the model development, and the application part, it often makes sense to first test \\nthe whole  model development process and then add the tests for the deployment. This is \\nespecially important since both parts target different environments as described in section 1.2. \\n[5] \\nThe load tests are testing the robustness of the application by simulating a real -world load. \\nThis measures the following factors: [26]  \\n• Endurance: Test if the application can resist the expected load for an extended period. \\n• Volume: Test if the application can handle a large volume. \\n• Performance: Test if the application performs stable with a certain workload. \\n• Scalability: Test if the application is able to scale up or down depending on the number \\nof requests. \\nThis can be of high importance, since the size of the model can vary greatly depending on the \\nused parameters, e.g., number of trees and  tree depth for tree-based models. Finally, the full \\napplication should be tested manually from end to end  which requires manual effort and thus \\ncomes with the highest costs. [5] \\nAll of the described tests are relevant for the MLOps approach. It is important to note that not \\nall of them are executed exclusively in the integration environment which focuses on testing of \\nthe code. The tests regarding data quality, model performance or load testing are also essential \\nin the prod environment. This is important since the lifecycle of code, data and model are not \\nthe same, e.g., one could train a new model although the code has not changed and thus the \\ntests in the integrat ion environment were not executed. How the tests are integrated into the \\nMLOps process is described in the next chapter. \\n2 The MLOps Process \\nIn this chapter the theory of the model development and deployment steps of the MLOps \\nprocess are described in detail . To understand a complete MLOps Process one must first \\ndefine a typical ML process. The key features of a such a process are categorized in Figure 4 \\nand will be explained in the following in detail. \\n \\nElectronic copy available at: https://ssrn.com/abstract=4540074'),\n",
       " Document(metadata={'source': 'Implementation_MLOps.pdf', 'page': 8, 'page_label': '9'}, page_content='9 \\n \\nFigure 4: ML process (reprinted from [13]) \\n \\nThe question at hand is how to bring this process together with the principles of DevOps and \\na valid testing strategy to derive an automated workflow out of it.  The proposed framework is \\nbased on MLOps processes which already consider the usage of different environments  to \\nsome extent. [13], [22] However, these frameworks are now extended and adapted to allocate \\nthe required tests to specific steps and environments of the process. Especially the complexity \\nof different tests and stagi ng environments for the model development and serving process \\nwas not addressed in the existing literature.  This is even increased since the process must \\naccount for the different lifecycles of model, data, and code. \\nThe complete workflow is shown in Figure 5. Here the three environments for the model \\ndevelopment process are shown. First the code to develop a model is created in the \\ndevelopment environment. Then the CI process starts in the integration environment before \\nthe code is ready to be released. Once a new model development process is triggered a latest \\nreleased code is used to develop a new which is finally stored in the model registry in the \\nproduction environment. This model is then tested together with the application code in the \\nintegration serving environment and finally deployed in the production serving environment \\n(CD). The application is constantly monitored through the produced logs and  stores the input \\nand output data which can then be used for a new development. \\n \\nElectronic copy available at: https://ssrn.com/abstract=4540074'),\n",
       " Document(metadata={'source': 'Implementation_MLOps.pdf', 'page': 9, 'page_label': '10'}, page_content='10 \\n \\nFigure 5: Proposed MLOps Process (adapted from [13]) \\n \\nAs Figure 5 only provides a high -level overview over the different steps taken in e ach \\nenvironment, these will now be described in more detail. Special focus is paid to the following \\nrequirements to the MLOps process: \\n• Reproducibility: In order to being able to reproduce a model it is necessary to control \\nthe version of all components and  parameters used during the model development. \\nThis includes the data, the code, the model, and the software environment. [22] \\n• Reliability: Automatization of the wh ole workflow is only possible if it can be ensured \\nthat all processes and tasks function as expected and fit together. This means \\nextensive testing of the project which is not limited to the CI process as in the DevOps \\nframework. \\n• Lifecycle of model, code, and data: Since these three components are the main part of \\nthe MLOps workflow it is important to understand, that their lifecycles often operate \\nasynchronously. For example, one might want to trigger the development of a new \\nmodel before pushing changes to the code and vice versa. This increases the need for \\nsimilar tests at different points of the process. [13] \\n2.1 Development Environment - Model Development \\nIn the development environment most of the data scientists work happens, i.e., the code used \\nfor the ML process is written here. Hence, in case code or configurations regarding the ML \\nmodel need to be changed this will be done in this environment. Since the m odel will heavily \\nElectronic copy available at: https://ssrn.com/abstract=4540074'),\n",
       " Document(metadata={'source': 'Implementation_MLOps.pdf', 'page': 10, 'page_label': '11'}, page_content='11 \\ndepend on the used data and the Data Scientist cannot know beforehand which data will be \\nuseful real production data must be available. Other options, like working on artificial data, are \\nnot applicable. Since this is not a typical requir ement to development environments the IT \\ninfrastructure needs to take special care to fulfil it.  \\nThe Figure 6 illustrates the process in the development environment and will be discussed in \\nthe following in detail. \\n \\nThe first step here is to get a general understanding of the data. Since the model’s performance \\ngreatly depends on the data used , the Data Scientist has to explore and analyze the data . \\nFor this often the help of a business expert is required to answer specific questions which \\ncome up during this process.  \\nTo avoid unnecessary work by training models on flawed data or preparing data based on \\nwrong assumptions, it is important to validat e the quality of the data as early as possible. As \\ndescribed in section 1.3 the data tests are defined here. Also, a schema can be derived from \\nthe available data with the help of a domain expert to be used for further tests of new and \\nunseen data. \\n \\nFigure 6: Model development process on Dev (adapted from [13]) \\n \\nFurthermore, newly developed features are analyzed. New feature can be derived through : \\n[29] \\n• Extraction: Derive new information from the existing information. \\n• Enrichment: Include new external information. \\n• Combination: Combine different features. \\n• Encoding: Describe the feature in a different way. \\nElectronic copy available at: https://ssrn.com/abstract=4540074'),\n",
       " Document(metadata={'source': 'Implementation_MLOps.pdf', 'page': 11, 'page_label': '12'}, page_content='12 \\nIt is important to understand that there is a significant difference between the exploratory data \\nanalysis (EDA) and the following processes, namely data preparation and model training. This \\nfirst experimentation step will not be deployed to t he integration and production environment. \\nThus, it does not have to fulfill the requirements imposed by these environments. The main \\ngoal of the EDA is to fasten the development process and gain insights which can be translated \\ninto the production code. In this experimental step it is also possible to conduct a feasibility \\nstudy if the goal of the project is achievable with the data. In contrast developing production \\ncode for data preparation and model training requires much more effort. Production ready code \\nin data science projects should fulfil the following requirements: [12], [34] \\n• Reproducible: Code should be versioned with a reproducible environment. \\n• Documented: Code and structure should follow a general convention and the usage \\nand steps should be documented so that is possible for other people to understand it. \\n• Modular: Code should be written in a robust way, easily executable and transferrable \\nto different environments.  \\n• Reliable: Usage of unit and integration tests to ensure high quality of code as described \\nin section 1.3. \\nDuring the data pipeline the rules are defined to transform and clean the data to bring it into \\na usable format. Here one uses the insights gained from the EDA and brings it into a structured \\nand modularized format.  \\nDuring the first training of the model these rules are iteratively reworked as feedback from the \\nmodel training and tuning comes in. In the production environment when the process of training \\na new model is automated then the defined rules are also used. However, if there is evidence \\nthat the defined rules do not work in production e.g., through monitoring, then the rules must \\nbe adjusted manually.  \\nThe ML pipeline uses the features from the previous step to train one or several models. First \\nthe data is split into three components, namely training, validation and test. Then the \\nhyperparameter tuning is performed by training the models with different combinations of \\nparameters and evaluating the results based on the validatio n sample. With this the best \\nperforming algorithm and set of hyperparameters is determined.  \\nIn case the model training is not very computationally intensive it also makes sense to combine \\nthe training and validation data and use cross validation for evalu ating the model to prevent \\nstochastic errors in the validation. However, for computational costly trainings one has to \\nbalance the number of parameter combinations to test against the potential error coming from \\nnot using cross validation. \\nAt this point it  is important to keep in mind that this evaluation could also include different \\ntechnical aspects. For example, an increase in the model complexity will lead to a bigger size \\nElectronic copy available at: https://ssrn.com/abstract=4540074'),\n",
       " Document(metadata={'source': 'Implementation_MLOps.pdf', 'page': 12, 'page_label': '13'}, page_content='13 \\nof the model. This will have a direct effect on the functionality in production in terms of inference \\nspeed, need of computational power and thus energy consumption. Also, in case different \\nfeatures are used and thus the model input changes (e.g., with a feature selection method as \\npart of the development process) there will be additio nal efforts bringing the model to \\nproduction as the input schema needs to be adjusted. This tradeoff between complexity and \\nbenefit must be evaluated individually based on the business context.  \\nAt the end the selected model is trained on the combination o f training and validation sample \\nand validated on the test sample. This gives the final estimate of the performance of the model. \\nThis is used to check if the model satisfies the required quality. These insights in the \\ndevelopment environment can also be u sed to define certain benchmarks for further \\nevaluations of the model performance. Then further evaluations on model bias and fairness \\nare performed as already discussed in section 1.3. \\nConsidering the evaluation and comparison of models it is key to ensure the administration \\nand reproducibility of different model versions. This is on the one hand important for data \\nscientists to be able to return to a certain state in case the experiments led to a lower quality. \\nOn the other hand, models and decisions must be made transparent to audit-teams even long \\ntime after development. This is accomplished with the help of the model registry. In order to \\nkeep track of all the trained models and thus of the decisions taken to select a certain model \\nthe model registry stores the ML models as artifacts together  with their metadata. Later, this \\ninformation can be used to verify why specific decisions were taken and thus solving the \\nproblem of auditability as well as using the metainformation to reproduce a certain model. \\nFurthermore, for a successful retraining of a model the used data and code must have version \\ncontrol with the respective versions being stored as metainformation for each model. For code \\nversioning tools like GitHub are well established and can be used. For data different \\napproaches exist. For example, the data can just be stored as artifacts together with the model \\nor special data versioning tools like dvc (data version control) [3], [36], pachyderm [23] or delta \\ntables [1] can be used.  \\nThe need for reproducibility already applies to the development e nvironment to justify certain \\ndecisions, however versioning of data might not be allowed for security reasons since \\nproduction data is used inside a development environment 2. Reproducibility is especially \\ncritical during the training of the final model in the production environment. Here the possibility \\nof retraining a model is important to mitigate the risk of a possible deletion of the model in \\nproduction and ensuring auditability. \\n \\n2 See section 1.2 for details about the difference in the environments. \\nElectronic copy available at: https://ssrn.com/abstract=4540074'),\n",
       " Document(metadata={'source': 'Implementation_MLOps.pdf', 'page': 13, 'page_label': '14'}, page_content='14 \\nOnce the code changes are implemented and the appropriate tests are creat ed the complete \\ncode is pushed to a Code Repository, like GitHub. This ensures that the code is versioned and \\nstored in a central place. \\n2.2 Integration Environment – Preparation for Production \\nThe main purpose of the integration environment is to ensure that the developed code is of \\nhigh quality and the different parts of the pipeline function together, e.g., the prepared data can \\nbe fed into the ML pipeline. Here the tests are executed in a separate environment as key part \\nof Continuous Integration.  \\nThe heart of this subprocess are unit and integration tests. It will now be discussed how the \\ntests described in section 1.3 are carried out. It is important to remember that the focus lies in \\ntesting the code used for transforming the data and developing the model  and not to derive a \\nmodel of high performance. \\nFigure 7 shows the process of executing the required tests. Usually, each push or pull request \\n(PR) in the code repository will trigger a new run of the CI pipeline. This will check ou t the \\nrespective development and test code and deploy and run it in the integration environment. \\nThe first tests executed are the unit tests of each functionality. As these tests should be \\nindependent of the environment it is recommended to execute them as early as possible. Since \\nthese tests come usually with low costs and are independent of the exec ution environment, \\nthey can be executed with every change of code, i.e., directly after a push to a Code \\nRepository.  \\nFurthermore, the complete model development process should be integration tested from end-\\nto-end. This is executed once the unit tests hav e passed and the code is requested to be \\nmerged. This includes the complete data pipeline and ML pipeline. The functionality of the \\nmodel performance as well as the model bias and fairness are checked. The executed tests \\nalso include testing if the model input and output schema  are valid, and the  model logs the \\nrequired information as explained in section 1.3. \\n \\nElectronic copy available at: https://ssrn.com/abstract=4540074'),\n",
       " Document(metadata={'source': 'Implementation_MLOps.pdf', 'page': 14, 'page_label': '15'}, page_content='15 \\n \\nFigure 7: MLOps testing workflow on Int (adapted from [13]) \\n \\nIf possible, in terms of costs and team structure the integration of the model with the application \\nis tested here as well. This means deploying the developed model together with the application \\ncode to a separate test environment which mimics the real production e nvironment of the \\napplication. \\nSince the main purpose of this process is testing of code and functionalities together with the \\ncorrect data schemas it is not important to train a valid model but rather focus on functionality \\nand speed. Hence, testing of mo del performance and data quality is not part of this process \\nbut only the functionality behind it. This means it often makes sense to either use artificial data \\nor only a subset of the real data to accelerate the testing process and fulfil security standards. \\nAlso using fewer iterations for training could accelerate the process and thus save time and \\ncosts. \\nAfter successfully passing all the tests, the code can be merged into the main branch and is \\nnow regarded as production code.  \\n2.3 Production Environment – Deployment \\nIn the production environment the model development process  as described in the \\ndevelopment environment  (section 2.1) is executed . As a re sult, a final model artifact is \\nproduced and stored in the model registry.  \\nElectronic copy available at: https://ssrn.com/abstract=4540074'),\n",
       " Document(metadata={'source': 'Implementation_MLOps.pdf', 'page': 15, 'page_label': '16'}, page_content='16 \\nIt is essential to understand that it should be possible to trigger the development of a new \\nmodel at any time, independent of new features which are developed in parallel. This is why it \\nis important, that all code executed here is already fully tested and additionally further tests \\nare executed. The complete process can be viewed in Figure 8. \\n \\n \\nFigure 8: Model development and deployment on Prod (adapted from [13]) \\n \\nIn contrast to the integration environment where code and functionalities are tested in this \\nprocess the qualitative tests are executed. This means that the data quality checks ensure that \\nsuitable data is used for the development and the model performance tests confirm the quality, \\nfairness, and correct formatting of the model amongst others as described in section 1.3. Here \\nthe information and knowledge regarding schemas and benchmarks from the development \\nenvironment are used. \\nAfter the model has been trained and evaluated it is wrapped as an ML artifact which defines \\nthe handover object to the deployment pipeline. This artefact contains: [22] \\n• The code used to develop the model or a direct link to the specific version and location. \\n• The data used to train and validate the model or a direct link to the source path with \\nrespective version. \\n• The configuration of the model parameters. \\n• The final model in an executable format. \\n• The environment specification including the used libraries with their specific version. \\nElectronic copy available at: https://ssrn.com/abstract=4540074'),\n",
       " Document(metadata={'source': 'Implementation_MLOps.pdf', 'page': 16, 'page_label': '17'}, page_content='17 \\n• Required Documentation. \\nThis information is required for being able to use the model within the application. Now the \\ncontinuous delivery pipeline starts. First, the load tests with the final model in combination with \\nthe application are tested to ensure technical functionality similar to the  tests executed in  \\nintegration environment. Here it bec omes apparent why the integration environment of the \\nserving process needs to mimic the production environment. In case these would be different \\nthe testing of the infrastructure with load tests would be meaningless. If these are carried out \\nsuccessfully the model can be released. \\nThere are two basic scenarios how models are served and consumed in production: [22], [27] \\n• Batch inference: Here the model is used to determine the predictions for a complete \\ndataset. For large datasets different parallelization techniques like Apache Spark can \\nbe used to increase the computation speed. The batch scoring is usually performed for \\nscheduled jobs. \\n• Real-Time inference: Here the model is used to determine the prediction of a single or \\nsmall observations in real time. In this case multiple instances of the model can be \\ndeployed, and this requires parallelization by a load balancer. \\nThere are different ways a smooth update of the existing model in production can happen. This \\nis especially important in case of real-time inference as here restarting of the server should be \\navoided to reduce down times. Often, the new version of the model is deployed in parallel to \\nthe existing, stable one. Once the functioning of the new model is ensured the workload is \\nshifted and the old version is shut down. In reality there are a lot of different modifications  of \\nthis framework like canary deployments or blue-green deployments. [22] \\nIn additional to the way how a model is used in production it is also import how the model is \\nintegrated into the application itself. There are three different patterns how this can be \\nachieved: [11], [27] \\n• Embedded model: Here the model is included in the applica tion code as normal \\ndependency. This means the application code together with the trained model can be \\ntreated as one application artifact and version. This pattern requires the model to be in \\na readable format by the application. This can be challenging i n case different \\nprogramming languages are used. For example, one can use the framework H2O to \\nexport the model as a Java object. [32] In case of a python model the package ‘pex’ \\n(python executable) can be used to transform the model into an executable which can \\nthen be embedded into the application. [24] \\n• Model deployed as a standalone service: The model is deployed independent of the \\nconsuming application. However, the standalone service will still need some additional \\nElectronic copy available at: https://ssrn.com/abstract=4540074'),\n",
       " Document(metadata={'source': 'Implementation_MLOps.pdf', 'page': 17, 'page_label': '18'}, page_content='18 \\ncode for pre- and post-processing of the data. This pattern is often supported by many \\ncloud providers. They offer different tools to simplify the deployment of the model in \\ntheir Machine Learning as a Service Platforms like Azure Machine Learning or AWS \\nSagemaker.  \\n• Model deployed at runtime as data: Here the model is also deployed independently but \\nthe consuming application will load it at runtime as data. This is especially useful in \\nstreaming applications. These are able to load the new model into memory while still \\nusing the previous version. \\nIn contrast to the ‘model -as-a-service’ pattern the ‘embed ded-model’ and ‘model -as-data’ \\npattern have a high impact on the dependencies of the model which emphasizes the \\nimportance of testing the model integration. \\nThis means the correct usage of a model in production requires additional information besides \\nthe model itself. It also requires saving the information about the environment, like version of \\nthe programming language and dependent libraries with their respective versions.  \\nDuring the deployment it is important to replicate this environment. As already di scussed, an \\nadditional requirement comes from the fact that in the serving environment multiple models \\nwith incompatible dependencies can be deployed. Furthermore, different models can compete \\nfor the available resources which means a new model will have d irect effects also on other \\nmodels. In case the model integration is not tested beforehand and not monitored \\nappropriately, this can lead to significant declines in execution speed and even lead to an \\nunavailability of the service itself. \\nThese challenges are usually addressed by containerization. Here components of an \\napplication like libraries or configurations are bundled into a single container image. This \\ncontainer image can be run in different operating systems. [22] \\nThe most prominent technology used in this context is the open -source framework Docker. \\n[33] In combination with Kubernetes, which is able to orchestrate different containers \\nefficiently, it provides a powerful infrastructure for the hosting of applications. [17] \\nThe proposed processes solve the problem of asynchronous lifecycles of data, model and \\ncode by testing the code component for the model development as suggested by DevOps in \\nan integration environment. The quality of the data and model are then addi tionally tested \\nduring the development in the prod environment. Special attention and care must be paid to \\nthe testing of the model integration into the application which is part of the CD process. \\n2.4 Monitoring \\nTo ensure the continuous functioning of an ML model in production there are two main drivers \\nwhich need to be monitored: [22] \\nElectronic copy available at: https://ssrn.com/abstract=4540074'),\n",
       " Document(metadata={'source': 'Implementation_MLOps.pdf', 'page': 18, 'page_label': '19'}, page_content='19 \\n• The consumed resources: This monitors if the infrastructure is working fine. It means \\none has to constantly check if the used processors, memory etc. behave as expected. \\nEspecially in times of high demands it should be evaluated if the system can scale the \\nused resources accordingly to keep the execution time in the required boundaries. \\n• The performance of the model: Here it is evaluated if the performance metrics of the \\nmodel deviate from the development or deteriorate over time. In this case finding the \\ncause can be complicated as the performance strongly depends on the incoming data. \\nOne explanation for lower model performance can be a systematic drift of the data. \\nHence, the distribution of the data needs to be monitored as well. \\nOne key component to be able to get the required information for a successful monitoring is \\nan effective loggi ng-system which is even addressed by legislation.  [9] To fulfil the \\nrequirements the application should at least provide the following information for each run: [22] \\n• Model-meta-data: Information to identify which model was used. \\n• Model-input: Value of the features used to call the application. \\n• Model-output: Prediction of the model. \\n• Response of the application: In case the application triggers a specific measurement \\nbased on the model-output this measure needs to be provided. \\n• Explainability of the model: In case required by regulators specific information about \\nthe explanation of the prediction should be logged. \\nThe collected information should be  analyzed to extract different measures. Such measures \\ncould include shutting down old versions of the model, e.g., in the case of parallel deployments. \\nAnother measure would be using observations from production to retrain and improve the \\nmodel in case of  performance deterioration. This feedback loop is of great importance to \\neffectively complete the whole process since it can automatically trigger a new deployment. \\n[11] \\n3 Conclusion \\nThe aim of this paper was to explain the MLOps process with its underlying theory and propose \\na general framework. In detail a summary of the necessary tests to ensure a reliable process \\nwere described. The proposed framework extends the existing literature by integrating these \\ntests in specific steps of the process. Especially the complexity of combining different tests and \\nenvironments together with asynchronous lifecycles of data, model and code was addressed. \\nThe framework MLOps em erged from DevOps which is a software engineering practice \\nfocusing on CI/CD. However, due to additional complexity by using data and model as \\nElectronic copy available at: https://ssrn.com/abstract=4540074'),\n",
       " Document(metadata={'source': 'Implementation_MLOps.pdf', 'page': 19, 'page_label': '20'}, page_content='20 \\nadditional components to code further adaptions are needed. Next, the concept of execution \\nenvironments was introduced to emphasis the need for different environments for developing, \\ntesting and execution of code.  Usually, there are separated environments used both, for the \\ndevelopment of the model and for the deployment of the application. The described test \\nstrategy covers all aspects of data, code and model relevant for the development of the model \\nand its deployment. A test pyramid was introduced which showed the low -cost unit tests for \\nthe code, the qualitative tests for model and data as well as the integration  tests of the whole \\npipeline.  \\nThe proposed MLOps framework is using these concepts to fulfil the requirements of \\nreproducibility, reliability and life cycle management . First, the data is analyzed, and the \\ninsights are integrated into the model development code. This code should later be able to run \\nat any time in the production environment an create new models on demand. Thus, a suitable \\ntesting of the code and the whole process to guarantee its functionality is required in the \\nintegration environment. Special attention must be provided to the integration of the model into \\nthe application. In the production environment the new model is trained leveraging the \\nknowledge gained in the development environment. Different tests regarding data and model \\nquality are executed. Finally, the application with the new model is tested and then deployed. \\nHere different deployment strategies can be applied. To ensure a continuous functioning of the \\napplication an appropriate monitoring is required. \\n \\nElectronic copy available at: https://ssrn.com/abstract=4540074'),\n",
       " Document(metadata={'source': 'Implementation_MLOps.pdf', 'page': 20, 'page_label': '21'}, page_content=\"21 \\nBibliography \\n[1] Armbrust, M., Das, T., Sun, L., Yavuz, B., Zhu, S., Murthy, M., ... & Zaharia, M. (2020). \\nDelta lake: high-performance ACID table storage over cloud object stores. Proceedings \\nof the VLDB Endowment, 13(12), 3411-3424. \\n[2] Atlassian (2023). History of DevOps. URL: https://www.atlassian.com/devops/what-is-\\ndevops/history-of-devops (accessed 23.02.2023). \\n[3] Banerjee, I., Ghanta, D., Nautiyal, G., Sanchana, P., Katageri, P., & Modi, A. (2023). \\nMLOps with enhanced performance control and observability . arXiv preprint \\narXiv:2302.01061.  \\n[4] Baumgartner, M., Klonk, M., Pichler, H., Seidl, R., & Tanczos, S. (2021). Agile Testing. \\nSpringer International Publishing.  \\n[5] Breck, E., Cai, S., Nielsen, E., Salib, M., & Sculley, D. (2017, December). The ML test \\nscore: A rubric for ML production readiness and technical debt reduction. In 2017 IEEE \\nInternational Conference on Big Data (Big Data) (pp. 1123-1132). IEEE.  \\n[6] Chattopadhyay, S., Prasad, I., Henley, A. Z., Sarma, A., & Barik, T. (2020, April). What's \\nwrong with computational notebooks? Pain points, needs, and design opportunities. \\nIn Proceedings of the 2020 CHI conference on human factors in computing \\nsystems (pp. 1-12). \\n[7] Ebert, C., Gallardo, G., Hernantes, J., & Serrano, N. (2016). DevOps.  Ieee \\nSoftware, 33(3), 94-100.  \\n[8] Erich, F., Amrit, C., & Daneva, M. (2014). Report: Devops literature review. University \\nof Twente, Tech. Rep.  \\n[9] European Commission. (2021). Laying down harmonised rules on artificial intelligence \\n(Artificial Intelligence Act) and amending certain union legislative acts. \\n[10] Garg, S., Pundir, P., Rathee , G., Gupta, P. K., Garg, S., & Ahlawat, S. (2021, \\nDecember). On continuous integration/continuous delivery for automated deployment \\nof machine learning models using mlops. In 2021 IEEE fourth international conference \\non artificial intelligence and knowledge engineering (AIKE) (pp. 25-28). IEEE.  \\n[11] Granlund, T., Stirbu, V., & Mikkonen, T. (2021). Towards regulatory-compliant MLOps: \\nOravizio’s journey from a machine learning experiment to a deployed certified medical \\nproduct. SN computer Science, 2(5), 342. \\n[12] Huijskens, T. (2019). Data Scientists, the Only Useful Code is Production Code . URL: \\nhttps://medium.com/quantumblack/data-scientists-the-only-useful-code-is-production-\\ncode-8b4806f2fe75 (accessed 25.04.2023). \\n[13] Joseph, B., Rafi, K., Matt, T., Niall, T. (2022). The Big Book of MLOps. Databricks.   \\nElectronic copy available at: https://ssrn.com/abstract=4540074\"),\n",
       " Document(metadata={'source': 'Implementation_MLOps.pdf', 'page': 21, 'page_label': '22'}, page_content='22 \\n[14] JWT (2023). Introduction to Json Web Tokens . URL: https://jwt.io/introduction \\n(accessed 22.04.2023). \\n[15] Karamitsos, I., Albarhami, S., & Apostolopoulos, C. (2020). Applying DevOps practices \\nof continuous automation for machine learning. Information, 11(7), 363. \\n[16] Kreuzberger, D., Kühl, N., & Hirschl, S. (2023). Machine learning operations (mlops): \\nOverview, definition, and architecture. IEEE Access. \\n[17] Kubernetes, T. (2019). Kubernetes. Kubernetes. \\n[18] Liu, Y., Zaharia, M. (2022).  Practical De ep learning at scale with mlflow . Packt \\nPublishing. \\n[19] Masse, M. (2011).  REST API design rulebook: designing consistent RESTful web \\nservice interfaces. \" O\\'Reilly Media, Inc.\".  \\n[20] Mercedes Benz Mobility.  About Us.  URL: https://www.mercedes-benz-\\nmobility.com/en/who-we-are/about-us/ (accessed  11.02.2023). \\n[21] Murphy, C., Kaiser, G. E., & Arias, M. (2007). An approach to software testing of \\nmachine learning applications. \\n[22] Omont, N., Treveil, M., Stenac, C., Lefèvre, K. (2021). MLOps – Kernkonzepte im \\nÜberblick: Machine -Learning-Prozesse im Unternehmen nachhaltig automatisieren \\nund skalieren. Germany: O\\'Reilly. \\n[23] Pachyderm (2023). Getting-Started. URL: https://docs.pachyderm.com/latest/getting-\\nstarted/ (accessed 27.04.2023). \\n[24] Pex (2023). Building .pex files. URL: \\nhttps://pex.readthedocs.io/en/v2.1.134/buildingpex.html (accessed 26.04.2023). \\n[25] Perez, F., Granger, B. E., & Hunter, J. D. (2010). Python: an ecosystem for scientific \\ncomputing. Computing in Science & Engineering, 13(2), 13-21.  \\n[26] Raj E. (2021). MLOps-Entwicklung: Schnelles und skalierbares Entwickeln, Testen und \\nVerwalten produktionsbereiter Machine-Learning-Lebenszyklen. Packt Publishing. \\n[27] Sato, D, Wilder, A, Windheuser, C. (2019). C. Continuous delivery for machine learning. \\nURL: https://martinfowler.com/articles/cd4ml.html (accessed 20.04.2023).  \\n[28] Scikit-learn (2023). Getting Started . Url: https://scikit-learn.org/stable/getting_ \\nstarted.html (accessed 20.04.2023). \\n[29] Seiter, M. (2019). Business Analytics. Vahlen. \\n[30] Subramanya, R., Sierla, S., & Vyatkin, V. (2022). From DevOps to MLOps: Overview \\nand Application to Electricity Market Forecasting. Applied Sciences, 12(19), 9851. \\n[31] Tiangolo (2023). FastApi. URL: https://fastapi.tiangolo.com (accessed 15.04.2023). \\n[32] Truong, A., Walters, A., Goodsitt, J., Hines, K., Bruss, C. B., & Farivar, R. (2019, \\nNovember). Towards automated machine learning: Evaluatio n and comparison of \\nElectronic copy available at: https://ssrn.com/abstract=4540074'),\n",
       " Document(metadata={'source': 'Implementation_MLOps.pdf', 'page': 22, 'page_label': '23'}, page_content='23 \\nAutoML approaches and tools. In 2019 IEEE 31st international conference on tools with \\nartificial intelligence (ICTAI) (pp. 1471-1479). IEEE.  \\n[33] Turnbull, J. (2014). The Docker Book: Containerization is the new virtualization. James \\nTurnbull.  \\n[34] Wilson, G., Aruliah, D. A., Brown, C. T., Chue Hong, N. P., Davis, M., Guy, R. T., ... & \\nWilson, P. (2014). Best practices for scientific computing.  PLoS biology , 12(1), \\ne1001745.  \\n[35] Zaharia, M., Chen, A., Davidson, A., Ghodsi, A., Hong, S. A., Konwinski, A., ... & Zumar, \\nC. (2018). Accelerating the machine learning lifecycle with MLflow.  IEEE Data Eng. \\nBull., 41(4), 39-45.  \\n[36] Zhao, Y., Belloum, A. S., & Zhao, Z. (2022). MLOps Scaling Machine Learning \\nLifecycle in an Industrial Setting. International Journal of Industrial and Manufacturing \\nEngineering, 16(5), 143-153. \\nElectronic copy available at: https://ssrn.com/abstract=4540074')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"Implementation_MLOps.pdf\")\n",
    "\n",
    "pages = loader.load_and_split()\n",
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Please answer the question based on the context below. If it's deemed too complex\n",
      "    or not relevant, please reply \"I afraid my capability has yet to satisfy to the topic you're asking\"\n",
      "\n",
      "Context: Here is some context\n",
      "\n",
      "Question: Here is a question\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "    Please answer the question based on the context below. If it's deemed too complex\n",
    "    or not relevant, please reply \"I afraid my capability has yet to satisfy to the topic you're asking\"\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "print(prompt.format(context=\"Here is some context\", question=\"Here is a question\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Linking customized prompt with Llama and parser\n",
    "chain = prompt | model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The topic of our conversation appears to be MLOps (Machine Learning Operations). Would you like to discuss aspects of implementing MLOps, such as pipeline management, model deployment, or hyperparameter tuning?'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    {\n",
    "        \"context\": \"The topic we're talking about is the implementation of MLOps\",\n",
    "        \"question\": \"What's the topic of talking?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'properties': {'context': {'title': 'Context', 'type': 'string'},\n",
       "  'question': {'title': 'Question', 'type': 'string'}},\n",
       " 'required': ['context', 'question'],\n",
       " 'title': 'PromptInput',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.input_schema.model_json_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Memory Storage with DocArrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\github_repos\\OpenAI\\ollama_project\\.venv\\lib\\site-packages\\pydantic\\_migration.py:283: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
      "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "vector_store = DocArrayInMemorySearch.from_documents(\n",
    "    pages, \n",
    "    embedding=embedding\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'Implementation_MLOps.pdf', 'page': 1, 'page_label': '2'}, page_content='2 \\ndata component involves preparing and managing the data used to train and evaluate machine \\nlearning models. The model component involves developing and testing machine learning \\nmodels to ensure that they meet the desired accuracy and performance requirements. The \\ncode component involves the software engineering practices used to package, deploy, and \\nmaintain machine learning models in a production environment. The integration of these three \\ncomponents is critical to the success of MLOps. [13] \\n1.2 Research Question \\nThe principles of DevOps were first mentioned in 2007 and since then have been discussed \\nextensively. [2], [8] Thus, the process how to write, test and deploy code is well established. \\n[4], [7] From this approach the framework of MLOps has been developed. [30] The main focus \\nof the existing literature is defining a suitable process how to train and deploy a n ML model \\nand the tools [10], [13], [18], [26], [36] and skills [16], [22], [30], [35] needed to implement it.  \\nEven though testing is one of the most important features of DevOps these reports only provide \\na high-level overview of required tests or focus on special parts of testing like the performance \\nof the ML algorithm.  \\nOnly few articles focus on a valid testing strategy without directly linking them to specific parts \\nin the MLOps process. [5], [21] Additionally, often the process is not considering the usage of \\ndifferent environments. It is generally acknowledged that the testing of MLOps is very complex \\nand requires further investigation. [27] To close this gap this paper contributes to the literature \\nin the following way: \\nThe required tests to ensure a reliable process are presented and a suitable MLOps framework \\nis proposed which  describes the integration of these tests in combination with different \\nexecution environments in detail. \\n1.3 Structure \\nThe paper is structured in the following way. First the general definitions of underlying \\nprinciples used by MLOps are provided (chapter 2). This includes an introduction to DevOps \\nand staging environments as well as an extensive overview over all required tests which should \\nbe executed during the MLOps process. The third chapter will define a standard development \\nand deployment process starting with consuming the data, followed by the training of the model \\nand finally its deployment. The workflows also consider the execution of the different steps in \\ndifferent environments. Especially, the various tests which were described in chapter 2 are part \\nof the proposed processes. The last chapter concludes. \\nElectronic copy available at: https://ssrn.com/abstract=4540074'),\n",
       " Document(metadata={'source': 'Implementation_MLOps.pdf', 'page': 16, 'page_label': '17'}, page_content='17 \\n• Required Documentation. \\nThis information is required for being able to use the model within the application. Now the \\ncontinuous delivery pipeline starts. First, the load tests with the final model in combination with \\nthe application are tested to ensure technical functionality similar to the  tests executed in  \\nintegration environment. Here it bec omes apparent why the integration environment of the \\nserving process needs to mimic the production environment. In case these would be different \\nthe testing of the infrastructure with load tests would be meaningless. If these are carried out \\nsuccessfully the model can be released. \\nThere are two basic scenarios how models are served and consumed in production: [22], [27] \\n• Batch inference: Here the model is used to determine the predictions for a complete \\ndataset. For large datasets different parallelization techniques like Apache Spark can \\nbe used to increase the computation speed. The batch scoring is usually performed for \\nscheduled jobs. \\n• Real-Time inference: Here the model is used to determine the prediction of a single or \\nsmall observations in real time. In this case multiple instances of the model can be \\ndeployed, and this requires parallelization by a load balancer. \\nThere are different ways a smooth update of the existing model in production can happen. This \\nis especially important in case of real-time inference as here restarting of the server should be \\navoided to reduce down times. Often, the new version of the model is deployed in parallel to \\nthe existing, stable one. Once the functioning of the new model is ensured the workload is \\nshifted and the old version is shut down. In reality there are a lot of different modifications  of \\nthis framework like canary deployments or blue-green deployments. [22] \\nIn additional to the way how a model is used in production it is also import how the model is \\nintegrated into the application itself. There are three different patterns how this can be \\nachieved: [11], [27] \\n• Embedded model: Here the model is included in the applica tion code as normal \\ndependency. This means the application code together with the trained model can be \\ntreated as one application artifact and version. This pattern requires the model to be in \\na readable format by the application. This can be challenging i n case different \\nprogramming languages are used. For example, one can use the framework H2O to \\nexport the model as a Java object. [32] In case of a python model the package ‘pex’ \\n(python executable) can be used to transform the model into an executable which can \\nthen be embedded into the application. [24] \\n• Model deployed as a standalone service: The model is deployed independent of the \\nconsuming application. However, the standalone service will still need some additional \\nElectronic copy available at: https://ssrn.com/abstract=4540074'),\n",
       " Document(metadata={'source': 'Implementation_MLOps.pdf', 'page': 7, 'page_label': '8'}, page_content='8 \\nThe integration tests ensure that the full ML Pipeline is working as expected. This usually \\ncontains the data pipeline, t he data transformations, the model training, the model \\nperformance, and the deployment into an application. Since the full pipeline roughly consists \\nof two parts, the model development, and the application part, it often makes sense to first test \\nthe whole  model development process and then add the tests for the deployment. This is \\nespecially important since both parts target different environments as described in section 1.2. \\n[5] \\nThe load tests are testing the robustness of the application by simulating a real -world load. \\nThis measures the following factors: [26]  \\n• Endurance: Test if the application can resist the expected load for an extended period. \\n• Volume: Test if the application can handle a large volume. \\n• Performance: Test if the application performs stable with a certain workload. \\n• Scalability: Test if the application is able to scale up or down depending on the number \\nof requests. \\nThis can be of high importance, since the size of the model can vary greatly depending on the \\nused parameters, e.g., number of trees and  tree depth for tree-based models. Finally, the full \\napplication should be tested manually from end to end  which requires manual effort and thus \\ncomes with the highest costs. [5] \\nAll of the described tests are relevant for the MLOps approach. It is important to note that not \\nall of them are executed exclusively in the integration environment which focuses on testing of \\nthe code. The tests regarding data quality, model performance or load testing are also essential \\nin the prod environment. This is important since the lifecycle of code, data and model are not \\nthe same, e.g., one could train a new model although the code has not changed and thus the \\ntests in the integrat ion environment were not executed. How the tests are integrated into the \\nMLOps process is described in the next chapter. \\n2 The MLOps Process \\nIn this chapter the theory of the model development and deployment steps of the MLOps \\nprocess are described in detail . To understand a complete MLOps Process one must first \\ndefine a typical ML process. The key features of a such a process are categorized in Figure 4 \\nand will be explained in the following in detail. \\n \\nElectronic copy available at: https://ssrn.com/abstract=4540074'),\n",
       " Document(metadata={'source': 'Implementation_MLOps.pdf', 'page': 21, 'page_label': '22'}, page_content='22 \\n[14] JWT (2023). Introduction to Json Web Tokens . URL: https://jwt.io/introduction \\n(accessed 22.04.2023). \\n[15] Karamitsos, I., Albarhami, S., & Apostolopoulos, C. (2020). Applying DevOps practices \\nof continuous automation for machine learning. Information, 11(7), 363. \\n[16] Kreuzberger, D., Kühl, N., & Hirschl, S. (2023). Machine learning operations (mlops): \\nOverview, definition, and architecture. IEEE Access. \\n[17] Kubernetes, T. (2019). Kubernetes. Kubernetes. \\n[18] Liu, Y., Zaharia, M. (2022).  Practical De ep learning at scale with mlflow . Packt \\nPublishing. \\n[19] Masse, M. (2011).  REST API design rulebook: designing consistent RESTful web \\nservice interfaces. \" O\\'Reilly Media, Inc.\".  \\n[20] Mercedes Benz Mobility.  About Us.  URL: https://www.mercedes-benz-\\nmobility.com/en/who-we-are/about-us/ (accessed  11.02.2023). \\n[21] Murphy, C., Kaiser, G. E., & Arias, M. (2007). An approach to software testing of \\nmachine learning applications. \\n[22] Omont, N., Treveil, M., Stenac, C., Lefèvre, K. (2021). MLOps – Kernkonzepte im \\nÜberblick: Machine -Learning-Prozesse im Unternehmen nachhaltig automatisieren \\nund skalieren. Germany: O\\'Reilly. \\n[23] Pachyderm (2023). Getting-Started. URL: https://docs.pachyderm.com/latest/getting-\\nstarted/ (accessed 27.04.2023). \\n[24] Pex (2023). Building .pex files. URL: \\nhttps://pex.readthedocs.io/en/v2.1.134/buildingpex.html (accessed 26.04.2023). \\n[25] Perez, F., Granger, B. E., & Hunter, J. D. (2010). Python: an ecosystem for scientific \\ncomputing. Computing in Science & Engineering, 13(2), 13-21.  \\n[26] Raj E. (2021). MLOps-Entwicklung: Schnelles und skalierbares Entwickeln, Testen und \\nVerwalten produktionsbereiter Machine-Learning-Lebenszyklen. Packt Publishing. \\n[27] Sato, D, Wilder, A, Windheuser, C. (2019). C. Continuous delivery for machine learning. \\nURL: https://martinfowler.com/articles/cd4ml.html (accessed 20.04.2023).  \\n[28] Scikit-learn (2023). Getting Started . Url: https://scikit-learn.org/stable/getting_ \\nstarted.html (accessed 20.04.2023). \\n[29] Seiter, M. (2019). Business Analytics. Vahlen. \\n[30] Subramanya, R., Sierla, S., & Vyatkin, V. (2022). From DevOps to MLOps: Overview \\nand Application to Electricity Market Forecasting. Applied Sciences, 12(19), 9851. \\n[31] Tiangolo (2023). FastApi. URL: https://fastapi.tiangolo.com (accessed 15.04.2023). \\n[32] Truong, A., Walters, A., Goodsitt, J., Hines, K., Bruss, C. B., & Farivar, R. (2019, \\nNovember). Towards automated machine learning: Evaluatio n and comparison of \\nElectronic copy available at: https://ssrn.com/abstract=4540074')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.as_retriever().invoke(\"Machine Learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'Implementation_MLOps.pdf', 'page': 1, 'page_label': '2'}, page_content='2 \\ndata component involves preparing and managing the data used to train and evaluate machine \\nlearning models. The model component involves developing and testing machine learning \\nmodels to ensure that they meet the desired accuracy and performance requirements. The \\ncode component involves the software engineering practices used to package, deploy, and \\nmaintain machine learning models in a production environment. The integration of these three \\ncomponents is critical to the success of MLOps. [13] \\n1.2 Research Question \\nThe principles of DevOps were first mentioned in 2007 and since then have been discussed \\nextensively. [2], [8] Thus, the process how to write, test and deploy code is well established. \\n[4], [7] From this approach the framework of MLOps has been developed. [30] The main focus \\nof the existing literature is defining a suitable process how to train and deploy a n ML model \\nand the tools [10], [13], [18], [26], [36] and skills [16], [22], [30], [35] needed to implement it.  \\nEven though testing is one of the most important features of DevOps these reports only provide \\na high-level overview of required tests or focus on special parts of testing like the performance \\nof the ML algorithm.  \\nOnly few articles focus on a valid testing strategy without directly linking them to specific parts \\nin the MLOps process. [5], [21] Additionally, often the process is not considering the usage of \\ndifferent environments. It is generally acknowledged that the testing of MLOps is very complex \\nand requires further investigation. [27] To close this gap this paper contributes to the literature \\nin the following way: \\nThe required tests to ensure a reliable process are presented and a suitable MLOps framework \\nis proposed which  describes the integration of these tests in combination with different \\nexecution environments in detail. \\n1.3 Structure \\nThe paper is structured in the following way. First the general definitions of underlying \\nprinciples used by MLOps are provided (chapter 2). This includes an introduction to DevOps \\nand staging environments as well as an extensive overview over all required tests which should \\nbe executed during the MLOps process. The third chapter will define a standard development \\nand deployment process starting with consuming the data, followed by the training of the model \\nand finally its deployment. The workflows also consider the execution of the different steps in \\ndifferent environments. Especially, the various tests which were described in chapter 2 are part \\nof the proposed processes. The last chapter concludes. \\nElectronic copy available at: https://ssrn.com/abstract=4540074'),\n",
       " Document(metadata={'source': 'Implementation_MLOps.pdf', 'page': 16, 'page_label': '17'}, page_content='17 \\n• Required Documentation. \\nThis information is required for being able to use the model within the application. Now the \\ncontinuous delivery pipeline starts. First, the load tests with the final model in combination with \\nthe application are tested to ensure technical functionality similar to the  tests executed in  \\nintegration environment. Here it bec omes apparent why the integration environment of the \\nserving process needs to mimic the production environment. In case these would be different \\nthe testing of the infrastructure with load tests would be meaningless. If these are carried out \\nsuccessfully the model can be released. \\nThere are two basic scenarios how models are served and consumed in production: [22], [27] \\n• Batch inference: Here the model is used to determine the predictions for a complete \\ndataset. For large datasets different parallelization techniques like Apache Spark can \\nbe used to increase the computation speed. The batch scoring is usually performed for \\nscheduled jobs. \\n• Real-Time inference: Here the model is used to determine the prediction of a single or \\nsmall observations in real time. In this case multiple instances of the model can be \\ndeployed, and this requires parallelization by a load balancer. \\nThere are different ways a smooth update of the existing model in production can happen. This \\nis especially important in case of real-time inference as here restarting of the server should be \\navoided to reduce down times. Often, the new version of the model is deployed in parallel to \\nthe existing, stable one. Once the functioning of the new model is ensured the workload is \\nshifted and the old version is shut down. In reality there are a lot of different modifications  of \\nthis framework like canary deployments or blue-green deployments. [22] \\nIn additional to the way how a model is used in production it is also import how the model is \\nintegrated into the application itself. There are three different patterns how this can be \\nachieved: [11], [27] \\n• Embedded model: Here the model is included in the applica tion code as normal \\ndependency. This means the application code together with the trained model can be \\ntreated as one application artifact and version. This pattern requires the model to be in \\na readable format by the application. This can be challenging i n case different \\nprogramming languages are used. For example, one can use the framework H2O to \\nexport the model as a Java object. [32] In case of a python model the package ‘pex’ \\n(python executable) can be used to transform the model into an executable which can \\nthen be embedded into the application. [24] \\n• Model deployed as a standalone service: The model is deployed independent of the \\nconsuming application. However, the standalone service will still need some additional \\nElectronic copy available at: https://ssrn.com/abstract=4540074'),\n",
       " Document(metadata={'source': 'Implementation_MLOps.pdf', 'page': 7, 'page_label': '8'}, page_content='8 \\nThe integration tests ensure that the full ML Pipeline is working as expected. This usually \\ncontains the data pipeline, t he data transformations, the model training, the model \\nperformance, and the deployment into an application. Since the full pipeline roughly consists \\nof two parts, the model development, and the application part, it often makes sense to first test \\nthe whole  model development process and then add the tests for the deployment. This is \\nespecially important since both parts target different environments as described in section 1.2. \\n[5] \\nThe load tests are testing the robustness of the application by simulating a real -world load. \\nThis measures the following factors: [26]  \\n• Endurance: Test if the application can resist the expected load for an extended period. \\n• Volume: Test if the application can handle a large volume. \\n• Performance: Test if the application performs stable with a certain workload. \\n• Scalability: Test if the application is able to scale up or down depending on the number \\nof requests. \\nThis can be of high importance, since the size of the model can vary greatly depending on the \\nused parameters, e.g., number of trees and  tree depth for tree-based models. Finally, the full \\napplication should be tested manually from end to end  which requires manual effort and thus \\ncomes with the highest costs. [5] \\nAll of the described tests are relevant for the MLOps approach. It is important to note that not \\nall of them are executed exclusively in the integration environment which focuses on testing of \\nthe code. The tests regarding data quality, model performance or load testing are also essential \\nin the prod environment. This is important since the lifecycle of code, data and model are not \\nthe same, e.g., one could train a new model although the code has not changed and thus the \\ntests in the integrat ion environment were not executed. How the tests are integrated into the \\nMLOps process is described in the next chapter. \\n2 The MLOps Process \\nIn this chapter the theory of the model development and deployment steps of the MLOps \\nprocess are described in detail . To understand a complete MLOps Process one must first \\ndefine a typical ML process. The key features of a such a process are categorized in Figure 4 \\nand will be explained in the following in detail. \\n \\nElectronic copy available at: https://ssrn.com/abstract=4540074'),\n",
       " Document(metadata={'source': 'Implementation_MLOps.pdf', 'page': 21, 'page_label': '22'}, page_content='22 \\n[14] JWT (2023). Introduction to Json Web Tokens . URL: https://jwt.io/introduction \\n(accessed 22.04.2023). \\n[15] Karamitsos, I., Albarhami, S., & Apostolopoulos, C. (2020). Applying DevOps practices \\nof continuous automation for machine learning. Information, 11(7), 363. \\n[16] Kreuzberger, D., Kühl, N., & Hirschl, S. (2023). Machine learning operations (mlops): \\nOverview, definition, and architecture. IEEE Access. \\n[17] Kubernetes, T. (2019). Kubernetes. Kubernetes. \\n[18] Liu, Y., Zaharia, M. (2022).  Practical De ep learning at scale with mlflow . Packt \\nPublishing. \\n[19] Masse, M. (2011).  REST API design rulebook: designing consistent RESTful web \\nservice interfaces. \" O\\'Reilly Media, Inc.\".  \\n[20] Mercedes Benz Mobility.  About Us.  URL: https://www.mercedes-benz-\\nmobility.com/en/who-we-are/about-us/ (accessed  11.02.2023). \\n[21] Murphy, C., Kaiser, G. E., & Arias, M. (2007). An approach to software testing of \\nmachine learning applications. \\n[22] Omont, N., Treveil, M., Stenac, C., Lefèvre, K. (2021). MLOps – Kernkonzepte im \\nÜberblick: Machine -Learning-Prozesse im Unternehmen nachhaltig automatisieren \\nund skalieren. Germany: O\\'Reilly. \\n[23] Pachyderm (2023). Getting-Started. URL: https://docs.pachyderm.com/latest/getting-\\nstarted/ (accessed 27.04.2023). \\n[24] Pex (2023). Building .pex files. URL: \\nhttps://pex.readthedocs.io/en/v2.1.134/buildingpex.html (accessed 26.04.2023). \\n[25] Perez, F., Granger, B. E., & Hunter, J. D. (2010). Python: an ecosystem for scientific \\ncomputing. Computing in Science & Engineering, 13(2), 13-21.  \\n[26] Raj E. (2021). MLOps-Entwicklung: Schnelles und skalierbares Entwickeln, Testen und \\nVerwalten produktionsbereiter Machine-Learning-Lebenszyklen. Packt Publishing. \\n[27] Sato, D, Wilder, A, Windheuser, C. (2019). C. Continuous delivery for machine learning. \\nURL: https://martinfowler.com/articles/cd4ml.html (accessed 20.04.2023).  \\n[28] Scikit-learn (2023). Getting Started . Url: https://scikit-learn.org/stable/getting_ \\nstarted.html (accessed 20.04.2023). \\n[29] Seiter, M. (2019). Business Analytics. Vahlen. \\n[30] Subramanya, R., Sierla, S., & Vyatkin, V. (2022). From DevOps to MLOps: Overview \\nand Application to Electricity Market Forecasting. Applied Sciences, 12(19), 9851. \\n[31] Tiangolo (2023). FastApi. URL: https://fastapi.tiangolo.com (accessed 15.04.2023). \\n[32] Truong, A., Walters, A., Goodsitt, J., Hines, K., Bruss, C. B., & Farivar, R. (2019, \\nNovember). Towards automated machine learning: Evaluatio n and comparison of \\nElectronic copy available at: https://ssrn.com/abstract=4540074')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "retriever.invoke(\"Machine Learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'Implementation_MLOps.pdf', 'page': 5, 'page_label': '6'}, page_content='and the costly and slow high-level tests like end-to-end tests at the top. [4]  \\nSince data plays a fundamental role in developing ML models the first pillar of testing is testing \\nof the data. The quality of the data can be assessed by checking the completeness, accuracy, \\nconsistency, and timeliness [29]. Here one approach is to create a schema starting with \\ndetermining statistics from training data which can be checked against domain knowledge.'),\n",
       " Document(metadata={'source': 'Implementation_MLOps.pdf', 'page': 1, 'page_label': '2'}, page_content='components is critical to the success of MLOps. [13] \\n1.2 Research Question \\nThe principles of DevOps were first mentioned in 2007 and since then have been discussed \\nextensively. [2], [8] Thus, the process how to write, test and deploy code is well established. \\n[4], [7] From this approach the framework of MLOps has been developed. [30] The main focus \\nof the existing literature is defining a suitable process how to train and deploy a n ML model'),\n",
       " Document(metadata={'source': 'Implementation_MLOps.pdf', 'page': 8, 'page_label': '9'}, page_content='9 \\n \\nFigure 4: ML process (reprinted from [13]) \\n \\nThe question at hand is how to bring this process together with the principles of DevOps and \\na valid testing strategy to derive an automated workflow out of it.  The proposed framework is \\nbased on MLOps processes which already consider the usage of different environments  to \\nsome extent. [13], [22] However, these frameworks are now extended and adapted to allocate'),\n",
       " Document(metadata={'source': 'Implementation_MLOps.pdf', 'page': 0, 'page_label': '1'}, page_content='healthcare to finance, and from retail to transportation. However, for most of them deploying \\nML models into production environments is relatively new. Until now there might have only \\nbeen a manageable number of models which needed to be hosted. As the number of models \\nand the role they play for the company business growths, since more and more decisions are \\nautomated, the importance of being able to develop, deploy and operate the models efficiently')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"DevOps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The text doesn\\'t explicitly state the \"main core\" of MLOps, but based on the context and information provided, it can be inferred that the core of MLOps consists of:\\n\\n1. **Model development**: This involves writing code for machine learning models, training them, and iterating through various configurations to improve performance.\\n2. **Data science workflow**: This includes data preprocessing, feature engineering, model selection, and hyperparameter tuning, among other tasks.\\n3. **Version control and reproducibility**: MLOps emphasizes the importance of controlling versions of code, parameters, data, and software environments to ensure reproducibility and reliability.\\n4. **Automation and testing**: The text mentions the need for extensive testing of the project, including CI processes, to ensure that all processes and tasks function as expected.\\n\\nHowever, if we consider the \"main core\" of MLOps in a more abstract sense, it could be argued that the core consists of:\\n\\n1. **Continuous Integration (CI)**: Integrating model development with automated testing and validation to ensure reliability and reproducibility.\\n2. **Continuous Deployment (CD)**: Deploying models and applications in production environments while ensuring continuous monitoring and logging.\\n\\nThese two aspects are often referred to as the \"DevOps\" of MLOps, as they aim to bridge the gap between machine learning development and production environments.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "    \"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | parser\n",
    ")\n",
    "\n",
    "chain.invoke({\"question\": \"What are main core of MLOps?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"What does MLOps aim for?\",\n",
    "    \"What underlies the theory of MLOps\",\n",
    "    \"According to the author's thought and opinion, how MLOps is structured?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question; What does MLOps aim for?\n",
      "Answer: MLOps (Machine Learning Operations) aims to bridge the gap between machine learning development and production, focusing on the testing of models and ensuring they meet certain standards before being deployed. It encompasses various aspects such as data quality control, model performance evaluation, fairness, bias detection, application integration testing, and continuous deployment. The ultimate goal is to deliver high-quality, reliable, and scalable machine learning models that meet business needs while maintaining regulatory compliance and transparency.\n",
      "\n",
      "Question; What underlies the theory of MLOps\n",
      "Answer: According to the text, several underlying concepts and theories are relevant to MLOps (Machine Learning Operations), including:\n",
      "\n",
      "1. **DevOps**: The integration of software development (Dev) and operations (Ops) to ensure smooth operation of software systems.\n",
      "2. **Continuous Delivery**: A practice where changes to code are delivered continuously to production, ensuring minimal disruption to the application or service.\n",
      "3. **Continuous Integration**: The process of integrating multiple components into a single unit, such as building a machine learning model with its dependencies.\n",
      "4. **Automated Machine Learning (AutoML)**: A field that aims to automate the process of building and deploying machine learning models.\n",
      "\n",
      "These concepts underlie the theory of MLOps, which seeks to provide a structured approach to developing, testing, and deploying machine learning models in production environments.\n",
      "\n",
      "Question; According to the author's thought and opinion, how MLOps is structured?\n",
      "Answer: Based on the text, it appears that the author suggests a three-pillar structure for MLOps:\n",
      "\n",
      "1. **Testing**: This pillar includes three stages:\n",
      "\t* Unit testing (or model testing): evaluating the performance of the model.\n",
      "\t* Integration testing: integrating the model with other components of the application.\n",
      "\t* Application testing: testing the entire application, including the integration of the model.\n",
      "2. **Deployment and Serving**: This pillar includes two main scenarios for deploying and serving models in production:\n",
      "\t* Batch inference: using a model to predict values for complete datasets in batches.\n",
      "\t* Real-time inference: using a model to predict values in real-time for individual or small observations.\n",
      "3. **Integration with the Application**: The author suggests three patterns for integrating models into applications:\n",
      "\t* Embedded model: including the model as a normal dependency within the application code.\n",
      "\t* Model deployed as a standalone service: deploying the model independently of the consuming application.\n",
      "\n",
      "Overall, the author's thought and opinion on MLOps structure seem to emphasize the importance of testing, deployment, and serving models in a structured and efficient manner.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for question in questions:\n",
    "    print(f\"Question; {question}\")\n",
    "    print(f\"Answer: {chain.invoke({'question': question})}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How about Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500, chunk_overlap=50, length_function=len, is_separator_regex=False\n",
    "    )\n",
    "\n",
    "chunks = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "### Embedding with nomic\n",
    "embedding_nomic = OllamaEmbeddings(model=\"nomic-embed-text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import ollama\n",
    "\n",
    "### Initialize vector DB Chroma\n",
    "vector_db_chroma = Chroma(\n",
    "    embedding_function=embedding_nomic,\n",
    "    collection_name=\"local-mlops\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ids = [str(i) for i in range(len(chunks))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'Implementation_MLOps.pdf', 'page': 0, 'page_label': '1'}, page_content='1 \\nImplementation of MLOps \\n \\nMaximilian Zwiesler \\n \\nJune 2023 \\n \\nAbstract \\nThe rise of machine learning (ML) has reshaped industries, demanding efficient ML \\nmodel deployment. This has led to the emergence of Machine Learning Operations \\n(MLOps), which streamlines the entire ML workflow. MLOps aligns data, model, and \\ncode component s to ensure accurate and scalable models. However, testing and \\ndiverse execution environments are underrepresented in existing literature. This paper'),\n",
       " Document(metadata={'source': 'Implementation_MLOps.pdf', 'page': 0, 'page_label': '1'}, page_content='bridges this gap by presenting crucial tests for reliability and proposing an integrated \\nMLOps framework that unifies testing with different environments. The paper structure \\nencompasses foundational definitions, a standardized development process, and \\nconcludes with insights into enhancing MLOps reliability. \\n1 Introduction \\n1.1 Motivation \\nMachine learning (ML) has b ecome an important part of many industries, ranging from'),\n",
       " Document(metadata={'source': 'Implementation_MLOps.pdf', 'page': 0, 'page_label': '1'}, page_content='healthcare to finance, and from retail to transportation. However, for most of them deploying \\nML models into production environments is relatively new. Until now there might have only \\nbeen a manageable number of models which needed to be hosted. As the number of models \\nand the role they play for the company business growths, since more and more decisions are \\nautomated, the importance of being able to develop, deploy and operate the models efficiently')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.add_documents(documents=chunks, ids=ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* components is critical to the success of MLOps. [13] \n",
      "1.2 Research Question \n",
      "The principles of DevOps were first mentioned in 2007 and since then have been discussed \n",
      "extensively. [2], [8] Thus, the process how to write, test and deploy code is well established. \n",
      "[4], [7] From this approach the framework of MLOps has been developed. [30] The main focus \n",
      "of the existing literature is defining a suitable process how to train and deploy a n ML model [{'source': 'Implementation_MLOps.pdf', 'page': 1, 'page_label': '2'}]\n",
      "* healthcare to finance, and from retail to transportation. However, for most of them deploying \n",
      "ML models into production environments is relatively new. Until now there might have only \n",
      "been a manageable number of models which needed to be hosted. As the number of models \n",
      "and the role they play for the company business growths, since more and more decisions are \n",
      "automated, the importance of being able to develop, deploy and operate the models efficiently [{'source': 'Implementation_MLOps.pdf', 'page': 0, 'page_label': '1'}]\n"
     ]
    }
   ],
   "source": [
    "results = vector_store.similarity_search(\n",
    "    \"DevOps constitutes the foundational aspect of MLOps\",\n",
    "    k=2,\n",
    "    filter={\"source\": \"Implementation_MLOps.pdf\"},\n",
    ")\n",
    "for res in results:\n",
    "    print(f\"* {res.page_content} [{res.metadata}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with Other Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Please answer the question based on the context below. If it's deemed too complex\n",
      "    or not relevant, please reply \"I afraid my capability has yet to extend to the topic you're asking\"\n",
      "\n",
      "Context: Here is some context\n",
      "\n",
      "Question: Here is a question\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "    Please answer the question based on the context below. If it's deemed too complex\n",
    "    or not relevant, please reply \"I afraid my capability has yet to extend to the topic you're asking\"\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "print(prompt.format(context=\"Here is some context\", question=\"Here is a question\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OllamaLLM(model=MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The main core of MLOps (Machine Learning Operating System) includes:\\n\\n1. Data Ingestion and Management\\n2. Model Development and Training\\n3. Model Deployment and Serving\\n4. Monitoring and Evaluation\\n\\nThese components work together to facilitate the end-to-end lifecycle of machine learning models, from data preparation to model deployment and monitoring, making it easier for data scientists and developers to build, deploy, and maintain scalable and reliable ML systems.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "    \"context\": itemgetter(\"question\"), \"question\": itemgetter(\"question\")\n",
    "    }\n",
    "    | prompt | model | parser\n",
    ")\n",
    "\n",
    "chain.invoke({\"question\": \"What are main core of MLOps?\"})\n",
    "\n",
    "# chain.invoke({\"question\": \"What MLOps mainly covers?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"\"\"During the research stage of machine learning experimentation, let say no one of the team has ever done time-series forecasting. That brings us to\n",
    "     the selection of source. So, which gives us a good starting point in getting familiar with models?\"\"\",\n",
    "    \"In the context of time-series, tell us the statistical test suitable for this task.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question; During the research stage of machine learning experimentation, let say no one of the team has ever done time-series forecasting. That brings us to\n",
      "     the selection of source. So, which gives us a good starting point in getting familiar with models?\n",
      "Answer: A great question for a team diving into time-series forecasting for the first time!\n",
      "\n",
      "I'm happy to help! Given that no one on your team has experience with time-series forecasting, I'd recommend starting with some fundamental concepts and popular models. Here's a good starting point:\n",
      "\n",
      "1. **Understand the basics of time-series data**: Familiarize yourself with what time-series data is, its characteristics, and common types (e.g., temporal, spatial, or both).\n",
      "2. **Explore ARIMA (AutoRegressive Integrated Moving Average) models**: ARIMA is a popular, widely used, and relatively simple model for time-series forecasting. It's a good starting point for understanding the basics of time-series modeling.\n",
      "3. **Look into other popular models**: After grasping the fundamentals of ARIMA, you can explore other models like:\n",
      "\t* Exponential Smoothing (ES)\n",
      "\t* Seasonal ARIMA (SARIMA)\n",
      "\t* Prophet (an open-source software for forecasting time series data)\n",
      "4. **Check out some tutorials and courses**:\n",
      "\t* Khan Academy's Time Series Analysis course\n",
      "\t* Coursera's \"Time Series Analysis\" Specialization by University of California, Irvine\n",
      "\t* Python libraries like Statsmodels, Scikit-learn, or PyAlgoTrade for time-series analysis\n",
      "5. **Start with simple datasets**: Begin with publicly available datasets, such as:\n",
      "\t* The Weather Channel dataset\n",
      "\t* The Boston Housing dataset (can be used to forecast housing prices)\n",
      "\t* The famous \"Airline Passenger Counts\" dataset\n",
      "\n",
      "These resources will provide a solid foundation for your team to start exploring time-series forecasting. As you become more comfortable with the basics, you can move on to more advanced models and techniques.\n",
      "\n",
      "Question; In the context of time-series, tell us the statistical test suitable for this task.\n",
      "Answer: In the context of time-series, a commonly used statistical test is the Augmented Dickey-Fuller (ADF) test or the Phillips-Perron (PP) test. These tests are used to determine if a time series is stationary or non-stationary.\n",
      "\n",
      "The ADF and PP tests can be used to:\n",
      "\n",
      "* Test for stationarity in a time series\n",
      "* Determine if a time series has a unit root, which indicates non-stationarity\n",
      "* Check if the null hypothesis of stationarity can be rejected\n",
      "\n",
      "However, it's worth noting that these tests assume certain conditions, such as the use of differencing (e.g., first difference or second difference) to stabilize the variance.\n",
      "\n",
      "Alternatively, other statistical tests like the KPSS test or the VAR (Vector Autoregression) model can also be used for time-series analysis.\n",
      "\n",
      "Please note that I've selected one of the most common and widely used tests in this context.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for question in questions:\n",
    "    print(f\"Question; {question}\")\n",
    "    print(f\"Answer: {chain.invoke({'question': question})}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The demonstration from Overfitted ends here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Adaptive RAG\n",
    "\n",
    "Source: https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_adaptive_rag_local/#components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "### Index \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Index\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OllamaEmbeddings(model='llama3.2', base_url=None, client_kwargs={})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Add to vectorDB\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=embedding,\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 'yes'}\n"
     ]
    }
   ],
   "source": [
    "### Retrieval Grader\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=\"llama3.2\", format=\"json\", temperature=0)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing relevance \n",
    "    of a retrieved document to a user question. If the document contains keywords related to the user question, \n",
    "    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
    "    Provide the binary score as a JSON with a single key 'score' and no premable or explanation.\n",
    "     <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Here is the retrieved document: \\n\\n {document} \\n\\n\n",
    "    Here is the user question: {question} \\n <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "retrieval_grader = prompt | llm | JsonOutputParser()\n",
    "question = \"agent memory\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_txt = docs[1].page_content\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 21\u001b[0m\n\u001b[0;32m      7\u001b[0m prompt \u001b[38;5;241m=\u001b[39m PromptTemplate(\n\u001b[0;32m      8\u001b[0m     template\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m <|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing whether \u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124m    an answer is grounded in / supported by a set of facts. Give a binary \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myes\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m score to indicate \u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m     input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocuments\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     18\u001b[0m )\n\u001b[0;32m     20\u001b[0m hallucination_grader \u001b[38;5;241m=\u001b[39m prompt \u001b[38;5;241m|\u001b[39m llm \u001b[38;5;241m|\u001b[39m JsonOutputParser()\n\u001b[1;32m---> 21\u001b[0m hallucination_grader\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocuments\u001b[39m\u001b[38;5;124m\"\u001b[39m: docs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mgeneration\u001b[49m})\n",
      "\u001b[1;31mNameError\u001b[0m: name 'generation' is not defined"
     ]
    }
   ],
   "source": [
    "### Hallucination Grader\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=\"llama3.2\", format=\"json\", temperature=0)\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing whether \n",
    "    an answer is grounded in / supported by a set of facts. Give a binary 'yes' or 'no' score to indicate \n",
    "    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \n",
    "    single key 'score' and no preamble or explanation. <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Here are the facts:\n",
    "    \\n ------- \\n\n",
    "    {documents} \n",
    "    \\n ------- \\n\n",
    "    Here is the answer: {generation}  <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"generation\", \"documents\"],\n",
    ")\n",
    "\n",
    "hallucination_grader = prompt | llm | JsonOutputParser()\n",
    "hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 'yes'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Answer Grader\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=\"llama3.2\", format=\"json\", temperature=0)\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing whether an \n",
    "    answer is useful to resolve a question. Give a binary score 'yes' or 'no' to indicate whether the answer is \n",
    "    useful to resolve a question. Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\n",
    "     <|eot_id|><|start_header_id|>user<|end_header_id|> Here is the answer:\n",
    "    \\n ------- \\n\n",
    "    {generation} \n",
    "    \\n ------- \\n\n",
    "    Here is the question: {question} <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"generation\", \"question\"],\n",
    ")\n",
    "\n",
    "answer_grader = prompt | llm | JsonOutputParser()\n",
    "answer_grader.invoke({\"question\": question, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know the answer to your question about trade-offs when fitting data with a Random Forest model. The provided context appears to be related to prompt engineering for autoregressive language models, and does not mention Random Forest at all.\n"
     ]
    }
   ],
   "source": [
    "### Generate\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an assistant for question-answering tasks. \n",
    "    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \n",
    "    Use three sentences maximum and keep the answer concise <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Question: {question} \n",
    "    Context: {context} \n",
    "    Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.2\", temperature=0)\n",
    "\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# Chain\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "question = \"What are some trade-off when fitting data with Random Forest model?\"\n",
    "docs = retriever.invoke(question)\n",
    "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'datasource': 'vectorstore'}\n"
     ]
    }
   ],
   "source": [
    "### Router\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an expert at routing a \n",
    "    user question to a vectorstore or web search. Use the vectorstore for questions on LLM  agents, \n",
    "    prompt engineering, and adversarial attacks. You do not need to be stringent with the keywords \n",
    "    in the question related to these topics. Otherwise, use web-search. Give a binary choice 'web_search' \n",
    "    or 'vectorstore' based on the question. Return the a JSON with a single key 'datasource' and \n",
    "    no premable or explanation. Question to route: {question} <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "\n",
    "question_router = prompt | llm | JsonOutputParser()\n",
    "question = \"The approach of bringing well-trained Random Forest to production environment.\"\n",
    "docs = retriever.get_relevant_documents(question)\n",
    "doc_txt = docs[1].page_content\n",
    "print(question_router.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Search\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "# set up API key\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"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\"\n",
    "\n",
    "\n",
    "web_search_tool = TavilySearchResults(k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import List\n",
    "\n",
    "### State\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        web_search: whether to add search\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    generation: str\n",
    "    web_search: str\n",
    "    documents: List[str]\n",
    "\n",
    "\n",
    "from langchain.schema import Document\n",
    "\n",
    "### Nodes\n",
    "\n",
    "\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents from vectorstore\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer using RAG on retrieved documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # RAG generation\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question\n",
    "    If any document is not relevant, we will set a flag to run web search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Filtered out irrelevant documents and updated web_search state\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    web_search = \"No\"\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke(\n",
    "            {\"question\": question, \"document\": d.page_content}\n",
    "        )\n",
    "        grade = score[\"score\"]\n",
    "        # Document relevant\n",
    "        if grade.lower() == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        # Document not relevant\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            # We do not include the document in filtered_docs\n",
    "            # We set a flag to indicate that we want to run web search\n",
    "            web_search = \"Yes\"\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search}\n",
    "\n",
    "\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based based on the question\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Appended web results to documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Web search\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "    if documents is not None:\n",
    "        documents.append(web_results)\n",
    "    else:\n",
    "        documents = [web_results]\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "### Conditional edge\n",
    "\n",
    "\n",
    "def route_question(state):\n",
    "    \"\"\"\n",
    "    Route question to web search or RAG.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ROUTE QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    print(question)\n",
    "    source = question_router.invoke({\"question\": question})\n",
    "    print(source)\n",
    "    print(source[\"datasource\"])\n",
    "    if source[\"datasource\"] == \"web_search\":\n",
    "        print(\"---ROUTE QUESTION TO WEB SEARCH---\")\n",
    "        return \"websearch\"\n",
    "    elif source[\"datasource\"] == \"vectorstore\":\n",
    "        print(\"---ROUTE QUESTION TO RAG---\")\n",
    "        return \"vectorstore\"\n",
    "\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or add web search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    question = state[\"question\"]\n",
    "    web_search = state[\"web_search\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if web_search == \"Yes\":\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\n",
    "            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, INCLUDE WEB SEARCH---\"\n",
    "        )\n",
    "        return \"websearch\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n",
    "\n",
    "\n",
    "### Conditional edge\n",
    "\n",
    "\n",
    "def grade_generation_v_documents_and_question(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    score = hallucination_grader.invoke(\n",
    "        {\"documents\": documents, \"generation\": generation}\n",
    "    )\n",
    "    grade = score[\"score\"]\n",
    "\n",
    "    # Check hallucination\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        # Check question-answering\n",
    "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
    "        grade = score[\"score\"]\n",
    "        if grade == \"yes\":\n",
    "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        pprint(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        return \"not supported\"\n",
    "\n",
    "\n",
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"websearch\", web_search)  # web search\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
    "workflow.add_node(\"generate\", generate)  # generatae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build graph\n",
    "workflow.set_conditional_entry_point(\n",
    "    route_question,\n",
    "    {\n",
    "        \"websearch\": \"websearch\",\n",
    "        \"vectorstore\": \"retrieve\",\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"websearch\": \"websearch\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"websearch\", \"generate\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        \"not supported\": \"generate\",\n",
    "        \"useful\": END,\n",
    "        \"not useful\": \"websearch\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ROUTE QUESTION---\n",
      "Have any knowledge about a popular Pokemon named Pikachu?\n",
      "{'datasource': 'web_search'}\n",
      "web_search\n",
      "---ROUTE QUESTION TO WEB SEARCH---\n",
      "---WEB SEARCH---\n"
     ]
    }
   ],
   "source": [
    "# Compile\n",
    "app = workflow.compile()\n",
    "\n",
    "# Test\n",
    "from pprint import pprint\n",
    "\n",
    "inputs = {\"question\": \"Have any knowledge about a popular Pokemon named Pikachu?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Finished running: {key}:\")\n",
    "pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
