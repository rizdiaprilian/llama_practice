{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68c40c43-80b8-4819-98a1-abfc9196b987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "MODEL = \"llama3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "973af2ca-94bd-4062-ae3c-e29dc6eddeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embedding = OllamaEmbeddings(model=\"llama3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24d65291-12c6-4369-a401-9db11563ee15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Retrieval Augmented Generation (RAG) is a deep learning technique that combines the strengths of retrieval-based models and text generation algorithms. RAG works by first retrieving relevant documents from an external knowledge base or database to inform the generation process. The retrieved information is then used as input to a language model, which generates new text based on the retrieved context. This approach has shown promising results in tasks such as question answering, text summarization, and conversational AI, where contextual understanding is crucial. RAG offers improved performance over traditional generation-based approaches.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Establish model and parser\n",
    "model = OllamaLLM(model=MODEL)\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = model | parser\n",
    "chain.invoke(\"Tell me a bit about Retrieval Augmented Generation (RAG) in less than 100 words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44719a8-8b7b-42e7-8b30-0e690cac33c4",
   "metadata": {},
   "source": [
    "#### PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c41e7dde-dd6a-4761-86d4-d85cf66d731a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"pdf_lists/model_evaluation_selection_ML.pdf\")\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b73a1b84-13ee-48d3-ac4d-187599f851ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aaee960b-5530-4e2e-867e-3492662beb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Choose a page\n",
    "page = pages[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cbb32d1-0703-4137-ad4b-2d99a9e9ce1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Introduction: Essential Model Evaluation Terms and Techniques\n",
      "Machine learning has become a central part of our life – as consumers, customers, and hopefully\n",
      "as researchers and practitioners. Whether we are applying predictive modeling techniques to our\n",
      "research or business problems, I believe we have one thing in common: We want to make \"good\"\n",
      "predictions. Fitting a model to our training data is one thing, but how do we know that it generalizes\n",
      "well to unseen data? How do we know that it does not simply memorize the data we fed it and fails to\n",
      "make good predictions on future samples, samples that it has not seen before? And how do we select\n",
      "a good model in the ﬁrst place? Maybe a different learning algorithm could be better-suited for the\n",
      "problem at hand?\n",
      "Model evaluation is certainly not just the end point of our machine learning pipeline. Before we\n",
      "handle any data, we want to plan ahead and use techniques that are suited for our purposes. In this\n",
      "article, we will go over a selecti\n"
     ]
    }
   ],
   "source": [
    "### View the content of the first chapter\n",
    "print(page.page_content[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fbf7c5e-d6b3-45b6-a138-2d62bdbca4ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'pdf_lists/model_evaluation_selection_ML.pdf',\n",
       " 'page': 3,\n",
       " 'page_label': '4'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873fde10-3ed0-42c0-9594-8f4864e74312",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### URLs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc33086d-08b3-41b4-ac34-fcb00cc91607",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://github.com/ayoub-berdeddouch/mlops-journey/blob/main/monitoring-05.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9fa6db3-029d-4861-8ff5-43f90ab95598",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "mlops-journey/monitoring-05.md at main · ayoub-berdeddouch/mlops-journey · GitHub\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Skip to content\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Navigation Menu\n",
      "\n",
      "Toggle navigation\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            Sign in\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        Product\n",
      "        \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "GitHub Copilot\n",
      "        Write better code with AI\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "GitHub Advanced Security\n",
      "        Find and fix vulnerabilities\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Actions\n",
      "        Automate any workflow\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Codespaces\n",
      "        Instant dev environments\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Issues\n",
      "        Plan and track work\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Code Review\n",
      "        Manage code changes\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Discussions\n",
      "        Collaborate outside of code\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Code Search\n",
      "        Find more, search less\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Explore\n",
      "\n",
      "\n",
      "\n",
      "      All features\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "      Documentation\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      GitHub Skills\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Blog\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        Solutions\n",
      "        \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "By c\n"
     ]
    }
   ],
   "source": [
    "docs = loader.load()\n",
    "print(docs[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6803ddbf-63e8-410c-a6d3-02ec2c778af7",
   "metadata": {},
   "source": [
    "#### Document Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f27847a-0dac-421c-aea5-9e7fc39cd61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "\n",
    "chunk_size =26\n",
    "chunk_overlap = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60595485-a4ca-4176-8970-05ab7a5539e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "c_splitter = CharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5242a413-ae3f-459b-829e-b218bfa02b96",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0b06c01-1e04-4fd1-9882-ad94426f8bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_text = \"\"\"When writing documents, writers will use document structure to group content. \\\n",
    "        This can convey to the reader, which idea's are related. For example, closely related ideas \\\n",
    "        are in sentances. Similar ideas are in paragraphs. Paragraphs form a document. \\n\\n  \\\n",
    "        Paragraphs are often delimited with a carriage return or two carriage returns. \\\n",
    "        Carriage returns are the \"backslash n\" you see embedded in this string. \\\n",
    "        Sentences have a period at the end, but also, have a space.\\\n",
    "        and words are separated by space.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d23714f4-98f7-4552-8a2c-ecd78dc101ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_splitter = CharacterTextSplitter(\n",
    "    chunk_size=450,\n",
    "    chunk_overlap=0,\n",
    "    separator = ' '\n",
    ")\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=450,\n",
    "    chunk_overlap=0, \n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80afee36-43d4-4eea-a12f-10cbeecb2e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['When writing documents, writers will use document structure to group content. This can convey to the reader, which idea\\'s are related. For example, closely related ideas are in sentances. Similar ideas are in paragraphs. Paragraphs form a document. \\n\\n Paragraphs are often delimited with a carriage return or two carriage returns. Carriage returns are the \"backslash n\" you see embedded in this string. Sentences have a period at the end, but also,',\n",
       " 'have a space. and words are separated by space.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_splitter.split_text(some_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bad0e89f-9715-4b70-a7d9-58c6f1e79974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"When writing documents, writers will use document structure to group content.         This can convey to the reader, which idea's are related. For example, closely related ideas         are in sentances. Similar ideas are in paragraphs. Paragraphs form a document.\",\n",
       " 'Paragraphs are often delimited with a carriage return or two carriage returns.         Carriage returns are the \"backslash n\" you see embedded in this string.         Sentences have a period at the end, but also, have a space.        and words are separated by space.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_splitter.split_text(some_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24978e9e-6107-42b4-af59-85bbd7f51c60",
   "metadata": {},
   "source": [
    "Let's reduce the chunk size a bit and add a period to our separators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa8288db-88ad-4742-a6fd-887574b5bb4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"When writing documents, writers will use document structure to group content.         This can convey to the reader, which idea's are related. For\",\n",
       " 'example, closely related ideas         are in sentances. Similar ideas are in paragraphs. Paragraphs form a document.',\n",
       " 'Paragraphs are often delimited with a carriage return or two carriage returns.         Carriage returns are the \"backslash n\" you see',\n",
       " 'embedded in this string.         Sentences have a period at the end, but also, have a space.        and words are separated by space.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=150,\n",
    "    chunk_overlap=0,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"\\. \", \" \", \"\"]\n",
    ")\n",
    "r_splitter.split_text(some_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "799ebf05-15e4-47fd-900c-d82a33654902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"When writing documents, writers will use document structure to group content.         This can convey to the reader, which idea's are related. For\",\n",
       " 'example, closely related ideas         are in sentances. Similar ideas are in paragraphs. Paragraphs form a document.',\n",
       " 'Paragraphs are often delimited with a carriage return or two carriage returns.         Carriage returns are the \"backslash n\" you see',\n",
       " 'embedded in this string.         Sentences have a period at the end, but also, have a space.        and words are separated by space.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=150,\n",
    "    chunk_overlap=0,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"]\n",
    ")\n",
    "r_splitter.split_text(some_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df9b7dea-639e-4344-b901-96779e26b1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=150,\n",
    "    length_function=len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "55811717-33a2-4044-aa56-91bea77fdfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1bad4268-c079-4cf3-945a-109d2d75269d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 0, 'page_label': '1'}, page_content='Model Evaluation, Model Selection, and Algorithm\\nSelection in Machine Learning\\nSebastian Raschka\\nUniversity of Wisconsin–Madison\\nDepartment of Statistics\\nNovember 2018\\nsraschka@wisc.edu\\nAbstract\\nThe correct use of model evaluation, model selection, and algorithm selection\\ntechniques is vital in academic machine learning research as well as in many\\nindustrial settings. This article reviews different techniques that can be used for\\neach of these three subtasks and discusses the main advantages and disadvantages\\nof each technique with references to theoretical and empirical studies. Further,\\nrecommendations are given to encourage best yet feasible practices in research and\\napplications of machine learning. Common methods such as the holdout method\\nfor model evaluation and selection are covered, which are not recommended\\nwhen working with small datasets. Different ﬂavors of the bootstrap technique\\nare introduced for estimating the uncertainty of performance estimates, as an'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 0, 'page_label': '1'}, page_content='are introduced for estimating the uncertainty of performance estimates, as an\\nalternative to conﬁdence intervals via normal approximation if bootstrapping is\\ncomputationally feasible. Common cross-validation techniques such as leave-one-\\nout cross-validation and k-fold cross-validation are reviewed, the bias-variance\\ntrade-off for choosing kis discussed, and practical tips for the optimal choice of\\nkare given based on empirical evidence. Different statistical tests for algorithm\\ncomparisons are presented, and strategies for dealing with multiple comparisons\\nsuch as omnibus tests and multiple-comparison corrections are discussed. Finally,\\nalternative methods for algorithm selection, such as the combined F-test 5x2 cross-\\nvalidation and nested cross-validation, are recommended for comparing machine\\nlearning algorithms when datasets are small.\\narXiv:1811.12808v3  [cs.LG]  11 Nov 2020'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 1, 'page_label': '2'}, page_content='Contents\\n1 Introduction: Essential Model Evaluation Terms and Techniques 4\\n1.1 Performance Estimation: Generalization Performance vs. Model Selection . . . . . 4\\n1.2 Assumptions and Terminology . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n1.3 Resubstitution Validation and the Holdout Method . . . . . . . . . . . . . . . . . . 7\\n1.4 Stratiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\\n1.5 Holdout Validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\\n1.6 Pessimistic Bias . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\\n1.7 Conﬁdence Intervals via Normal Approximation . . . . . . . . . . . . . . . . . . . 10\\n2 Bootstrapping and Uncertainties 11\\n2.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\\n2.2 Resampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 1, 'page_label': '2'}, page_content='2.2 Resampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\\n2.3 Repeated Holdout Validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\n2.4 The Bootstrap Method and Empirical Conﬁdence Intervals . . . . . . . . . . . . . 15\\n3 Cross-validation and Hyperparameter Optimization 20\\n3.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\n3.2 About Hyperparameters and Model Selection . . . . . . . . . . . . . . . . . . . . 21\\n3.3 The Three-Way Holdout Method for Hyperparameter Tuning . . . . . . . . . . . . 22\\n3.4 Introduction to k-fold Cross-Validation . . . . . . . . . . . . . . . . . . . . . . . . 24\\n3.5 Special Cases: 2-Fold and Leave-One-Out Cross-Validation . . . . . . . . . . . . . 26\\n3.6 k-fold Cross-Validation and the Bias-Variance Trade-off . . . . . . . . . . . . . . 28\\n3.7 Model Selection via k-fold Cross-Validation . . . . . . . . . . . . . . . . . . . . . 30'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 1, 'page_label': '2'}, page_content='3.7 Model Selection via k-fold Cross-Validation . . . . . . . . . . . . . . . . . . . . . 30\\n3.8 A Note About Model Selection and Large Datasets . . . . . . . . . . . . . . . . . 30\\n3.9 A Note About Feature Selection During Model Selection . . . . . . . . . . . . . . 30\\n3.10 The Law of Parsimony . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\\n3.11 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\\n4 Algorithm Comparison 34\\n4.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\\n4.2 Testing the Difference of Proportions . . . . . . . . . . . . . . . . . . . . . . . . . 34\\n4.3 Comparing Two Models with the McNemar Test . . . . . . . . . . . . . . . . . . . 35\\n4.4 Exact p-Values via the Binomial Test . . . . . . . . . . . . . . . . . . . . . . . . . 37\\n4.5 Multiple Hypotheses Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 1, 'page_label': '2'}, page_content='4.5 Multiple Hypotheses Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\\n4.6 Cochran’s QTest for Comparing the Performance of Multiple Classiﬁers . . . . . . 39\\n4.7 The F-test for Comparing Multiple Classiﬁers . . . . . . . . . . . . . . . . . . . . 41\\n4.8 Comparing Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\\n4.9 Resampled Paired t-Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\\n4.10 k-fold Cross-validated Paired t-Test . . . . . . . . . . . . . . . . . . . . . . . . . 44\\n2'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 2, 'page_label': '3'}, page_content='4.11 Dietterich’s 5x2-Fold Cross-Validated Paired t-Test . . . . . . . . . . . . . . . . . 44\\n4.12 Alpaydin’s Combined 5x2cv F-test . . . . . . . . . . . . . . . . . . . . . . . . . 45\\n4.13 Effect size . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\\n4.14 Nested Cross-Validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\\n4.15 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\\n4.16 Acknowledgments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\\n3'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 3, 'page_label': '4'}, page_content='1 Introduction: Essential Model Evaluation Terms and Techniques\\nMachine learning has become a central part of our life – as consumers, customers, and hopefully\\nas researchers and practitioners. Whether we are applying predictive modeling techniques to our\\nresearch or business problems, I believe we have one thing in common: We want to make \"good\"\\npredictions. Fitting a model to our training data is one thing, but how do we know that it generalizes\\nwell to unseen data? How do we know that it does not simply memorize the data we fed it and fails to\\nmake good predictions on future samples, samples that it has not seen before? And how do we select\\na good model in the ﬁrst place? Maybe a different learning algorithm could be better-suited for the\\nproblem at hand?\\nModel evaluation is certainly not just the end point of our machine learning pipeline. Before we\\nhandle any data, we want to plan ahead and use techniques that are suited for our purposes. In this'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 3, 'page_label': '4'}, page_content='handle any data, we want to plan ahead and use techniques that are suited for our purposes. In this\\narticle, we will go over a selection of these techniques, and we will see how they ﬁt into the bigger\\npicture, a typical machine learning workﬂow.\\n1.1 Performance Estimation: Generalization Performance vs. Model Selection\\nLet us consider the obvious question, \"How do we estimate the performance of a machine learning\\nmodel?\" A typical answer to this question might be as follows: \"First, we feed the training data\\nto our learning algorithm to learn a model. Second, we predict the labels of our test set. Third,\\nwe count the number of wrong predictions on the test dataset to compute the model’s prediction\\naccuracy.\" Depending on our goal, however, estimating the performance of a model is not that trivial,\\nunfortunately. Maybe we should address the previous question from a different angle: \"Why do we'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 3, 'page_label': '4'}, page_content='unfortunately. Maybe we should address the previous question from a different angle: \"Why do we\\ncare about performance estimates at all?\" Ideally, the estimated performance of a model tells how\\nwell it performs on unseen data – making predictions on future data is often the main problem we\\nwant to solve in applications of machine learning or the development of new algorithms. Typically,\\nmachine learning involves a lot of experimentation, though – for example, the tuning of the internal\\nknobs of a learning algorithm, the so-called hyperparameters. Running a learning algorithm over a\\ntraining dataset with different hyperparameter settings will result in different models. Since we are\\ntypically interested in selecting the best-performing model from this set, we need to ﬁnd a way to\\nestimate their respective performances in order to rank them against each other.\\nGoing one step beyond mere algorithm ﬁne-tuning, we are usually not only experimenting with'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 3, 'page_label': '4'}, page_content='Going one step beyond mere algorithm ﬁne-tuning, we are usually not only experimenting with\\nthe one single algorithm that we think would be the \"best solution\" under the given circumstances.\\nMore often than not, we want to compare different algorithms to each other, oftentimes in terms of\\npredictive and computational performance. Let us summarize the main points why we evaluate the\\npredictive performance of a model:\\n1. We want to estimate the generalization performance, the predictive performance of our\\nmodel on future (unseen) data.\\n2. We want to increase the predictive performance by tweaking the learning algorithm and\\nselecting the best performing model from a given hypothesis space.\\n3. We want to identify the machine learning algorithm that is best-suited for the problem at\\nhand; thus, we want to compare different algorithms, selecting the best-performing one as\\nwell as the best performing model from the algorithm’s hypothesis space.'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 3, 'page_label': '4'}, page_content='well as the best performing model from the algorithm’s hypothesis space.\\nAlthough these three sub-tasks listed above have all in common that we want to estimate the\\nperformance of a model, they all require different approaches. We will discuss some of the different\\nmethods for tackling these sub-tasks in this article.\\nOf course, we want to estimate the future performance of a model as accurately as possible. However,\\nwe shall note that biased performance estimates are perfectly okay in model selection and algorithm\\nselection if the bias affects all models equally. If we rank different models or algorithms against each\\nother in order to select the best-performing one, we only need to know their \"relative\" performance.\\nFor example, if all performance estimates are pessimistically biased, and we underestimate their\\nperformances by 10%, it will not affect the ranking order. More concretely, if we obtaind three\\nmodels with prediction accuracy estimates such as\\nM2: 75% > M1: 70% > M3: 65%,\\n4'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 4, 'page_label': '5'}, page_content='we would still rank them the same way if we added a 10% pessimistic bias:\\nM2: 65% > M1: 60% > M3: 55%.\\nHowever, note that if we reported the generalization (future prediction) accuracy of the best ranked\\nmodel (M2) to be 65%, this would obviously be quite inaccurate. Estimating the absolute performance\\nof a model is probably one of the most challenging tasks in machine learning.\\n1.2 Assumptions and Terminology\\nModel evaluation is certainly a complex topic. To make sure that we do not diverge too much from\\nthe core message, let us make certain assumptions and go over some of the technical terms that we\\nwill use throughout this article.\\ni.i.d. We assume that the training examples are i.i.d (independent and identically distributed), which\\nmeans that all examples have been drawn from the same probability distribution and are statistically\\nindependent from each other. A scenario where training examples are not independent would be\\nworking with temporal data or time-series data.'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 4, 'page_label': '5'}, page_content='independent from each other. A scenario where training examples are not independent would be\\nworking with temporal data or time-series data.\\nSupervised learning and classiﬁcation. This article focusses on supervised learning, a subcategory\\nof machine learning where the target values are known in a given dataset. Although many concepts\\nalso apply to regression analysis, we will focus on classiﬁcation, the assignment of categorical target\\nlabels to the training and test examples.\\n0-1 loss and prediction accuracy. In the following article, we will focus on the prediction accuracy,\\nwhich is deﬁned as the number of all correct predictions divided by the number of examples in the\\ndataset. We compute the prediction accuracy as the number of correct predictions divided by the\\nnumber of examples n. Or in more formal terms, we deﬁne the prediction accuracy ACC as\\nACC = 1 −ERR, (1)\\nwhere the prediction error, ERR, is computed as the expected value of the 0-1 loss over nexamples\\nin a dataset S:'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 4, 'page_label': '5'}, page_content='ACC = 1 −ERR, (1)\\nwhere the prediction error, ERR, is computed as the expected value of the 0-1 loss over nexamples\\nin a dataset S:\\nERRS = 1\\nn\\nn∑\\ni=1\\nL( ˆyi,yi). (2)\\nThe 0-1 loss L(·) is deﬁned as\\nL( ˆyi,yi) =\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\n0 if ˆyi = yi\\n1 if ˆyi ̸= yi,\\n(3)\\nwhere yi is the ith true class label and ˆyi the ith predicted class label, respectively. Our objective is to\\nlearn a model hthat has a good generalization performance. Such a model maximizes the prediction\\naccuracy or, vice versa, minimizes the probability,C(h), of making a wrong prediction:\\nC(h) = Pr\\n(x,y)∼D\\n[h(x) ̸= y]. (4)\\nHere, Dis the generating distribution the dataset has been drawn from, x is the feature vector of a\\ntraining example with class label y.\\nLastly, since this article mostly refers to the prediction accuracy (instead of the error), we deﬁne\\nKronecker’s Delta function:\\nδ\\n(\\nL( ˆyi,yi)\\n)\\n= 1 −L( ˆyi,yi), (5)\\n5'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 5, 'page_label': '6'}, page_content='such that\\nδ\\n(\\nL( ˆyi,yi)\\n)\\n= 1 if ˆyi = yi (6)\\nand\\nδ\\n(\\nL( ˆyi,yi)\\n)\\n= 0 if ˆyi ̸= yi. (7)\\nBias. Throughout this article, the term bias refers to the statistical bias (in contrast to the bias in a\\nmachine learning system). In general terms, the bias of an estimator ˆβis the difference between its\\nexpected value E[ ˆβ] and the true value of a parameter βbeing estimated:\\nBias = E[ ˆβ] −β. (8)\\nThus, if Bias = E[ ˆβ] −β = 0, then ˆβis an unbiased estimator of β. More concretely, we compute\\nthe prediction bias as the difference between the expected prediction accuracy of a model and its\\ntrue prediction accuracy. For example, if we computed the prediction accuracy on the training set,\\nthis would be an optimistically biased estimate of the absolute accuracy of a model since it would\\noverestimate its true accuracy.\\nVariance. The variance is simply the statistical variance of the estimator ˆβand its expected value\\nE[ ˆβ], for instance, the squared difference of the :\\nVariance = E\\n[(ˆβ−E[ ˆβ]\\n)2]'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 5, 'page_label': '6'}, page_content='E[ ˆβ], for instance, the squared difference of the :\\nVariance = E\\n[(ˆβ−E[ ˆβ]\\n)2]\\n. (9)\\nThe variance is a measure of the variability of a model’s predictions if we repeat the learning process\\nmultiple times with small ﬂuctuations in the training set. The more sensitive the model-building\\nprocess is towards these ﬂuctuations, the higher the variance.1\\nFinally, let us disambiguate the terms model, hypothesis, classiﬁer, learning algorithms, and parame-\\nters:\\nTarget function. In predictive modeling, we are typically interested in modeling a particular\\nprocess; we want to learn or approximate a speciﬁc, unknown function. The target function f(x) = y\\nis the true function f(·) that we want to model.\\nHypothesis. A hypothesis is a certain function that we believe (or hope) is similar to the true\\nfunction, the target function f(·) that we want to model. In context of spam classiﬁcation, it would\\nbe a classiﬁcation rule we came up with that allows us to separate spam from non-spam emails.'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 5, 'page_label': '6'}, page_content='be a classiﬁcation rule we came up with that allows us to separate spam from non-spam emails.\\nModel. In the machine learning ﬁeld, the termshypothesis and model are often used interchangeably.\\nIn other sciences, these terms can have different meanings: A hypothesis could be the \"educated\\nguess\" by the scientist, and the model would be the manifestation of this guess to test this hypothesis.\\nLearning algorithm. Again, our goal is to ﬁnd or approximate the target function, and the learning\\nalgorithm is a set of instructions that tried to model the target function using a training dataset. A\\nlearning algorithm comes with a hypothesis space, the set of possible hypotheses it can explore to\\nmodel the unknown target function by formulating the ﬁnal hypothesis.\\n1For a more detailed explanation of the bias-variance decomposition of loss functions, and how\\nhigh variance relates to overﬁtting and high bias relates to underﬁtting, please see my lecture notes I'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 5, 'page_label': '6'}, page_content='high variance relates to overﬁtting and high bias relates to underﬁtting, please see my lecture notes I\\nmade available at https://github.com/rasbt/stat479-machine-learning-fs18/blob/master/08_\\neval-intro/08_eval-intro_notes.pdf.\\n6'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 6, 'page_label': '7'}, page_content='Hyperparameters. Hyperparameters are the tuning parameters of a machine learning algorithm –\\nfor example, the regularization strength of an L2 penalty in the loss function of logistic regression, or\\na value for setting the maximum depth of a decision tree classiﬁer. In contrast, model parameters\\nare the parameters that a learning algorithm ﬁts to the training data – the parameters of the model\\nitself. For example, the weight coefﬁcients (or slope) of a linear regression line and its bias term\\n(here: y-axis intercept) are model parameters.\\n1.3 Resubstitution Validation and the Holdout Method\\nThe holdout method is inarguably the simplest model evaluation technique; it can be summarized as\\nfollows. First, we take a labeled dataset and split it into two parts: A training and a test set. Then, we\\nﬁt a model to the training data and predict the labels of the test set. The fraction of correct predictions,'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 6, 'page_label': '7'}, page_content='ﬁt a model to the training data and predict the labels of the test set. The fraction of correct predictions,\\nwhich can be computed by comparing the predicted labels to the ground truth labels of the test set,\\nconstitutes our estimate of the model’s prediction accuracy. Here, it is important to note that we\\ndo not want to train and evaluate a model on the same training dataset (this is calledresubstitution\\nvalidation or resubstitution evaluation), since it would typically introduce a very optimistic bias due\\nto overﬁtting. In other words, we cannot tell whether the model simply memorized the training data,\\nor whether it generalizes well to new, unseen data. (On a side note, we can estimate this so-called\\noptimism bias as the difference between the training and test accuracy.)\\nTypically, the splitting of a dataset into training and test sets is a simple process ofrandom subsam-\\npling. We assume that all data points have been drawn from the same probability distribution (with'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 6, 'page_label': '7'}, page_content='pling. We assume that all data points have been drawn from the same probability distribution (with\\nrespect to each class). And we randomly choose 2/3 of these samples for the training set and 1/3\\nof the samples for the test set. Note that there are two problems with this approach, which we will\\ndiscuss in the next sections.\\n1.4 Stratiﬁcation\\nWe have to keep in mind that a dataset represents a random sample drawn from a probability\\ndistribution, and we typically assume that this sample is representative of the true population – more\\nor less. Now, further subsampling without replacement alters the statistic (mean, proportion, and\\nvariance) of the sample. The degree to which subsampling without replacement affects the statistic of\\na sample is inversely proportional to the size of the sample. Let us have a look at an example using\\nthe Iris dataset 2, which we randomly divide into 2/3 training data and 1/3 test data as illustrated in'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 6, 'page_label': '7'}, page_content='the Iris dataset 2, which we randomly divide into 2/3 training data and 1/3 test data as illustrated in\\nFigure 1. (The source code for generating this graphic is available on GitHub3.)\\nWhen we randomly divide a labeled dataset into training and test sets, we violate the assumption\\nof statistical independence. The Iris datasets consists of 50 Setosa, 50 Versicolor, and 50 Virginica\\nﬂowers; the ﬂower species are distributed uniformly:\\n• 33.3% Setosa\\n• 33.3% Versicolor\\n• 33.3% Virginica\\nIf a random function assigns 2/3 of the ﬂowers (100) to the training set and 1/3 of the ﬂowers (50) to\\nthe test set, it may yield the following (also shown in Figure 1):\\n• training set →38 ×Setosa, 28 ×Versicolor, 34×Virginica\\n• test set →12 ×Setosa, 22 ×Versicolor, 16×Virginica\\nAssuming that the Iris dataset is representative of the true population (for instance, assuming that\\niris ﬂower species are distributed uniformly in nature), we just created two imbalanced datasets with'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 6, 'page_label': '7'}, page_content='iris ﬂower species are distributed uniformly in nature), we just created two imbalanced datasets with\\nnon-uniform class distributions. The class ratio that the learning algorithm uses to learn the model\\nis \"38% / 28% / 34%.\" The test dataset that is used for evaluating the model is imbalanced as well,\\nand even worse, it is balanced in the \"opposite\" direction: \"24% / 44% / 32%.\" Unless the learning\\nalgorithm is completely insensitive to these perturbations, this is certainly not ideal. The problem\\nbecomes even worse if a dataset has a high class imbalance upfront, prior to the random subsampling.\\n2https://archive.ics.uci.edu/ml/datasets/iris\\n3https://github.com/rasbt/model-eval-article-supplementary/blob/master/code/iris-random-dist.ipynb\\n7'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 7, 'page_label': '8'}, page_content='Dataset before splitting (n = 150)\\nTest dataset (n = 50)\\nThis work by Sebastian Raschka is licensed under a \\nCreative Commons Attribution 4.0 International License.\\nSepal Length [cm]\\nTraining dataset (n = 100)\\nSepal Length [cm]\\nFigure 1: Distribution of Iris ﬂower classes upon random subsampling into training and test sets.\\nIn the worst-case scenario, the test set may not contain any instance of a minority class at all. Thus,\\na recommended practice is to divide the dataset in a stratiﬁed fashion. Here, stratiﬁcation simply\\nmeans that we randomly split a dataset such that each class is correctly represented in the resulting\\nsubsets (the training and the test set) – in other words, stratiﬁcation is an approach to maintain the\\noriginal class proportion in resulting subsets.\\nIt shall be noted that random subsampling in non-stratiﬁed fashion is usually not a big concern when\\nworking with relatively large and balanced datasets. However, in my opinion, stratiﬁed resampling is'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 7, 'page_label': '8'}, page_content='working with relatively large and balanced datasets. However, in my opinion, stratiﬁed resampling is\\nusually beneﬁcial in machine learning applications. Moreover, stratiﬁed sampling is incredibly easy\\nto implement, and Ron Kohavi provides empirical evidence [Kohavi, 1995] that stratiﬁcation has a\\npositive effect on the variance and bias of the estimate in k-fold cross-validation, a technique that\\nwill be discussed later in this article.\\n1.5 Holdout Validation\\nBefore diving deeper into the pros and cons of the holdout validation method, Figure 2 provides a\\nvisual summary of this method that will be discussed in the following text.\\nStep 1. First, we randomly divide our available data into two subsets: a training and a test set.\\nSetting test data aside is a work-around for dealing with the imperfections of a non-ideal world, such\\nas limited data and resources, and the inability to collect more data from the generating distribution.'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 7, 'page_label': '8'}, page_content='as limited data and resources, and the inability to collect more data from the generating distribution.\\nHere, the test set shall represent new, unseen data to the model; it is important that the test set is only\\nused once to avoid introducing bias when we estimating the generalization performance. Typically,\\nwe assign 2/3 to the training set and 1/3 of the data to the test set. Other common training/test splits\\nare 60/40, 70/30, or 80/20 – or even 90/10 if the dataset is relatively large.\\nStep 2. After setting test examples aside, we pick a learning algorithm that we think could be\\nappropriate for the given problem. As a quick reminder regarding theHyperparameter Valuesdepicted\\nin Figure 2, hyperparameters are the parameters of our learning algorithm, or meta-parameters. And\\nwe have to specify these hyperparameter values manually – the learning algorithm does not learn\\nthese from the training data in contrast to the actual model parameters. Since hyperparameters are not\\n8'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 8, 'page_label': '9'}, page_content='Learning  \\nAlgorithm\\nHyperparameter  \\nValues\\nModel\\nPrediction\\nTest Labels\\nPerformance\\nModel\\nLearning  \\nAlgorithm\\nHyperparameter  \\nValues Final \\nModel\\n2\\n3\\n4\\n1\\nTest Labels\\nTest Data\\nTraining Data\\nTraining Labels\\nData\\nLabels\\nData\\nLabels\\nTraining Data\\nTraining Labels\\nTest Data\\nFigure 2: Visual summary of the holdout validation method.\\nlearned during model ﬁtting, we need some sort of \"extra procedure\" or \"external loop\" to optimize\\nthese separately – this holdout approach is ill-suited for the task. So, for now, we have to go with some\\nﬁxed hyperparameter values – we could use our intuition or the default parameters of an off-the-shelf\\nalgorithm if we are using an existing machine learning library.\\nStep 3. After the learning algorithm ﬁt a model in the previous step, the next question is: How\\n\"good\" is the performance of the resulting model? This is where the independent test set comes into'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 8, 'page_label': '9'}, page_content='\"good\" is the performance of the resulting model? This is where the independent test set comes into\\nplay. Since the learning algorithm has not \"seen\" this test set before, it should provide a relatively\\nunbiased estimate of its performance on new, unseen data. Now, we take this test set and use the\\nmodel to predict the class labels. Then, we take the predicted class labels and compare them to the\\n\"ground truth,\" the correct class labels, to estimate the models generalization accuracy or error.\\nStep 4. Finally, we obtained an estimate of how well our model performs on unseen data. So, there\\nis no reason for with-holding the test set from the algorithm any longer. Since we assume that our\\nsamples are i.i.d., there is no reason to assume the model would perform worse after feeding it all the\\navailable data. As a rule of thumb, the model will have a better generalization performance if the\\nalgorithms uses more informative data – assuming that it has not reached its capacity, yet.\\n9'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 9, 'page_label': '10'}, page_content='1.6 Pessimistic Bias\\nSection 1.3 (Resubstitution Validation and the Holdout Method) referenced two types of problems\\nthat occur when a dataset is split into separate training and test sets. The ﬁrst problem that occurs is\\nthe violation of independence and the changing class proportions upon subsampling (discussed in\\nSection 1.4). Walking through the holdout validation method (Section 1.5) touched upon a second\\nproblem we encounter upon subsampling a dataset: Step 4 mentioned capacity of a model, and\\nwhether additional data could be useful or not. To follow up on the capacity issue: If a model has\\nnot reached its capacity, the performance estimate would be pessimistically biased. This assumes\\nthat the algorithm could learn a better model if it was given more data – by splitting off a portion of\\nthe dataset for testing, we withhold valuable data for estimating the generalization performance (for'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 9, 'page_label': '10'}, page_content='the dataset for testing, we withhold valuable data for estimating the generalization performance (for\\ninstance, the test dataset). To address this issue, one might ﬁt the model to the whole dataset after\\nestimating the generalization performance (see Figure 2 step 4). However, using this approach, we\\ncannot estimate its generalization performance of the reﬁt model, since we have now \"burned\" the\\ntest dataset. It is a dilemma that we cannot really avoid in real-world application, but we should be\\naware that our estimate of the generalization performance may be pessimistically biased if only a\\nportion of the dataset, the training dataset, is used for model ﬁtting (this is especially affects models\\nﬁt to relatively small datasets).\\n1.7 Conﬁdence Intervals via Normal Approximation\\nUsing the holdout method as described in Section 1.5, we computed a point estimate of the gener-\\nalization performance of a model. Certainly, a conﬁdence interval around this estimate would not'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 9, 'page_label': '10'}, page_content='alization performance of a model. Certainly, a conﬁdence interval around this estimate would not\\nonly be more informative and desirable in certain applications, but our point estimate could be quite\\nsensitive to the particular training/test split (for instance, suffering from high variance). A simple\\napproach for computing conﬁdence intervals of the predictive accuracy or error of a model is via the\\nso-called normal approximation. Here, we assume that the predictions follow a normal distribution,\\nto compute the conﬁdence interval on the mean on a single training-test split under the central limit\\ntheorem. The following text illustrates how this works.\\nAs discussed earlier, we compute the prediction accuracy on a dataset S(here: test set) of size nas\\nfollows:\\nACCS = 1\\nn\\nn∑\\ni=1\\nδ(L( ˆyi,yi)), (10)\\nwhere L(·) is the 0-1 loss function (Equation 3), and ndenotes the number of samples in the test'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 9, 'page_label': '10'}, page_content='follows:\\nACCS = 1\\nn\\nn∑\\ni=1\\nδ(L( ˆyi,yi)), (10)\\nwhere L(·) is the 0-1 loss function (Equation 3), and ndenotes the number of samples in the test\\ndataset. Further, let ˆyi be the predicted class label and yi be the ground truth class label of the ith test\\nexample, respectively. So, we could now consider each prediction as a Bernoulli trial, and the number\\nof correct predictions X is following a binomial distribution X ∼(n,p) with ntest examples, k\\ntrials, and the probability of success p, where n∈N and p∈[0,1] :\\nf(k; n,p) = Pr(X = k) =\\n(n\\nk\\n)\\npk(1 −p)n−k, (11)\\nfor k= 0,1,2,...,n , where\\n(n\\nk\\n)\\n= n!\\nk!(n−k)!. (12)\\n(Here, pis the probability of success, and consequently, (1 −p) is the probability of failure – a wrong\\nprediction.)\\nNow, the expected number of successes is computed as µ= np, or more concretely, if the model has\\na 50% success rate, we expect 20 out of 40 predictions to be correct. The estimate has a variance of\\nσ2 = np(1 −p) = 10 (13)\\n10'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 10, 'page_label': '11'}, page_content='and a standard deviation of\\nσ=\\n√\\nnp(1 −p) = 3.16. (14)\\nSince we are interested in the average number of successes, not its absolute value, we compute the\\nvariance of the accuracy estimate as\\nσ2 = 1\\nnACCS(1 −ACCS), (15)\\nand the respective standard deviation as\\nσ=\\n√\\n1\\nnACCS(1 −ACCS). (16)\\nUnder the normal approximation, we can then compute the conﬁdence interval as\\nACCS ±z\\n√\\n1\\nnACCS(1 −ACCS), (17)\\nwhere αis the error quantile and z is the 1 −α\\n2 quantile of a standard normal distribution. For a\\ntypical conﬁdence interval of 95%, (α= 0.05), we have z= 1.96.\\nIn practice, however, I would rather recommend repeating the training-test split multiple times to\\ncompute the conﬁdence interval on the mean estimate (for instance, averaging the individual runs).\\nIn any case, one interesting take-away for now is that having fewer samples in the test set increases\\nthe variance (see nin the denominator above) and thus widens the conﬁdence interval. Conﬁdence'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 10, 'page_label': '11'}, page_content='the variance (see nin the denominator above) and thus widens the conﬁdence interval. Conﬁdence\\nintervals and estimating uncertainty will be discussed in more detail in the next section, Section 2.\\n2 Bootstrapping and Uncertainties\\n2.1 Overview\\nThe previous section (Section 1, Introduction: Essential Model Evaluation Terms and Techniques)\\nintroduced the general ideas behind model evaluation in supervised machine learning. We discussed\\nthe holdout method, which helps us to deal with real world limitations such as limited access to new,\\nlabeled data for model evaluation. Using the holdout method, we split our dataset into two parts: A\\ntraining and a test set. First, we provide the training data to a supervised learning algorithm. The\\nlearning algorithm builds a model from the training set of labeled observations. Then, we evaluate the\\npredictive performance of the model on an independent test set that shall represent new, unseen data.'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 10, 'page_label': '11'}, page_content='predictive performance of the model on an independent test set that shall represent new, unseen data.\\nAlso, we brieﬂy introduced the normal approximation, which requires us to make certain assumptions\\nthat allow us to compute conﬁdence intervals for modeling the uncertainty of our performance\\nestimate based on a single test set, which we have to take with a grain of salt.\\nThis section introduces some of the advanced techniques for model evaluation. We will start by\\ndiscussing techniques for estimating the uncertainty of our estimated model performance as well\\nas the model’s variance and stability. And after getting these basics under our belt, we will look at\\ncross-validation techniques for model selection in the next article in this series. As we remember from\\nSection 1, there are three related, yet different tasks or reasons why we care about model evaluation:\\n1. We want to estimate the generalization accuracy, the predictive performance of a model on\\nfuture (unseen) data.'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 10, 'page_label': '11'}, page_content='1. We want to estimate the generalization accuracy, the predictive performance of a model on\\nfuture (unseen) data.\\n2. We want to increase the predictive performance by tweaking the learning algorithm and\\nselecting the best-performing model from a given hypothesis space.\\n3. We want to identify the machine learning algorithm that is best-suited for the problem at\\nhand. Hence, we want to compare different algorithms, selecting the best-performing one as\\nwell as the best-performing model from the algorithm’s hypothesis space.\\n11'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 11, 'page_label': '12'}, page_content='(The code for generating the ﬁgures discussed in this section are available on GitHub4.)\\n2.2 Resampling\\nThe ﬁrst section of this article introduced the prediction accuracy or error measures of classiﬁcation\\nmodels. To compute the classiﬁcation error or accuracy on a dataset S, we deﬁned the following\\nequation:\\nERRS = 1\\nn\\nn∑\\ni=1\\nL\\n(\\nˆyi,yi\\n)\\n= 1 −ACCS. (18)\\nHere, L(·) represents the 0-1 loss, which is computed from a predicted class label ( ˆyi) and a true\\nclass label (yi) for i= 1,...,n in dataset S:\\nL( ˆyi,yi) =\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\n0 if ˆyi = yi\\n1 if ˆyi ̸= yi.\\n(19)\\nIn essence, the classiﬁcation error is simply the count of incorrect predictions divided by the number\\nof samples in the dataset. Vice versa, we compute the prediction accuracy as the number of correct\\npredictions divided by the number of samples.\\nNote that the concepts presented in this section also apply to other types of supervised learning,'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 11, 'page_label': '12'}, page_content='predictions divided by the number of samples.\\nNote that the concepts presented in this section also apply to other types of supervised learning,\\nsuch as regression analysis. To use the resampling methods presented in the following sections for\\nregression models, we swap the accuracy or error computation by, for example, the mean squared\\nerror (MSE):\\nMSES = 1\\nn\\nn∑\\ni=1\\n(ˆyi −yi)2. (20)\\nAs discussed in Section 1, performance estimates may suffer from bias and variance, and we are\\ninterested in ﬁnding a good trade-off. For instance, the resubstitution evaluation (ﬁtting a model to\\na training set and using the same training set for model evaluation) is heavily optimistically biased.\\nVice versa, withholding a large portion of the dataset as a test set may lead to pessimistically biased\\nestimates. While reducing the size of the test set may decrease this pessimistic bias, the variance of a\\nperformance estimates will most likely increase. An intuitive illustration of the relationship between'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 11, 'page_label': '12'}, page_content='performance estimates will most likely increase. An intuitive illustration of the relationship between\\nbias and variance is given in Figure 3. This section will introduce alternative resampling methods for\\nﬁnding a good balance between bias and variance for model evaluation and selection.\\nThe reason why a proportionally large test sets increase the pessimistic bias is that the model may\\nnot have reached its full capacity, yet. In other words, the learning algorithm could have formulated\\na more powerful, more generalizable hypothesis for classiﬁcation if it had seen more data. To\\ndemonstrate this effect, Figure 4 shows learning curves of a softmax classiﬁers, which were ﬁtted to\\nsmall subsets of the MNIST5 dataset.\\nTo generate the learning curves shown in Figure 4, 500 random samples of each of the ten classes\\nfrom MNIST – instances of the handwritten digits 0 to 9 – were drawn. The 5000-sample MNIST'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 11, 'page_label': '12'}, page_content='from MNIST – instances of the handwritten digits 0 to 9 – were drawn. The 5000-sample MNIST\\nsubset was then randomly divided into a 3500-sample training subset and a test set containing 1500\\nsamples while keeping the class proportions intact via stratiﬁcation. Finally, even smaller subsets\\nof the 3500-sample training set were produced via randomized, stratiﬁed splits, and these subsets\\nwere used to ﬁt softmax classiﬁers and the same 1500-sample test set was used to evaluate their\\nperformances (samples may overlap between these training subsets). Looking at the plot above, we\\ncan see two distinct trends. First, the resubstitution accuracy (training set) declines as the number of\\ntraining samples grows. Second, we observe an improving generalization accuracy (test set) with\\nan increasing training set size. These trends can likely be attributed to a reduction in overﬁtting. If\\n4https://github.com/rasbt/model-eval-article-supplementary/blob/master/code/resampling-and-kfold.ipynb'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 11, 'page_label': '12'}, page_content='4https://github.com/rasbt/model-eval-article-supplementary/blob/master/code/resampling-and-kfold.ipynb\\n5http://yann.lecun.com/exdb/mnist\\n12'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 12, 'page_label': '13'}, page_content='Low Variance\\n(Precise)\\nHigh Variance\\n(Not Precise)\\nLow Bias\\n(Accurate)\\nHigh Bias\\n(Not Accurate)\\nFigure 3: Illustration of bias and variance.\\nFigure 4: Learning curves of softmax classiﬁers ﬁt to MNIST.\\nthe training set is small, the algorithm is more likely picking up noise in the training set so that the\\nmodel fails to generalize well to data that it has not seen before. This observation also explains the\\npessimistic bias of the holdout method: A training algorithm may beneﬁt from more training data,\\ndata that was withheld for testing. Thus, after we evaluated a model, we may want to run the learning\\nalgorithm once again on the complete dataset before we use it in a real-world application.\\nNow, that we established the point of pessimistic biases for disproportionally large test sets, we may\\nask whether it is a good idea to decrease the size of the test set. Decreasing the size of the test set'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 12, 'page_label': '13'}, page_content='ask whether it is a good idea to decrease the size of the test set. Decreasing the size of the test set\\nbrings up another problem: It may result in a substantial variance of a model’s performance estimate.\\nThe reason is that it depends on which instances end up in training set, and which particular instances\\n13'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 13, 'page_label': '14'}, page_content='end up in test set. Keeping in mind that each time we resample a dataset, we alter the statistics of the\\ndistribution of the sample. Most supervised learning algorithms for classiﬁcation and regression as\\nwell as the performance estimates operate under the assumption that a dataset is representative of the\\npopulation that this dataset sample has been drawn from. As discussed in Section 1.4, stratiﬁcation\\nhelps with keeping the sample proportions intact upon splitting a dataset. However, the change in the\\nunderlying sample statistics along the features axes is still a problem that becomes more pronounced\\nif we work with small datasets, which is illustrated in Figure 5.\\nDataset \\nDistributionSample 1Sample 2Sample 3\\nTrain\\n(70%)\\nTest \\n(30%)\\nTrain\\n(70%)\\nTest \\n(30%)\\nn=1000n=100\\nReal World \\nDistribution\\nResampling\\nFigure 5: Repeated subsampling from a two-dimensional Gaussian distribution.\\n14'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 14, 'page_label': '15'}, page_content='2.3 Repeated Holdout Validation\\nOne way to obtain a more robust performance estimate that is less variant to how we split the data\\ninto training and test sets is to repeat the holdout method ktimes with different random seeds and\\ncompute the average performance over these krepetitions:\\nACCavg = 1\\nk\\nk∑\\nj=1\\nACCj, (21)\\nwhere ACCj is the accuracy estimate of the jth test set of size m,\\nACCj = 1 − 1\\nm\\nm∑\\ni=1\\nL\\n(\\nˆyi,yi\\n)\\n. (22)\\nThis repeated holdout procedure, sometimes also called Monte Carlo Cross-Validation, provides a\\nbetter estimate of how well our model may perform on a random test set, compared to the standard\\nholdout validation method. Also, it provides information about the model’s stability – how the model,\\nproduced by a learning algorithm, changes with different training set splits. Figure 6 shall illustrate\\nhow repeated holdout validation may look like for different training-test split using the Iris dataset to\\nﬁt to 3-nearest neighbors classiﬁers.\\n50/50 Train-Test Split'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 14, 'page_label': '15'}, page_content='ﬁt to 3-nearest neighbors classiﬁers.\\n50/50 Train-Test Split \\nAvg. Acc. 0.95\\n90/10 Train-Test Split \\nAvg. Acc. 0.96\\nFigure 6: Repeated holdout validation with 3-nearest neighbor classiﬁers ﬁt to the Iris dataset.\\nThe left subplot in Figure 6 was generated by performing 50 stratiﬁed training/test splits with 75\\nsamples in the test and training set each; a 3-nearest neighbors model was ﬁt to the training set and\\nevaluated on the test set in each repetition. The average accuracy of these 50 50/50 splits was 95%.\\nThe same procedure was used to produce the right subplot in Figure 6. Here, the test sets consisted of\\nonly 15 samples each due to the 90/10 splits, and the average accuracy over these 50 splits was 96%.\\nFigure 6 demonstrates two of the points that were previously discussed. First, we see that the variance\\nof our estimate increases as the size of the test set decreases. Second, we see a small increase in the'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 14, 'page_label': '15'}, page_content='of our estimate increases as the size of the test set decreases. Second, we see a small increase in the\\npessimistic bias when we decrease the size of the training set – we withhold more training data in the\\n50/50 split, which may be the reason why the average performance over the 50 splits is slightly lower\\ncompared to the 90/10 splits.\\nThe next section introduces an alternative method for evaluating a model’s performance; it will\\ndiscuss about different ﬂavors of the bootstrap method that are commonly used to infer the uncertainty\\nof a performance estimate.\\n2.4 The Bootstrap Method and Empirical Conﬁdence Intervals\\nThe previous examples of Monte Carlo Cross-Validation may have convinced us that repeated holdout\\nvalidation could provide us with a more robust estimate of a model’s performance on random test\\nsets compared to an evaluation based on a single train/test split via holdout validation (Section\\n15'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 15, 'page_label': '16'}, page_content='1.5). In addition, the repeated holdout may give us an idea about the stability of our model. This\\nsection explores an alternative approach to model evaluation and for estimating uncertainty using the\\nbootstrap method.\\nLet us assume that we would like to compute a conﬁdence interval around a performance estimate\\nto judge its certainty – or uncertainty. How can we achieve this if our sample has been drawn\\nfrom an unknown distribution? Maybe we could use the sample mean as a point estimate of the\\npopulation mean, but how would we compute the variance or conﬁdence intervals around the mean if\\nits distribution is unknown? Sure, we could collect multiple, independent samples; this is a luxury we\\noften do not have in real world applications, though. Now, the idea behind the bootstrap is to generate\\n\"new samples\" by sampling from an empirical distribution. As a side note, the term \"bootstrap\" likely\\noriginated from the phrase \"to pull oneself up by one’s bootstraps:\"'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 15, 'page_label': '16'}, page_content='originated from the phrase \"to pull oneself up by one’s bootstraps:\"\\nCirca 1900, to pull (oneself) up by (one’s) bootstraps was used ﬁguratively of\\nan impossible task (Among the \"practical questions\" at the end of chapter one of\\nSteele’s \"Popular Physics\" schoolbook (1888) is \"Why can not a man lift himself\\nby pulling up on his boot-straps?\". By 1916 its meaning expanded to include\\n\"better oneself by rigorous, unaided effort.\" The meaning \"ﬁxed sequence of\\ninstructions to load the operating system of a computer\" (1953) is from the notion\\nof the ﬁrst-loaded program pulling itself, and the rest, up by the bootstrap.\\n[Source: Online Etymology Dictionary6]\\nThe bootstrap method is a resampling technique for estimating a sampling distribution, and in the\\ncontext of this article, we are particularly interested in estimating the uncertainty of a performance\\nestimate – the prediction accuracy or error. The bootstrap method was introduced by Bradley Efron'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 15, 'page_label': '16'}, page_content='estimate – the prediction accuracy or error. The bootstrap method was introduced by Bradley Efron\\nin 1979 [Efron, 1992]. About 15 years later, Bradley Efron and Robert Tibshirani even devoted a\\nwhole book to the bootstrap, \"An Introduction to the Bootstrap\" [Efron and Tibshirani, 1994], which\\nis a highly recommended read for everyone who is interested in more details on this topic. In brief,\\nthe idea of the bootstrap method is to generate new data from a population by repeated sampling from\\nthe original dataset with replacement – in contrast, the repeated holdout method can be understood as\\nsampling without replacement. Walking through it step by step, the bootstrap method works like this:\\n1. We are given a dataset of size n.\\n2. For bbootstrap rounds:\\nWe draw one single instance from this dataset and assign it to the jth bootstrap sample.\\nWe repeat this step until our bootstrap sample has size n – the size of the original dataset.'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 15, 'page_label': '16'}, page_content='We repeat this step until our bootstrap sample has size n – the size of the original dataset.\\nEach time, we draw samples from the same original dataset such that certain examples may\\nappear more than once in a bootstrap sample and some not at all.\\n3. We ﬁt a model to each of the bbootstrap samples and compute the resubstitution accuracy.\\n4. We compute the model accuracy as the average over thebaccuracy estimates (Equation 23).\\nACCboot = 1\\nb\\nb∑\\nj=1\\n1\\nn\\nn∑\\ni=1\\n(\\n1 −L\\n(\\nˆyi,yi\\n))\\n(23)\\nAs discussed previously, the resubstitution accuracy usually leads to an extremely optimistic bias,\\nsince a model can be overly sensible to noise in a dataset. Originally, the bootstrap method aims to\\ndetermine the statistical properties of an estimator when the underlying distribution was unknown\\nand additional samples are not available. So, in order to exploit this method for the evaluation of\\npredictive models, such as hypotheses for classiﬁcation and regression, we may prefer a slightly'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 15, 'page_label': '16'}, page_content='predictive models, such as hypotheses for classiﬁcation and regression, we may prefer a slightly\\ndifferent approach to bootstrapping using the so-called Leave-One-Out Bootstrap (LOOB) technique.\\nHere, we use out-of-bag samples as test sets for evaluation instead of evaluating the model on the\\ntraining data. Out-of-bag samples are the unique sets of instances that are not used for model ﬁtting\\nas shown in Figure 7.\\nFigure 7 illustrates how three random bootstrap samples drawn from an exemplary ten-sample dataset\\n(x1,x2,...,x 10) and how the out-of-bag sample might look like. In practice, Bradley Efron and\\n6https://www.etymonline.com/word/bootstrap\\n16'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 16, 'page_label': '17'}, page_content='x1\\nx1\\nx1\\nx1 x1x1\\nx2 x3 x4 x5 x6 x7 x8 x9 x10\\nx2\\nx2\\nx2\\nx2 x8x8 x10x7x3\\nx6 x9\\nx3 x8 x10x7x2x2 x9x6x6 x4x4x5\\nx10 x8x7x5 x4x3\\nx4x5x6x8 x9\\nTraining Sets Test Sets\\nBootstrap 1\\nBootstrap 2\\nBootstrap 3\\nOriginal Dataset\\nFigure 7: Illustration of training and test data splits in the Leave-One-Out Bootstrap (LOOB).\\nRobert Tibshirani recommend drawing 50 to 200 bootstrap samples as being sufﬁcient for producing\\nreliable estimates [Efron and Tibshirani, 1994].\\nTaking a step back, let us assume that a sample that has been drawn from a normal distribution. Using\\nbasic concepts from statistics, we use the sample mean ¯xas a point estimate of the population mean\\nµ:\\n¯x= 1\\nn\\nn∑\\ni=1\\nxi. (24)\\nSimilarly, the variance σ2 is estimated as follows:\\nV AR= 1\\nn−1\\nn∑\\ni=1\\n(xi −¯x)2. (25)\\nConsequently, the standard error (SE) is computed as the standard deviation’s estimate (SD ≈σ)\\ndivided by the square root of the sample size:\\nSE = SD√n. (26)'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 16, 'page_label': '17'}, page_content='divided by the square root of the sample size:\\nSE = SD√n. (26)\\nUsing the standard error we can then compute a 95% conﬁdence interval of the mean according to\\n¯x±z× σ√n, (27)\\nsuch that\\n¯x±t×SE, (28)\\nwith z= 1.96 for the 95 % conﬁdence interval. Since SD is the standard deviation of the population\\n(σ) estimated from the sample, we have to consult a t-table to look up the actual value of t, which\\ndepends on the size of the sample – or the degrees of freedom to be precise. For instance, given a\\nsample with n= 100, we ﬁnd that t95 = 1.984.\\n17'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 17, 'page_label': '18'}, page_content='Similarly, we can compute the 95% conﬁdence interval of the bootstrap estimate starting with the\\nmean accuracy,\\nACCboot = 1\\nb\\nb∑\\ni=1\\nACCi, (29)\\nand use it to compute the standard error\\nSEboot =\\n\\ued6a\\ued6b\\ued6b√ 1\\nb−1\\nb∑\\ni=1\\n(ACCi −ACCboot)2. (30)\\nHere, ACCi is the value of the statistic (the estimate ofACC) calculated on the ith bootstrap replicate.\\nAnd the standard deviation of the values ACC1,ACC1,..., ACCb is the estimate of the standard error\\nof ACC [Efron and Tibshirani, 1994].\\nFinally, we can then compute the conﬁdence interval around the mean estimate as\\nACCboot ±t×SEboot. (31)\\nAlthough the approach outlined above seems intuitive, what can we do if our samples do not follow a\\nnormal distribution? A more robust, yet computationally straight-forward approach is the percentile\\nmethod as described by B. Efron [Efron, 1981]. Here, we pick the lower and upper conﬁdence bounds\\nas follows:\\n• ACClower = α1th percentile of the ACCboot distribution'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 17, 'page_label': '18'}, page_content='as follows:\\n• ACClower = α1th percentile of the ACCboot distribution\\n• ACCupper = α2th percentile of the ACCboot distribution,\\nwhere α1 = αand α2 = 1−α, and αis the degree of conﬁdence for computing the100×(1−2×α)\\nconﬁdence interval. For instance, to compute a 95% conﬁdence interval, we pick α= 0.025 to obtain\\nthe the 2.5th and 97.5th percentiles of the bbootstrap samples distribution as our upper and lower\\nconﬁdence bounds.\\nIn practice, if the data is indeed (roughly) following a normal distribution, the \"standard\" conﬁdence\\ninterval and percentile method typically agree as illustrated in the Figure 8.\\n\\x01 \\x02\\nFigure 8: Comparison of the standard and percentile method for computing conﬁdence intervals\\nfrom leave-one-out bootstrap samples. Subpanel A evaluates 3-nearest neighbors models on Iris, and\\nsublpanel B shows the results of softmax regression models on MNIST.\\nIn 1983, Bradley Efron described the .632 Estimate, a further improvement to address the pessimistic'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 17, 'page_label': '18'}, page_content='In 1983, Bradley Efron described the .632 Estimate, a further improvement to address the pessimistic\\nbias of the bootstrap cross-validation approach described above [Efron, 1983]. The pessimistic bias\\nin the \"classic\" bootstrap method can be attributed to the fact that the bootstrap samples only contain\\napproximately 63.2% of the unique examples from the original dataset. For instance, we can compute\\n18'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 18, 'page_label': '19'}, page_content='the probability that a given example from a dataset of size nis not drawn as a bootstrap sample as\\nfollows:\\nP(not chosen) =\\n(\\n1 −1\\nn\\n)n\\n, (32)\\nwhich is asymptotically equivalent to 1\\ne ≈0.368 as n→∞.\\nVice versa, we can then compute the probability that a sample is chosen as\\nP(chosen) = 1 −\\n(\\n1 −1\\nn\\n)n\\n≈0.632 (33)\\nfor reasonably large datasets, so that we select approximately0.632×nunique examples as bootstrap\\ntraining sets and reserve 0.382 ×n out-of-bag examples for testing in each iteration, which is\\nillustrated in Figure 9.\\nFigure 9: Probability of including an example from the dataset in a bootstrap sample for different\\ndataset sizes n.\\nNow, to address the bias that is due to this the sampling with replacement, Bradley Efron proposed\\nthe .632 Estimate mentioned earlier, which is computed via the following equation:\\nACCboot = 1\\nb\\nb∑\\ni=1\\n(\\n0.632 ·ACCh,i + 0.368 ·ACCr,i\\n)\\n, (34)\\nwhere ACCr,i is the resubstitution accuracy, and ACCh,i is the accuracy on the out-of-bag sample.'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 18, 'page_label': '19'}, page_content='b\\nb∑\\ni=1\\n(\\n0.632 ·ACCh,i + 0.368 ·ACCr,i\\n)\\n, (34)\\nwhere ACCr,i is the resubstitution accuracy, and ACCh,i is the accuracy on the out-of-bag sample.\\nNow, while the .632 Boostrap attempts to address the pessimistic bias of the estimate, an optimistic\\nbias may occur with models that tend to overﬁt so that Bradley Efron and Robert Tibshirani proposed\\nThe .632+ Bootstrap Method[Efron and Tibshirani, 1997]. Instead of using a ﬁxed weightω= 0.632\\nin\\nACCboot = 1\\nb\\nb∑\\ni=1\\n(\\nω·ACCh,i + (1 −ω) ·ACCr,i\\n)\\n, (35)\\n19'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 19, 'page_label': '20'}, page_content='we compute the weight ωas\\nω= 0.632\\n1 −0.368 ×R, (36)\\nwhere R is the relative overﬁtting rate:\\nR= (−1) ×(ACCh,i −ACCr,i)\\nγ−(1 −ACCh,i) . (37)\\n(Since we are plugging ωinto Equation 35 for computing ACCbootthat we deﬁned above, ACCh,i\\nand ACCr,i still refer to the resubstitution and out-of-bag accuracy estimates in the ith bootstrap\\nround, respectively.)\\nFurther, we need to determine the no-information rate γin order to compute R. For instance, we can\\ncompute γby ﬁtting a model to a dataset that contains all possible combinations between samples x′\\ni\\nand target class labels yi – we pretend that the observations and class labels are independent:\\nγ = 1\\nn2\\nn∑\\ni=1\\nn∑\\ni′=1\\nL(yi,f(xi′ )). (38)\\nAlternatively, we can estimate the no-information rate γas follows:\\nγ =\\nK∑\\nk=1\\npk(1 −qk), (39)\\nwhere pk is the proportion of class kexamples observed in the dataset, and qk is the proportion of\\nclass kexamples that the classiﬁer predicts in the dataset.'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 19, 'page_label': '20'}, page_content='class kexamples that the classiﬁer predicts in the dataset.\\nThe OOB bootstrap, 0.632 bootstrap, and .632+ boostrap method discussed in this section are\\nimplemented in MLxtend [Raschka, 2018] to enable comparison studies.7\\nThis section continued the discussion around biases and variances in evaluating machine learning\\nmodels in more detail. Further, it introduced the repeated hold-out method that may provide us with\\nsome further insights into a model’s stability. Then, we looked at the bootstrap method and explored\\ndifferent ﬂavors of this bootstrap method that help us estimate the uncertainty of our performance\\nestimates. After covering the basics of model evaluation in this and the previous section, the next\\nsection introduces hyperparameter tuning and model selection.\\n3 Cross-validation and Hyperparameter Optimization\\n3.1 Overview\\nAlmost every machine learning algorithm comes with a large number of settings that we, the'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 19, 'page_label': '20'}, page_content='3.1 Overview\\nAlmost every machine learning algorithm comes with a large number of settings that we, the\\nmachine learning researchers and practitioners, need to specify. These tuning knobs, the so-called\\nhyperparameters, help us control the behavior of machine learning algorithms when optimizing\\nfor performance, ﬁnding the right balance between bias and variance. Hyperparameter tuning for\\nperformance optimization is an art in itself, and there are no hard-and-fast rules that guarantee best\\nperformance on a given dataset. The previous sections covered holdout and bootstrap techniques for\\nestimating the generalization performance of a model. The bias-variance trade-off was discussed in the\\ncontext of estimating the generalization performance as well as methods for computing the uncertainty\\nof performance estimates. This third section focusses on different methods of cross-validation for\\nmodel evaluation and model selection. It covers cross-validation techniques to rank models from'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 19, 'page_label': '20'}, page_content='model evaluation and model selection. It covers cross-validation techniques to rank models from\\nseveral hyperparameter conﬁgurations and estimate how well these generalize to independent datasets.\\n(Code for generating the ﬁgures discussed in this section is available on GitHub8.)\\n7http://rasbt.github.io/mlxtend/user_guide/evaluate/bootstrap_point632_score/\\n8https://github.com/rasbt/model-eval-article-supplementary/blob/master/code/resampling-and-kfold.ipynb\\n20'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 20, 'page_label': '21'}, page_content='3.2 About Hyperparameters and Model Selection\\nPreviously, the holdout method and different ﬂavors of the bootstrap were introduced to estimate\\nthe generalization performance of our predictive models. We split the dataset into two parts: a\\ntraining and a test dataset. After the machine learning algorithm ﬁt a model to the training set, we\\nevaluated it on the independent test set that we withheld from the machine learning algorithm during\\nmodel ﬁtting. While we were discussing challenges such as the bias-variance trade-off, we used\\nﬁxed hyperparameter settings in our learning algorithms, such as the number of kin the k-nearest\\nneighbors algorithm. We deﬁned hyperparameters as the parameters of the learning algorithm itself,\\nwhich we have to specify a priori – before model ﬁtting. In contrast, we referred to the parameters of\\nour resulting model as the model parameters.\\nSo, what are hyperparameters, exactly? Considering the k-nearest neighbors algorithm, one example'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 20, 'page_label': '21'}, page_content='our resulting model as the model parameters.\\nSo, what are hyperparameters, exactly? Considering the k-nearest neighbors algorithm, one example\\nof a hyperparameter is the integer value of k(Figure 10). If we set k= 3, the k-nearest neighbors\\nalgorithm will predict a class label based on a majority vote among the 3-nearest neighbors in the\\ntraining set. The distance metric for ﬁnding these nearest neighbors is yet another hyperparameter of\\nthis algorithm.\\n?\\nk = 5\\nk = 3\\nk = 1\\nfeature 1\\nfeature 2\\nFigure 10: Illustration of the k-nearest neighbors algorithm with different choices for k.\\nNow, the k-nearest neighbors algorithm may not be an ideal choice for illustrating the difference\\nbetween hyperparameters and model parameters, since it is alazy learner and a nonparametric method.\\nIn this context, lazy learning (or instance-based learning) means that there is no training or model\\nﬁtting stage: A k-nearest neighbors model literally stores or memorizes the training data and uses it'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 20, 'page_label': '21'}, page_content='ﬁtting stage: A k-nearest neighbors model literally stores or memorizes the training data and uses it\\nonly at prediction time. Thus, each training instance represents a parameter in thek-nearest neighbors\\nmodel. In short, nonparametric models are models that cannot be described by a ﬁxed number of\\nparameters that are being adjusted to the training set. The structure of parametric models is not\\ndecided by the training data rather than being seta priori; nonparamtric models do not assume that the\\ndata follows certain probability distributions unlike parametric methods (exceptions of nonparametric\\nmethods that make such assumptions are Bayesian nonparametric methods). Hence, we may say that\\nnonparametric methods make fewer assumptions about the data than parametric methods.\\nIn contrast to k-nearest neighbors, a simple example of a parametric method is logistic regression,\\na generalized linear model with a ﬁxed number of model parameters: a weight coefﬁcient for each'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 20, 'page_label': '21'}, page_content='a generalized linear model with a ﬁxed number of model parameters: a weight coefﬁcient for each\\nfeature variable in the dataset plus a bias (or intercept) unit. These weight coefﬁcients in logistic\\n21'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 21, 'page_label': '22'}, page_content='regression, the model parameters, are updated by maximizing a log-likelihood function or minimizing\\nthe logistic cost. For ﬁtting a model to the training data, a hyperparameter of a logistic regression\\nalgorithm could be the number of iterations or passes over the training set (epochs) in gradient-based\\noptimization. Another example of a hyperparameter would be the value of a regularization parameter\\nsuch as the lambda-term in L2-regularized logistic regression (Figure 11).\\nFigure 11: Conceptual overview of logistic regression.\\nChanging the hyperparameter values when running a learning algorithm over a training set may result\\nin different models. The process of ﬁnding the best-performing model from a set of models that were\\nproduced by different hyperparameter settings is called model selection. The next section introduces\\nan extension to the holdout method that is useful when carrying out this selection process.\\n3.3 The Three-Way Holdout Method for Hyperparameter Tuning'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 21, 'page_label': '22'}, page_content='an extension to the holdout method that is useful when carrying out this selection process.\\n3.3 The Three-Way Holdout Method for Hyperparameter Tuning\\nSection 1 provided an explanation why resubstitution validation is a bad approach for estimating of\\nthe generalization performance. Since we want to know how well our model generalizes to new data,\\nwe used the holdout method to split the dataset into two parts, a training set and an independent test\\nset. Can we use the holdout method for hyperparameter tuning? The answer is \"yes.\" However, we\\nhave to make a slight modiﬁcation to our initial approach, the \"two-way\" split, and split the dataset\\ninto three parts: a training, a validation, and a test set.\\nThe process of hyperparameter tuning (or hyperparameter optimization) and model selection can be\\nregarded as a meta-optimization task. While the learning algorithm optimizes an objective function'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 21, 'page_label': '22'}, page_content='regarded as a meta-optimization task. While the learning algorithm optimizes an objective function\\non the training set (with exception to lazy learners), hyperparameter optimization is yet another\\ntask on top of it; here, we typically want to optimize a performance metric such as classiﬁcation\\naccuracy or the area under a Receiver Operating Characteristic curve. After the tuning stage, selecting\\na model based on the test set performance seems to be a reasonable approach. However, reusing the\\ntest set multiple times would introduce a bias and the ﬁnal performance estimate and likely result\\nin overly optimistic estimates of the generalization performance – one might say that \"the test set\\nleaks information.\" To avoid this problem, we could use a three-way split, dividing the dataset into a\\ntraining, validation, and test dataset. Having a training-validation pair for hyperparameter tuning and'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 21, 'page_label': '22'}, page_content='training, validation, and test dataset. Having a training-validation pair for hyperparameter tuning and\\nmodel selections allows us to keep the test set \"independent\" for model evaluation. Once more, let us\\nrecall the \"3 goals\" of performance estimation:\\n22'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 22, 'page_label': '23'}, page_content='1. We want to estimate the generalization accuracy, the predictive performance of a model on\\nfuture (unseen) data.\\n2. We want to increase the predictive performance by tweaking the learning algorithm and\\nselecting the best-performing model from a given hypothesis space.\\n3. We want to identify the machine learning algorithm that is best-suited for the problem at\\nhand; thus, we want to compare different algorithms, selecting the best-performing one as\\nwell as the best-performing model from the algorithm’s hypothesis space.\\nThe \"three-way holdout method\" is one way to tackle points 1 and 2 (more on point 3 in Section\\n4). Though, if we are only interested in point 2, selecting the best model, and do not care so much\\nabout an \"unbiased\" estimate of the generalization performance, we could stick to the two-way split\\nfor model selection. Thinking back of our discussion about learning curves and pessimistic biases'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 22, 'page_label': '23'}, page_content='for model selection. Thinking back of our discussion about learning curves and pessimistic biases\\nin Section 2, we noted that a machine learning algorithm often beneﬁts from more labeled data;\\nthe smaller the dataset, the higher the pessimistic bias and the variance – the sensitivity of a model\\ntowards the data is partitioned.\\n\"There ain’t no such thing as a free lunch.\" The three-way holdout method for hyperparameter tuning\\nand model selection is not the only – and certainly often not the best – way to approach this task. Later\\nsections, will introduce alternative methods and discuss their advantages and trade-offs. However,\\nbefore we move on to the probably most popular method for model selection, k-fold cross-validation\\n(or sometimes also called \"rotation estimation\" in older literature), let us have a look at an illustration\\nof the 3-way split holdout method in Figure 12.\\nLet us walk through Figure 12 step by step.'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 22, 'page_label': '23'}, page_content='of the 3-way split holdout method in Figure 12.\\nLet us walk through Figure 12 step by step.\\nStep 1. We start by splitting our dataset into three parts, a training set for model ﬁtting, a validation\\nset for model selection, and a test set for the ﬁnal evaluation of the selected model.\\nStep 2. This step illustrates the hyperparameter tuning stage. We use the learning algorithm with\\ndifferent hyperparameter settings (here: three) to ﬁt models to the training data.\\nStep 3. Next, we evaluate the performance of our models on the validation set. This step illustrates\\nthe model selection stage; after comparing the performance estimates, we choose the hyperparameters\\nsettings associated with the best performance. Note that we often merge steps two and three in\\npractice: we ﬁt a model and compute its performance before moving on to the next model in order to\\navoid keeping all ﬁtted models in memory.\\nStep 4. As discussed earlier, the performance estimates may suffer from pessimistic bias if the'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 22, 'page_label': '23'}, page_content='avoid keeping all ﬁtted models in memory.\\nStep 4. As discussed earlier, the performance estimates may suffer from pessimistic bias if the\\ntraining set is too small. Thus, we can merge the training and validation set after model selection and\\nuse the best hyperparameter settings from the previous step to ﬁt a model to this larger dataset.\\nStep 5. Now, we can use the independent test set to estimate the generalization performance our\\nmodel. Remember that the purpose of the test set is to simulate new data that the model has not\\nseen before. Re-using this test set may result in an overoptimistic bias in our estimate of the model’s\\ngeneralization performance.\\nStep 6. Finally, we can make use of all our data – merging training and test set– and ﬁt a model to\\nall data points for real-world use.\\nNote that ﬁtting the model on all available data might yield a model that is likely slightly different'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 22, 'page_label': '23'}, page_content='all data points for real-world use.\\nNote that ﬁtting the model on all available data might yield a model that is likely slightly different\\nfrom the model evaluated in Step 5. However, in theory, using all data (that is, training and test\\ndata) to ﬁt the model should only improve its performance. Under this assumption, the evaluated\\nperformance from Step 5 might slightly underestimate the performance of the model ﬁtted in Step\\n6. (If we use test data for ﬁtting, we do not have data left to evaluate the model, unless we collect\\nnew data.) In real-world applications, having the \"best possible\" model is often desired – or in other\\nwords, we do not mind if we slightly underestimated its performance. In any case, we can regard this\\nsixth step as optional.\\n23'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 23, 'page_label': '24'}, page_content='2\\n1 Data\\nLabels\\nTraining Data\\nValidation  \\nData\\nValidation  \\nLabels\\nTest  \\nData\\nTest  \\nLabels\\nTraining Labels\\nPerformance\\nModel\\nValidation  \\nData\\nValidation  \\nLabels\\nPrediction\\nPerformance\\nModel\\nValidation  \\nData\\nValidation  \\nLabels\\nPrediction\\nPerformance\\nModel\\nValidation  \\nData\\nValidation  \\nLabels\\nPrediction\\nBest \\nModel\\nLearning  \\nAlgorithm\\nHyperparameter  \\nvalues\\nModelHyperparameter  \\nvalues\\nHyperparameter  \\nvalues\\nModel\\nModel\\nTraining Data\\nTraining Labels\\nLearning  \\nAlgorithm\\nBest  \\nHyperparameter  \\nValues Final \\nModel6 Data\\nLabels\\n3\\nBest \\nHyperparameter  \\nvalues\\nPrediction\\nTest Labels\\nPerformance\\nModel\\n4\\nTest Data\\nLearning  \\nAlgorithm\\nBest  \\nHyperparameter  \\nValues Model\\nTraining Data\\nTraining Labels\\n5\\nValidation  \\nData\\nValidation  \\nLabels\\nFigure 12: Illustration of the three-way holdout method for hyperparameter tuning.\\n3.4 Introduction to k-fold Cross-Validation\\nIt is about time to introduce the probably most common technique for model evaluation and model'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 23, 'page_label': '24'}, page_content='3.4 Introduction to k-fold Cross-Validation\\nIt is about time to introduce the probably most common technique for model evaluation and model\\nselection in machine learning practice: k-fold cross-validation. The term cross-validation is used\\nloosely in literature, where practitioners and researchers sometimes refer to the train/test holdout\\nmethod as a cross-validation technique. However, it might make more sense to think of cross-\\n24'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 24, 'page_label': '25'}, page_content='validation as a crossing over of training and validation stages in successive rounds. Here, the main\\nidea behind cross-validation is that each sample in our dataset has the opportunity of being tested.\\nk-fold cross-validation is a special case of cross-validation where we iterate over a dataset setktimes.\\nIn each round, we split the dataset into kparts: one part is used for validation, and the remaining\\nk−1 parts are merged into a training subset for model evaluation as shown in Figure 13 , which\\nillustrates the process of 5-fold cross-validation.\\n1st\\n2nd\\n3rd\\n4th\\n5th\\nK Iterations (K-Folds)\\nValidation  \\nFold\\nTraining  \\nFold\\nLearning  \\nAlgorithm\\n Hyperparameter  \\nValues\\nModel\\nTraining Fold Data\\nTraining Fold Labels\\nPrediction\\nPerformance\\nModel\\nValidation  \\nFold Data\\nValidation  \\nFold Labels\\nPerformance\\nPerformance\\nPerformance\\nPerformance\\nPerformance\\n1\\n2\\n3\\n4\\n5\\nPerformance  \\n1 \\n5 ∑5\\ni =1\\nPerformancei=\\nA\\nB C\\nFigure 13: Illustration of the k-fold cross-validation procedure.'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 24, 'page_label': '25'}, page_content='Performance\\nPerformance\\n1\\n2\\n3\\n4\\n5\\nPerformance  \\n1 \\n5 ∑5\\ni =1\\nPerformancei=\\nA\\nB C\\nFigure 13: Illustration of the k-fold cross-validation procedure.\\nJust as in the \"two-way\" holdout method (Section 1.5), we use a learning algorithm with ﬁxed\\nhyperparameter settings to ﬁt models to the training folds in each iteration when we use the k-fold\\ncross-validation method for model evaluation. In 5-fold cross-validation, this procedure will result\\nin ﬁve different models ﬁtted; these models were ﬁt to distinct yet partly overlapping training sets\\nand evaluated on non-overlapping validation sets. Eventually, we compute the cross-validation\\nperformance as the arithmetic mean over the kperformance estimates from the validation sets.\\nWe saw the main difference between the \"two-way\" holdout method and k-fold cross validation:\\nk-fold cross-validation uses all data for training and testing. The idea behind this approach is to reduce'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 24, 'page_label': '25'}, page_content='k-fold cross-validation uses all data for training and testing. The idea behind this approach is to reduce\\nthe pessimistic bias by using more training data in contrast to setting aside a relatively large portion\\nof the dataset as test data. And in contrast to the repeated holdout method, which was discussed in\\nSection 2, test folds in k-fold cross-validation are not overlapping. In repeated holdout, the repeated\\nuse of samples for testing results in performance estimates that become dependent between rounds;\\n25'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 25, 'page_label': '26'}, page_content='this dependence can be problematic for statistical comparisons, which we will be discussed in Section\\n4. Also, k-fold cross-validation guarantees that each sample is used for validation in contrast to the\\nrepeated holdout-method, where some samples may never be part of the test set.\\nThis section introduced k-fold cross-validation for model evaluation. In practice, however, k-fold\\ncross-validation is more commonly used for model selection or algorithm selection. k-fold cross-\\nvalidation for model selection is a topic that we will be covered in the next sections, and algorithm\\nselection will be discussed in detail throughout Section 4.\\n3.5 Special Cases: 2-Fold and Leave-One-Out Cross-Validation\\nAt this point, you may wonder why k = 5 was chosen to illustrate k-fold cross-validation in the\\nprevious section. One reason is that it makes it easier to illustrate k-fold cross-validation compactly.\\nMoreover, k = 5 is also a common choice in practice, since it is computationally less expensive'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 25, 'page_label': '26'}, page_content='Moreover, k = 5 is also a common choice in practice, since it is computationally less expensive\\ncompared to larger values of k. If kis too small, though, the pessimistic bias of the performance\\nestimate may increase (since less training data is available for model ﬁtting), and the variance of the\\nestimate may increase as well since the model is more sensitive to how the data was split (in the next\\nsections, experiments will be discussed that suggest k= 10 as a good choice for k).\\nIn fact, there are two prominent, special cases of k-fold cross validation: k = 2 and k = n. Most\\nliterature describes 2-fold cross-validation as being equal to the holdout method. However, this\\nstatement would only be true if we performed the holdout method by rotating the training and\\nvalidation set in two rounds (for instance, using exactly 50% data for training and 50% of the\\nexamples for validation in each round, swapping these sets, repeating the training and evaluation'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 25, 'page_label': '26'}, page_content='examples for validation in each round, swapping these sets, repeating the training and evaluation\\nprocedure, and eventually computing the performance estimate as the arithmetic mean of the two\\nperformance estimates on the validation sets). Given how the holdout method is most commonly\\nused though, this article describes the holdout method and 2-fold cross-validation as two different\\nprocesses as illustrated in Figure 14.\\nNow, if we set k = n, that is, if we set the number of folds as being equal to the number of\\ntraining instances, we refer to the k-fold cross-validation process as Leave-One-Out Cross-Validation\\n(LOOCV), which is illustrated in Figure 15. In each iteration during LOOCV , we ﬁt a model ton−1\\nsamples of the dataset and evaluate it on the single, remaining data point. Although this process is\\ncomputationally expensive, given that we haveniterations, it can be useful for small datasets, cases\\nwhere withholding data from the training set would be too wasteful.'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 25, 'page_label': '26'}, page_content='where withholding data from the training set would be too wasteful.\\nSeveral studies compared different values of kin k-fold cross-validation, analyzing how the choice\\nof kaffects the variance and the bias of the estimate. Unfortunately, there is no Free Lunch though as\\nshown by Yohsua Bengio and Yves Grandvalet in \"No unbiased estimator of the variance ofk-fold\\ncross-validation:\"\\nThe main theorem shows that there exists no universal (valid under all distributions)\\nunbiased estimator of the variance of k-fold cross-validation.\\n[Bengio and Grandvalet, 2004]\\nHowever, we may still be interested in ﬁnding a \"sweet spot,\" a value for kthat seems to be a good\\ntrade-off between variance and bias in most cases, and the bias-variance trade-off discussion will\\nbe continued in the next section. For now, let us conclude this section by looking at an interesting\\nresearch project where Hawkins and others compared performance estimates via LOOCV to the'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 25, 'page_label': '26'}, page_content='research project where Hawkins and others compared performance estimates via LOOCV to the\\nholdout method and recommend the LOOCV over the latter – if computationally feasible:\\n[...] where available sample sizes are modest, holding back compounds for model\\ntesting is ill-advised. This fragmentation of the sample harms the calibration and\\ndoes not give a trustworthy assessment of ﬁt anyway. It is better to use all data\\nfor the calibration step and check the ﬁt by cross-validation, making sure that\\nthe cross-validation is carried out correctly. [...] The only motivation to rely on\\nthe holdout sample rather than cross-validation would be if there was reason to\\nthink the cross-validation not trustworthy – biased or highly variable. But neither\\ntheoretical results nor the empiric results sketched here give any reason to disbelieve\\nthe cross-validation results.\\n[Hawkins et al., 2003]\\n26'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 26, 'page_label': '27'}, page_content='1 2 3 4 5 6 7 8 109\\n1 2 3 4 5 6 7 8 109\\n1 2 3 4 5 6 7 8 109\\n1 2 3 4 5 6 7 8 109\\n1 2 3 4 5 6 7 8 109\\n...\\n1 2 3 4 5 6 7 8 109\\n1 2 3 4 5 6 7 8 109 Holdout Method\\n2-Fold Cross-Validation\\nRepeated Holdout\\nTraining Evaluation\\nFigure 14: Comparison of the holdout method, 2-fold cross-validation, and the repeated holdout\\nmethod.\\nTable 1: Summary of the ﬁndings from the LOOCV vs. holdout comparison study conducted by\\nHawkins and others [Hawkins et al., 2003]. See text for details.\\nExperiment Mean Standard deviation\\nTrue R2 — q2 0.010 0.149\\nTrue R2 — hold 50 0.028 0.184\\nTrue R2 — hold 20 0.055 0.305\\nTrue R2 — hold 10 0.123 0.504\\nThe conclusions in the previous quotation are partly based on the experiments carried out in this study\\nusing a 469-sample dataset, and Table 1 summarizes the ﬁndings in a comparison of different Ridge\\nRegression models evaluated by LOOCV and the holdout method [Hawkins et al., 2003]. The ﬁrst'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 26, 'page_label': '27'}, page_content='Regression models evaluated by LOOCV and the holdout method [Hawkins et al., 2003]. The ﬁrst\\nrow corresponds to an experiment where the researchers used LOOCV to ﬁt regression models to\\n100-example training subsets. The reported \"mean\" refers to the averaged difference between the true\\ncoefﬁcients of determination (R2) and the coefﬁcients obtained via LOOCV (here called q2) after\\nrepeating this procedure on different 100-example training sets and averaging the results. In rows\\n2-4, the researchers used the holdout method for ﬁtting models to the 100-example training sets, and\\nthey evaluated the performances on holdout sets of sizes 10, 20, and 50 samples. Each experiment\\nwas repeated 75 times, and the mean column shows the average difference between the estimated R2\\nand the true R2 values. As we can see, the estimates obtained via LOOCV (q2) are the closest to the\\ntrue R2 on average. The estimates obtained from the 50-example test set via the holdout method are'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 26, 'page_label': '27'}, page_content='true R2 on average. The estimates obtained from the 50-example test set via the holdout method are\\nalso passable, though. Based on these particular experiments, we may agree with the researchers’\\nconclusion:\\n27'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 27, 'page_label': '28'}, page_content='1 2 3 4 5 6 7 8 109\\n1 2 3 4 5 6 7 8 109\\n1 2 3 4 5 6 7 8 109\\n1 2 3 4 5 6 7 8 109\\n1 2 3 4 5 6 7 8 109\\n1 2 3 4 5 6 7 8 109\\n1 2 3 4 5 6 7 8 109\\n1 2 3 4 5 6 7 8 109\\n1 2 3 4 5 6 7 8 109\\n1 2 3 4 5 6 7 8 109\\nTraining evaluation\\nFigure 15: Illustration of leave-one-out cross-validation.\\nTaking the third of these points, if you have 150 or more compounds available,\\nthen you can certainly make a random split into 100 for calibration and 50 or more\\nfor testing. However it is hard to see why you would want to do this.\\n[Hawkins et al., 2003]\\nOne reason why we may prefer the holdout method may be concerns about computational efﬁciency,\\nif the dataset is sufﬁciently large. As a rule of thumb, we can say that the pessimistic bias and large\\nvariance concerns are less problematic the larger the dataset. Moreover, it is not uncommon to repeat\\nthe k-fold cross-validation procedure with different random seeds in hope to obtain a \"more robust\"'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 27, 'page_label': '28'}, page_content='the k-fold cross-validation procedure with different random seeds in hope to obtain a \"more robust\"\\nestimate. For instance, if we repeated a 5-fold cross-validation run 100 times, we would compute\\nthe performance estimate for 500 test folds report the cross-validation performance as the arithmetic\\nmean of these 500 folds. (Although this is commonly done in practice, we note that the test folds are\\nnow overlapping.) However, there is no point in repeating LOOCV , since LOOCV always produces\\nthe same splits.\\n3.6 k-fold Cross-Validation and the Bias-Variance Trade-off\\nBased on the study by Hawkins and others [Hawkins et al., 2003] discussed in Section 3.5 we may\\nprefer LOOCV over single train/test splits via the holdout method for small and moderately sized\\ndatasets. In addition, we can think of the LOOCV estimate as being approximately unbiased: the\\npessimistic bias of LOOCV (k= n) is intuitively lower compared k <n-fold cross-validation, since'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 27, 'page_label': '28'}, page_content='pessimistic bias of LOOCV (k= n) is intuitively lower compared k <n-fold cross-validation, since\\nalmost all (for instance, n−1) training samples are available for model ﬁtting.\\nWhile LOOCV is almost unbiased, one downside of using LOOCV over k-fold cross-validation\\nwith k < nis the large variance of the LOOCV estimate. First, we have to note that LOOCV is\\ndefect when using a discontinuous loss-function such as the 0-1 loss in classiﬁcation or even in\\ncontinuous loss functions such as the mean-squared-error. It is said that LOOCV \"[LOOCV has] high\\nvariance because the test set only contains one sample\" [Tan et al., 2005] and \"[LOOCV] is highly\\nvariable, since it is based upon a single observation (x1, y1)\" [James et al., 2013]. These statements\\nare certainly true if we refer to the variance between folds. Remember that if we use the 0-1 loss\\n28'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 28, 'page_label': '29'}, page_content='function (the prediction is either correct or not), we could consider each prediction as a Bernoulli\\ntrial, and the number of correct predictions X if following a binomial distribution X ≈B(n,p),\\nwhere n∈N and p∈[0,1]; the variance of a binomial distribution is deﬁned as\\nσ2 = np(1 −p) (40)\\nWe can estimate the variability of a statistic (here: the performance of the model) from the variability\\nof that statistic between subsamples. Obviously though, the variance between folds is a poor estimate\\nof the variance of the LOOCV estimate – the variability due to randomness in our training data. Now,\\nwhen we are talking about the variance of LOOCV , we typically mean the difference in the results\\nthat we would get if we repeated the resampling procedure multiple times on different data samples\\nfrom the underlying distribution. In this context interesting point has been made by Hastie, Tibshirani,\\nand Friedman:\\nWith k= n, the cross-validation estimator is approximately unbiased for the true'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 28, 'page_label': '29'}, page_content='and Friedman:\\nWith k= n, the cross-validation estimator is approximately unbiased for the true\\n(expected) prediction error, but can have high variance because then\"training sets\"\\nare so similar to one another.\\n[Hastie et al., 2009]\\nOr in other words, we can attribute the high variance to the well-known fact that the mean of highly\\ncorrelated variables has a higher variance than the mean of variables that are not highly correlated.\\nMaybe, this can intuitively be explained by looking at the relationship between covariance (cov) and\\nvariance (σ2):\\ncovX,X = σ2\\nX. (41)\\nor\\ncovX,X = E\\n[\\n(X−µ)2]\\n= σ2\\nX (42)\\nif we let µ= E(X).\\nAnd the relationship between covariancecovX,Y and correlation ρX,Y (X and Y are random variables)\\nis deﬁned as\\ncovX,Y = ρX,Y σXσY, (43)\\nwhere\\ncovX,Y = E[(X−µX)(Y −µY)] (44)\\nand\\nρX,Y = E[(X−µX)(Y −µY)]/(σXσY). (45)\\nThe large variance that is often associated with LOOCV has also been observed in empirical studies\\n[Kohavi, 1995].'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 28, 'page_label': '29'}, page_content='The large variance that is often associated with LOOCV has also been observed in empirical studies\\n[Kohavi, 1995].\\nNow that we established that LOOCV estimates are generally associated with a large variance and a\\nsmall bias, how does this method compare to k-fold cross-validation with other choices for kand the\\nbootstrap method? Section 2 discussed the pessimistic bias of the standard bootstrap method, where\\nthe training set asymptotically (only) contains 0.632 of the samples from the original dataset; 2- or\\n3-fold cross-validation has about the same problem (the .632 bootstrap that was designed to address\\nthis pessimistic bias issue). However, Kohavi also observed in his experiments [Kohavi, 1995] that\\nthe bias in bootstrap was still extremely large for certain real-world datasets (now, optimistically\\nbiased) compared to k-fold cross-validation. Eventually, Kohavi’s experiments on various real-world'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 28, 'page_label': '29'}, page_content='biased) compared to k-fold cross-validation. Eventually, Kohavi’s experiments on various real-world\\ndatasets suggest that 10-fold cross-validation offers the best trade-off between bias and variance.\\n29'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 29, 'page_label': '30'}, page_content='Furthermore, other researchers found that repeating k-fold cross-validation can increase the precision\\nof the estimates while still maintaining a small bias [Molinaro et al., 2005, Kim, 2009].\\nBefore moving on to model selection in the next section, the following points shall summarize the\\ndiscussion of the bias-variance trade-off, by listing the general trends when increasing the number of\\nfolds or k:\\n• The bias of the performance estimator decreases (more accurate)\\n• The variance of the performance estimators increases (more variability)\\n• The computational cost increases (more iterations, larger training sets during ﬁtting)\\n• Exception: decreasing the value of kin k-fold cross-validation to small values (for example,\\n2 or 3) also increases the variance on small datasets due to random sampling effects.\\n3.7 Model Selection via k-fold Cross-Validation\\nPrevious sections introduced k-fold cross-validation for modelevaluation. Now, this section discusses'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 29, 'page_label': '30'}, page_content='3.7 Model Selection via k-fold Cross-Validation\\nPrevious sections introduced k-fold cross-validation for modelevaluation. Now, this section discusses\\nhow to use the k-fold cross-validation method for model selection. Again, the key idea is to keep\\nan independent test dataset, that we withhold from during training and model selection, to avoid the\\nleaking of test data in the training stage. This procedure is outlined in Figure 16.\\nAlthough Figure 16 might seem complicated at ﬁrst, the process is quite simple and analogous to\\nthe \"three-way holdout\" workﬂow that we discussed at the beginning of this article. The following\\nparagraphs will discuss Figure 16 step by step.\\nStep 1. Similar to the holdout method, we split the dataset into two parts, a training and an\\nindependent test set; we tuck away the test set for the ﬁnal model evaluation step at the end (Step 4).\\nStep 2. In this second step, we can now experiment with various hyperparameter settings; we could'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 29, 'page_label': '30'}, page_content='Step 2. In this second step, we can now experiment with various hyperparameter settings; we could\\nuse Bayesian optimization, randomized search, or grid search, for example. For each hyperparameter\\nconﬁguration, we apply the k-fold cross-validation method on the training set, resulting in multiple\\nmodels and performance estimates.\\nStep 3. Taking the hyperparameter settings that produced the best results in the k-fold cross-\\nvalidation procedure, we can then use the complete training set for model ﬁtting with these settings.\\nStep 4. Now, we use the independent test set that we withheld earlier (Step 1); we use this test set\\nto evaluate the model that we obtained from Step 3.\\nStep 5. Finally, after we completed the evaluation stage, we can optionally ﬁt a model to all data\\n(training and test datasets combined), which could be the model for (the so-called) deployment.\\n3.8 A Note About Model Selection and Large Datasets'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 29, 'page_label': '30'}, page_content='(training and test datasets combined), which could be the model for (the so-called) deployment.\\n3.8 A Note About Model Selection and Large Datasets\\nWhen we browse the deep learning literature, we often ﬁnd that that the 3-way holdout method is\\nthe method of choice when it comes to model evaluation; it is also common in older (non-deep\\nlearning literature) as well. As mentioned earlier, the three-way holdout may be preferred over k-fold\\ncross-validation since the former is computationally cheap in comparison. Aside from computational\\nefﬁciency concerns, we only use deep learning algorithms when we have relatively large sample\\nsizes anyway, scenarios where we do not have to worry about high variance – the sensitivity of our\\nestimates towards how we split the dataset for training, validation, and testing – so much. Thus, it is\\nﬁne to use the holdout method with a training, validation, and test split over thek-fold cross-validation\\nfor model selection if the dataset is relatively large.'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 29, 'page_label': '30'}, page_content='for model selection if the dataset is relatively large.\\n3.9 A Note About Feature Selection During Model Selection\\nNote that if we normalize data or select features, we typically perform these operations inside the\\nk-fold cross-validation loop in contrast to applying these steps to the whole dataset upfront before\\nsplitting the data into folds. Feature selection inside the cross-validation loop reduces the bias through\\n30'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 30, 'page_label': '31'}, page_content='Test Labels\\nTest Data\\nTraining Data\\nTraining Labels\\nData\\nLabels\\nLearning  \\nAlgorithm\\nHyperparameter  \\nvalues\\nHyperparameter  \\nvalues\\nHyperparameter  \\nvalues\\nTraining Data\\nTraining Labels\\nLearning  \\nAlgorithm\\nBest  \\nHyperparameter  \\nValues\\nModel\\nTraining Data\\nTraining Labels\\nPrediction\\nTest Labels\\nPerformance\\nModel\\nTest Data\\nLearning  \\nAlgorithm\\nBest  \\nHyperparameter  \\nValues Final \\nModel\\nData\\nLabels\\n2\\n1\\n3\\n4\\n5\\nPerformance\\nPerformance\\nPerformance\\nFigure 16: Illustration of k-fold cross-validation for model selection.\\noverﬁtting, since it avoids peaking at the test data information during the training stage. However,\\nfeature selection inside the cross-validation loop may lead to an overly pessimistic estimate, since\\nless data is available for training. A more detailed discussion on this topic, whether to perform feature\\nselection inside or outside the cross-validation loop, can be found in Refaeilzadeh’s \"On comparison\\nof feature selection algorithms\" [Refaeilzadeh et al., 2007].\\n31'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 31, 'page_label': '32'}, page_content='3.10 The Law of Parsimony\\nNow that we discussed model selection in the previous section, let us take a moment and consider the\\nLaw of Parsimony, which is also known as Occam’s Razor: \"Among competing hypotheses, the one\\nwith the fewest assumptions should be selected.\" In model selection practice, Occam’s razor can be\\napplied, for example, by using the one-standard error method [Breiman et al., 1984] as follows:\\n1. Consider the numerically optimal estimate and its standard error.\\n2. Select the model whose performance is within one standard error of the value obtained in\\nstep 1.\\nAlthough, we may prefer simpler models for several reasons, Pedro Domingos made a good point\\nregarding the performance of \"complex\" models. Here is an excerpt from his article, \"Ten Myths\\nAbout Machine Learning9:\"\\nSimpler models are more accurate. This belief is sometimes equated with Occam’s\\nrazor, but the razor only says that simpler explanations are preferable, not why.'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 31, 'page_label': '32'}, page_content='razor, but the razor only says that simpler explanations are preferable, not why.\\nThey’re preferable because they’re easier to understand, remember, and reason\\nwith. Sometimes the simplest hypothesis consistent with the data is less accurate\\nfor prediction than a more complicated one. Some of the most powerful learning\\nalgorithms output models that seem gratuitously elaborate? – sometimes even\\ncontinuing to add to them after they’ve perfectly ﬁt the data – but that’s how they\\nbeat the less powerful ones.\\nAgain, there are several reasons why we may prefer a simpler model if its performance is within a\\ncertain, acceptable range – for example, using the one-standard error method. Although a simpler\\nmodel may not be the most \"accurate\" one, it may be computationally more efﬁcient, easier to\\nimplement, and easier to understand and reason with compared to more complicated alternatives.\\nTo see how the one-standard error method works in practice, let us consider a simple toy dataset: 300'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 31, 'page_label': '32'}, page_content='To see how the one-standard error method works in practice, let us consider a simple toy dataset: 300\\ntraining examples, concentric circles, and a uniform class distribution (150 samples from class 1 and\\n150 samples from class 2).\\nFigure 17: Concentric circles dataset with 210 training examples and uniform class distribution.\\nThe concentric circles dataset is then split into two parts, 70% training data and 30% test data, using\\nstratiﬁcation to maintain equal class proportions. The 210 samples from the training dataset are\\nshown in Figure 17.\\n9https://medium.com/@pedromdd/ten-myths-about-machine-learning-d888b48334a3\\n32'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 32, 'page_label': '33'}, page_content='Now, let us assume that our goal is to optimize the γ(gamma) hyperparameter of a Support Vector\\nMachine (SVM) with a non-linear Radial Basis Function-kernel (RBF-kernel), where γis the free\\nparameter of the Gaussian RBF:\\nK(xi,xj) = exp(−γ||xi −xj||2),γ >0. (46)\\n(Intuitively, we can think of γas a parameter that controls the inﬂuence of single training samples on\\nthe decision boundary.)\\nAfter running the RBF-kernel SVM algorithm with different γ values over the training set, using\\nstratiﬁed 10-fold cross-validation, the performance estimates shown in Figure 18 were obtained,\\nwhere the error bars are the standard errors of the cross-validation estimates.\\n= 0.1\\n= 0.001\\n= 10.0\\nFigure 18: Performance estimates and decision regions of an RBF-kernel SVM with stratiﬁed 10-fold\\ncross-validation for different γvalues. Error bars represent the standard error of the cross-validation\\nestimates.\\nAs shown in Figure 18, Choosing γ values between 0.1 and 100 resulted in a prediction accuracy'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 32, 'page_label': '33'}, page_content='estimates.\\nAs shown in Figure 18, Choosing γ values between 0.1 and 100 resulted in a prediction accuracy\\nof 80% or more. Furthermore, we can see that γ = 10 .0 resulted in a fairly complex decision\\nboundary, and γ = 0 .001 resulted in a decision boundary that is too simple to separate the two\\nclasses in the concentric circles dataset. In fact, γ = 0.1 seems like a good trade-off between the two\\naforementioned models (γ = 10.0 and γ = 0.1) – the performance of the corresponding model falls\\nwithin one standard error of the best performing model with γ = 0 or γ = 10.\\n3.11 Summary\\nThere are many ways for evaluating the generalization performance of predictive models. So far,\\nthis article covered the holdout method, different ﬂavors of the bootstrap approach, and k-fold cross-\\nvalidation. Using holdout method is absolutely ﬁne for model evaluation when working with relatively\\nlarge sample sizes. For hyperparameter optimization, we may prefer 10-fold cross-validation, and'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 32, 'page_label': '33'}, page_content='large sample sizes. For hyperparameter optimization, we may prefer 10-fold cross-validation, and\\nLeave-One-Out cross-validation is a good option when working with small sample sizes. When\\nit comes to model selection, again, the \"three-way\" holdout method may be a good choice if the\\n33'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 33, 'page_label': '34'}, page_content='dataset is large, if computational efﬁciency is a concern; a good alternative is using k-fold cross-\\nvalidation with an independent test set. While the previous sections focused on model evaluation\\nand hyperparameter optimization, Section 4 introduces different techniques for comparing different\\nlearning algorithms.\\nThe next section will discuss different statistical methods for comparing the performance of different\\nmodels as well as empirical approaches for comparing different machine learning algorithms.\\n4 Algorithm Comparison\\n4.1 Overview\\nThis ﬁnal section in this article presents overviews of several statistical hypothesis testing approaches,\\nwith applications to machine learning model and algorithm comparisons. This includes statistical\\ntests based on target predictions for independent test sets (the downsides of using a single test set for\\nmodel comparisons was discussed in previous sections) as well as methods for algorithm comparisons'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 33, 'page_label': '34'}, page_content='model comparisons was discussed in previous sections) as well as methods for algorithm comparisons\\nby ﬁtting and evaluating models via cross-validation. Lastly, this ﬁnal section will introduce nested\\ncross-validation, which has become a common and recommended a method of choice for algorithm\\ncomparisons for small to moderately-sized datasets.\\nThen, at the end of this article, I provide a list of my personal suggestions concerning model evaluation,\\nselection, and algorithm selection summarizing the several techniques covered in this series of articles.\\n4.2 Testing the Difference of Proportions\\nThere are several different statistical hypothesis testing frameworks that are being used in practice to\\ncompare the performance of classiﬁcation models, including conventional methods such as difference\\nof two proportions (here, the proportions are the estimated generalization accuracies from a test\\nset), for which we can construct 95% conﬁdence intervals based on the concept of the Normal'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 33, 'page_label': '34'}, page_content='set), for which we can construct 95% conﬁdence intervals based on the concept of the Normal\\nApproximation to the Binomial that was covered in Section 1.\\nPerforming a z-score test for two population proportions is inarguably the most straight-forward way\\nto compare to models (but certainly not the best!): In a nutshell, if the 95% conﬁdence intervals of\\nthe accuracies of two models do not overlap, we can reject the null hypothesis that the performance\\nof both classiﬁers is equal at a conﬁdence level of α = 0.05 (or 5% probability). Violations of\\nassumptions aside (for instance that the test set samples are not independent), as Thomas Dietterich\\nnoted based on empirical results in a simulated study [Dietterich, 1998], this test tends to have a high\\nfalse positive rate (here: incorrectly detecting difference when there is none), which is among the\\nreasons why it is not recommended in practice.\\nNonetheless, for the sake of completeness, and since it a commonly used method in practice, the'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 33, 'page_label': '34'}, page_content='reasons why it is not recommended in practice.\\nNonetheless, for the sake of completeness, and since it a commonly used method in practice, the\\ngeneral procedure is outlined below as follows (which also generally applies to the different hypothesis\\ntests presented later):\\n1. formulate the hypothesis to be tested (for instance, the null hypothesis stating that the\\nproportions are the same; consequently, the alternative hypothesis that the proportions are\\ndifferent, if we use a two-tailed test);\\n2. decide upon a signiﬁcance threshold (for instance, if the probability of observing a difference\\nmore extreme than the one seen is more than 5%, then we plan to reject the null hypothesis);\\n3. analyze the data, compute the test statistic ( here: z-score), and compare its associated\\np-value (probability) to the previously determined signiﬁcance threshold;\\n4. based on the p-value and signiﬁcance threshold, either accept or reject the null hypothesis at'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 33, 'page_label': '34'}, page_content='4. based on the p-value and signiﬁcance threshold, either accept or reject the null hypothesis at\\nthe given conﬁdence level and interpret the results.\\nThe z-score is computed as the observed difference divided by the square root for their combined\\nvariances\\nz= ACC1 −ACC2√\\nσ2\\n1 + σ2\\n2\\n,\\n34'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 34, 'page_label': '35'}, page_content='where ACC1 is the accuracy of one model and ACC2 is the accuracy of a second model estimated\\nfrom the test set. Recall that we computed the variance of the estimated accuracy as\\nσ2 = ACC(1 −ACC)\\nn\\nin Section 1 and then computed the conﬁdence interval (Normal Approximation Interval) as\\nACC ±z×σ,\\nwhere z= 1.96 for a 95% conﬁdence interval. Comparing the conﬁdence intervals of two accuracy\\nestimates and checking whether they overlap is then analogous to computing the z value for the\\ndifference in proportions and comparing the probability (p-value) to the chosen signiﬁcance threshold.\\nSo, to compute the z-score directly for the difference of two proportions, ACC1 and ACC2, we pool\\nthese proportions (assuming that ACC1 and ACC2 are the performances of two models estimated\\non two indendent test sets of size n1 and n2, respectively),\\nACC1,2 = ACC1 ×n1 + ACC2 ×n2\\nn1 + n2\\n,\\nand compute the standard deviation as\\nσ1,2 =\\n√\\nACC1,2(1 −ACC1,2) ×\\n(1\\nn1\\n+ 1\\nn2\\n)\\n,\\nsuch that we can compute the z-score,'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 34, 'page_label': '35'}, page_content='n1 + n2\\n,\\nand compute the standard deviation as\\nσ1,2 =\\n√\\nACC1,2(1 −ACC1,2) ×\\n(1\\nn1\\n+ 1\\nn2\\n)\\n,\\nsuch that we can compute the z-score,\\nz= ACC1 −ACC2\\nσ1,2\\n.\\nSince, due to using the same test set (and violating the independence assumption) we have n1 =\\nn2 = n, so that we can simplify the z-score computation to\\nz= ACC1 −ACC2√\\n2σ2 = ACC1 −ACC2√\\n2 ·ACC1,2(1 −ACC1,2))/n\\n.\\nwhere ACC1,2 is simply (ACC1 + ACC2)/2.\\nIn the second step, based on the computedzvalue (this assumes the test errors are independent, which\\nis usually violated in practice as we use the same test set) we can reject the null hypothesis that the\\npair of models has equal performance (here, measured in \"classiﬁcation accuracy\") at an α= 0.05\\nlevel if |z|is higher than 1.96. Alternatively, if we want to put in the extra work, we can compute\\nthe area under the standard normal cumulative distribution at the z-score threshold. If we ﬁnd this'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 34, 'page_label': '35'}, page_content='the area under the standard normal cumulative distribution at the z-score threshold. If we ﬁnd this\\np-value is smaller than a signiﬁcance level we set before conducting the test, then we can reject the\\nnull hypothesis at that given signiﬁcance level.\\nThe problem with this test though is that we use the same test set to compute the accuracy of the two\\nclassiﬁers; thus, it might be better to use a paired test such as a paired sample t-test, but a more robust\\nalternative is the McNemar test illustrated in the next section.\\n4.3 Comparing Two Models with the McNemar Test\\nSo, instead of using the \"difference of proportions\" test, Dietterich [ Dietterich, 1998] found that\\nthe McNemar test is to be preferred. The McNemar test, introduced by Quinn McNemar in 1947\\n[McNemar, 1947], is a non-parametric statistical test for paired comparisons that can be applied to\\ncompare the performance of two machine learning classiﬁers.'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 34, 'page_label': '35'}, page_content='compare the performance of two machine learning classiﬁers.\\nOften, McNemar’s test is also referred to as \"within-subjects chi-squared test,\" and it is applied to\\npaired nominal data based on a version of 2x2 confusion matrix (sometimes also referred to as 2x2\\ncontingency table) that compares the predictions of two models to each other (not be confused with\\nthe typical confusion matrices encountered in machine learning, which are listing false positive, true\\n35'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 35, 'page_label': '36'}, page_content='This work by Sebastian Raschka is licensed under a  Creative Commons Attribution 4.0 International License. \\nA B\\nC D\\nModel 2\\ncorrect\\nModel 2\\nwrong\\nModel 1\\ncorrect\\nModel 1\\nwrong\\nFigure 19: Confusion matrix layout in context of McNemar’s test.\\npositive, false negative, and true negative counts of a single model). The layout of the 2x2 confusion\\nmatrix suitable for McNemar’s test is shown in Figure 19.\\nGiven such a 2x2 confusion matrix as shown in Figure 19, we can compute the accuracy of a Model\\n1 via (A+ B)/(A+ B+ C+ D), where A+ B+ C+ Dis the total number of test examples n.\\nSimilarly, we can compute the accuracy of Model 2 as (A+ C)/n. The most interesting numbers\\nin this table are in cells B and C, though, as A and D merely count the number of samples where\\nboth Model 1 and Model 2 made correct or wrong predictions, respectively. Cells B and C (the\\noff-diagonal entries), however, tell us how the models differ. To illustrate this point, let us take a look\\nat the following example:'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 35, 'page_label': '36'}, page_content='off-diagonal entries), however, tell us how the models differ. To illustrate this point, let us take a look\\nat the following example:\\n9945 25\\n15 15\\nModel 2\\ncorrect\\nModel 2\\nwrong\\nModel 1\\ncorrect\\nModel 1\\nwrong\\nThis work by Sebastian Raschka is licensed under a  Creative Commons Attribution 4.0 International License. \\n9959 11\\n1 29\\nModel 2\\ncorrect\\nModel 2\\nwrong\\nModel 1\\ncorrect\\nModel 1\\nwrong\\nA B\\nFigure 20: Confusion matrix for exemplary classiﬁcation outcomes of two models, Model 1 and\\nModel 2.\\nIn both subpanels, A and B, in Figure 20, the accuracy of Model 1 and Model 2 are 99.6% and 99.7%,\\nrespectively.\\n• Model 1 accuracy subpanel A: (9959 + 11)/10000 ×100% = 99.7%\\n36'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 36, 'page_label': '37'}, page_content='• Model 1 accuracy subpanel B: (9945 + 25)/10000 ×100% = 99.7%\\n• Model 2 accuracy subpanel A: (9959 + 1)/10000 ×100% = 99.6%\\n• Model 2 accuracy subpanel B: (9945 + 15)/10000 ×100% = 99.6%\\nNow, in subpanel A, we can see that Model 1 got 11 predictions right that Model 2 got wrong. Vice\\nversa, Model 2 got one prediction right that Model 1 got wrong. Thus, based on this 11:1 ratio,\\nwe may conclude, based on our intuition, that Model 1 performs substantially better than Model 2.\\nHowever, in subpanel B, the Model 1:Model 2 ratio is 25:15, which is less conclusive about which\\nmodel is the better one to choose. This is a good example where McNemar’s test can come in handy.\\nIn McNemar’s Test, we formulate the null hypothesis that the probabilitiesp(B) and p(C) – where\\nB and C refer to the confusion matrix cells introduced in an earlier ﬁgure – are the same, or in\\nsimpliﬁed terms: None of the two models performs better than the other. Thus, we might consider'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 36, 'page_label': '37'}, page_content='simpliﬁed terms: None of the two models performs better than the other. Thus, we might consider\\nthe alternative hypothesis that the performances of the two models are not equal.\\nThe McNemar test statistic (\"chi-squared\") can be computed as follows:\\nχ2 = (B−C)2\\nB+ C .\\nAfter setting a signiﬁcance threshold, for example, α= 0.05, we can compute a p-value – assuming\\nthat the null hypothesis is true, the p-value is the probability of observing the given empirical (or a\\nlarger) χ2-squared value. If the p-value is lower than our chosen signiﬁcance level, we can reject the\\nnull hypothesis that the two models’ performances are equal.\\nSince the McNemar test statistic, χ2, follows a χ2 distribution with one degree of freedom (assuming\\nthe null hypothesis and relatively large numbers in cells B and C, say > 25), we can now use our\\nfavorite software package to \"look up\" the (1-tail) probability via the χ2 probability distribution with\\none degree of freedom.'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 36, 'page_label': '37'}, page_content='favorite software package to \"look up\" the (1-tail) probability via the χ2 probability distribution with\\none degree of freedom.\\nIf we did this for scenario B in the previous ﬁgure (χ2 = 2.5), we would obtain a p-value of 0.1138,\\nwhich is larger than our signiﬁcance threshold, and thus, we cannot reject the null hypothesis. Now,\\nif we computed the p-value for scenario A (χ2 = 8.3), we would obtain a p-value of 0.0039, which is\\nbelow the set signiﬁcance threshold (α= 0.05) and leads to the rejection of the null hypothesis; we\\ncan conclude that the models’ performances are different (for instance, Model 1 performs better than\\nModel 2).\\nApproximately one year after Quinn McNemar published the McNemar Test [McNemar, 1947], Allen\\nL. Edwards [Edwards, 1948] proposed a continuity corrected version, which is the more commonly\\nused variant today:\\nχ2 =\\n(\\n|B−C|−1\\n)2\\nB+ C .\\nIn particular, Edwards wrote:\\nThis correction will have the apparent result of reducing the absolute value of the'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 36, 'page_label': '37'}, page_content='χ2 =\\n(\\n|B−C|−1\\n)2\\nB+ C .\\nIn particular, Edwards wrote:\\nThis correction will have the apparent result of reducing the absolute value of the\\ndifference, [B - C], by unity.\\nAccording to Edward, this continuity correction increases the usefulness and accuracy of McNemar’s\\ntest if we are dealing with discrete frequencies and the data is evaluated regarding the chi-squared\\ndistribution.\\nA function for using McNemar’s test is implemented in MLxtend [Raschka, 2018].10\\n4.4 Exact p-Values via the Binomial Test\\nWhile McNemar’s test approximates thep-values reasonably well if the values in cells B and C are\\nlarger than 50 (referring to the 2x2 confusion matrix shown earlier), for example, it makes sense\\n10http://rasbt.github.io/mlxtend/user_guide/evaluate/mcnemar/\\n37'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 37, 'page_label': '38'}, page_content='to use a computationally more expensive binomial test to compute the exact p-values if the values\\nof B and C are relatively small – since the chi-squared value from McNemar’s test may not be\\nwell-approximated by the chi-squared distribution.\\nThe exact p-value can be computed as follows (based on the fact that McNemar’s test, under the null\\nhypothesis, is essentially a binomial test with proportion 0.5):\\np= 2\\nn∑\\ni=B\\n(n\\ni\\n)\\n0.5i(1 −0.5)n−i,\\nwhere n= B+ C, and the factor 2 is used to compute the two-sided p-value (here, nis not to be\\nconfused with the test set size n).\\nThe heat map shown in Figure 21 illustrates the differences between the McNemar approximation\\nof the chi-squared value (with and without Edward’s continuity correction) to the exact p-values\\ncomputed via the binomial test.\\n0 20 40 60 80 100\\nB\\n0\\n20\\n40\\n60\\n80\\n100 C\\n0.0000\\n0.0025\\n0.0050\\n0.0075\\n0.0100\\n0.0125\\n0.0150\\n0.0175\\n0.0200\\nThis work by Sebastian Raschka is licensed under a'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 37, 'page_label': '38'}, page_content='B\\n0\\n20\\n40\\n60\\n80\\n100 C\\n0.0000\\n0.0025\\n0.0050\\n0.0075\\n0.0100\\n0.0125\\n0.0150\\n0.0175\\n0.0200\\nThis work by Sebastian Raschka is licensed under a  \\nCreative Commons Attribution 4.0 International License. \\n0 20 40 60 80 100\\nB\\n0\\n20\\n40\\n60\\n80\\n100 C\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\nUncorrected vs. exact Corrected vs. exact\\nabsolute p-value diﬀerence\\nabsolute p-value diﬀerence\\nFigure 21: Differences between the McNemar approximation of the chi-squared value (with and\\nwithout Edward’s continuity correction) and the exactp-values computed via the binomial test.\\nAs we can see in Figure 21, the p-values from the continuity-corrected version of McNemar’s test\\nare almost identical to the p-values from a binomial test if both B and C are larger than 50. (The\\nMLxtend function, \"mcnemar,\" provides different options for toggling between the regular and the\\ncorrected McNemar test as well as including an option to compute the exact p-values.11)\\n4.5 Multiple Hypotheses Testing'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 37, 'page_label': '38'}, page_content='corrected McNemar test as well as including an option to compute the exact p-values.11)\\n4.5 Multiple Hypotheses Testing\\nIn the previous section, we discussed how we could compare two machine learning classiﬁers using\\nMcNemar’s test. However, in practice, we often have more than two models that we like to compare\\nbased on their estimated generalization performance – for instance, the predictions on an independent\\ntest set. Now, applying the testing procedure described earlier multiple times will result in a typical\\nissue called \"multiple hypotheses testing.\" A common approach for dealing with such scenarios is the\\nfollowing:\\n1. Conduct an omnibus test under the null hypothesis that there is no difference between the\\nclassiﬁcation accuracies.\\n2. If the omnibus test led to the rejection of the null hypothesis, conduct pairwise post hoc tests,\\nwith adjustments for multiple comparisons, to determine where the differences between the'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 37, 'page_label': '38'}, page_content='with adjustments for multiple comparisons, to determine where the differences between the\\nmodel performances occurred. (Here, we could use McNemar’s test, for example.)\\nOmnibus tests are statistical tests designed to check whether random samples depart from a null\\nhypothesis. A popular example of an omnibus test is the so-called Analysis of Variance (ANOV A),\\n11http://rasbt.github.io/mlxtend/user_guide/evaluate/mcnemar/\\n38'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 38, 'page_label': '39'}, page_content='which is a procedure for analyzing the differences between group means. In other words, ANOV A is\\ncommonly used for testing the signiﬁcance of the null hypothesis that the means of several groups are\\nequal. To compare multiple machine learning models, Cochran’sQtest would be a possible choice,\\nwhich is essentially a generalized version of McNemar’s test for three or more models. However,\\nomnibus tests are overall signiﬁcance tests that do not provide information about how the different\\nmodels differ – omnibus tests such as Cochran’sQonly tell us that a group of models differs or not.\\nSince omnibus tests can tell us that but not how models differ, we can perform post hoc tests if\\nan omnibus test leads to the rejection of the null hypothesis. In other words, if we successfully\\nrejected the null hypothesis that the performance of three or models is the same at a predetermined\\nsigniﬁcance threshold, we may conclude that there is at least one signiﬁcant difference among the'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 38, 'page_label': '39'}, page_content='signiﬁcance threshold, we may conclude that there is at least one signiﬁcant difference among the\\ndifferent models.\\nBy deﬁnition, post hoc testing procedures do not require any prior plan for testing. Hence, post\\nhoc testing procedures may have a bad reputation and may be regarded as \"ﬁshing expeditions\" or\\n\"searching for the needle in the haystack,\" because no hypothesis is clear beforehand in terms of\\nwhich models should be compared, so that it is required to compare all possible pairs of models with\\neach other, which leads to the multiple hypothesis problems that we brieﬂy discussed in Section\\n3. However, please keep in mind that these are all approximations and everything concerning\\nstatistical tests and reusing test sets (independence violation) should be taken with (at least) a\\ngrain of salt.\\nFor the post hoc procedure, we could use one of the many correction terms, for example, Bonferroni’s'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 38, 'page_label': '39'}, page_content='grain of salt.\\nFor the post hoc procedure, we could use one of the many correction terms, for example, Bonferroni’s\\ncorrection [Bonferroni, 1936, Dunn, 1961] for multiple hypothesis testing. In a nutshell, where a\\ngiven signiﬁcance threshold (or α-level) may be appropriate for a single comparison between to\\nmodels, it is not suitable for multiple pair-wise comparisons. For instance, using the Bonferroni\\ncorrection is a means to reduce the false positive rate in multiple comparison tests by adjusting the\\nsigniﬁcance threshold to be more conservative.\\nThe next two sections will discuss two omnibus tests, Cochran’sQtest and the F-test for comparing\\nmultiple classiﬁers on the same test set proposed by Looney [Looney, 1988].\\n4.6 Cochran’s QTest for Comparing the Performance of Multiple Classiﬁers\\nCochran’sQtest can be regarded as a generalized version of McNemar’s test that can be applied to\\ncompare three or more classiﬁers. In a sense, Cochran’sQtest is similar to ANOV A but for paired'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 38, 'page_label': '39'}, page_content='compare three or more classiﬁers. In a sense, Cochran’sQtest is similar to ANOV A but for paired\\nnominal data. Similar to ANOV A, it does not provide us with information about which groups (or\\nmodels) differ – only tells us that there is a difference among the models.\\nThe test statistic Qis approximately, (similar to McNemar’s test), distributed as chi-squared with\\nM −1 degrees of freedom, where M is the number of models we evaluate (since M = 2 for\\nMcNemar’s test, McNemar’s test statistic approximates a chi-squared distribution with one degree of\\nfreedom).\\nMore formally, Cochran’sQtest tests the null hypothesis (H0) that there is no difference between the\\nclassiﬁcation accuracies [Fleiss et al., 2013]:\\nH0 : ACC1 = ACC2 = ... = ACCM.\\nLet {C1,...,C M}be a set of classiﬁers who have all been tested on the same dataset. If the M\\nclassiﬁers do not differ in terms of their performance, then the following Qstatistic is distributed\\napproximately as \"chi-squared\" with M −1 degrees of freedom:'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 38, 'page_label': '39'}, page_content='approximately as \"chi-squared\" with M −1 degrees of freedom:\\nQ= (M −1)M∑M\\ni=1 G2\\ni −T2\\nMT −∑n\\nj=1 M2\\nj\\n.\\nHere, Gi is the number of objects out of the ntest examples correctly classiﬁed by Ci = 1,...M ;\\nMj is the number of classiﬁers out of M that correctly classiﬁed the jth example in the test dataset;\\nand T is the total number of correct number of votes among the M classiﬁers [Kuncheva, 2004]:\\nT =\\nM∑\\ni=1\\n; Gi =\\nN∑\\nj=1\\nMj.\\n39'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 39, 'page_label': '40'}, page_content='To perform Cochran’sQtest, we typically organize the classiﬁer predictions in a binary n×M\\nmatrix (number of test examples vs. the number of classiﬁers). The ijth entry of such matrix is 0 if a\\nclassiﬁer Cj has misclassiﬁed a data example (vector) xi and 1 otherwise (if the classiﬁer predicted\\nthe class label f(xi) correctly).\\nApplied to an example dataset that was taken from [Kuncheva, 2004], the procedure below illustrates\\nhow the classiﬁcation results may be organized. For instance, assume we have the ground truth labels\\nof the test dataset ytrue\\nand the following predictions on the test set by 3 classiﬁers (yC1 , yC2 , and yC3 ):\\nytrue = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\\n0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\\n0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\\n0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\\n0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0];\\n(47)\\nyC1 = [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,\\n0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\\n0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 39, 'page_label': '40'}, page_content='(47)\\nyC1 = [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,\\n0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\\n0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\\n0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\\n0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0];\\n(48)\\nyC2 = [1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\\n1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\\n0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\\n0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\\n0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0];\\n(49)\\nyC3 = [1,1,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,\\n1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\\n0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\\n0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\\n0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1].\\n(50)\\nWe can then tabulate the correct (1) and incorrect (0) classiﬁcations as shown in Table 2.\\nBy plugging in the respective value into the previous equation, we obtain the following Qvalue:\\nQ= 2 × 3 ×(842 + 922 + 922) −2682\\n3 ×268 −(80 ×9 + 11×4 + 6×1) ≈7.5294. (51)'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 39, 'page_label': '40'}, page_content='Q= 2 × 3 ×(842 + 922 + 922) −2682\\n3 ×268 −(80 ×9 + 11×4 + 6×1) ≈7.5294. (51)\\nNow, the Qvalue (approximating χ2) corresponds to a p-value of approximately 0.023 assuming a\\nχ2 distribution with M −1 = 2 degrees of freedom. Assuming that we chose a signiﬁcance level\\nof α = 0.05, we would reject the null hypothesis that all classiﬁers perform equally well, since\\n0.023 <α. (An implementation of Cochran’sQtest is provided in MLxtend [Raschka, 2018].12)\\nIn practice, if we successfully rejected the null hypothesis, we could perform multiple post hoc\\npair-wise tests – for example, McNemar’s test with a Bonferroni correction – to determine which\\npairs have different population proportions. Unfortunately, numerous comparisons are usually very\\ntricky in practice. Peter H. Westfall, James F. Troendl, and Gene Pennello wrote a nice article\\non how to approach such situations where we want to compare multiple models to each other\\n[Westfall et al., 2010].\\nAs Perneger, Thomas V [Perneger, 1998] writes:'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 39, 'page_label': '40'}, page_content='[Westfall et al., 2010].\\nAs Perneger, Thomas V [Perneger, 1998] writes:\\n12http://rasbt.github.io/mlxtend/user_guide/evaluate/cochrans_q/\\n40'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 40, 'page_label': '41'}, page_content='Table 2: Table comparing model performances of 3 classiﬁers used to illustrate the computation of\\nCochran’sQin Equation 51 .\\nC1 C2 C3 Occurrences\\n1 1 1 80\\n1 1 0 2\\n1 0 1 0\\n1 0 0 2\\n0 1 1 9\\n0 1 0 1\\n0 0 1 3\\n0 0 0 3\\nClassiﬁcation Accuracies:\\n84/100 ×100% = 84% 92 /100 ×100% = 92% 92 /100 ×100% = 92%\\nType I errors [False Positives] cannot decrease (the whole point of Bonferroni\\nadjustments) without inﬂating type II errors (the probability of accepting the null\\nhypothesis when the alternative is true) [Rothman, 1990]. And type II errors [False\\nNegatives] are no less false than type I errors.\\nEventually, once more it comes down to the \"no free lunch\" – in this context, let us refer of it as\\nthe \"no free lunch theorem of statistical tests.\" However, statistical testing frameworks can be a\\nvaluable aid in decision making. So, in practice, if we are honest and rigorous, the process of multiple\\nhypothesis testing with appropriate corrections can be a useful aid in decision making. However,'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 40, 'page_label': '41'}, page_content='hypothesis testing with appropriate corrections can be a useful aid in decision making. However,\\nwe have to be careful that we do not put too much emphasis on such procedures when it comes to\\nassessing evidence in data.\\n4.7 The F-test for Comparing Multiple Classiﬁers\\nAlmost ironically, Cochran noted that in his work on the Qtest [Cochran, 1950]\\nIf the data had been measured variables that appeared normally distributed, instead\\nof a collection of 1’s and 0’s, theF-test would be almost automatically applied as\\nthe appropriate method. Without having looked into that matter, I had once or twice\\nsuggested to research workers that the F-test might serve as an approximation even\\nwhen the table consists of 1’s and 0’s\\nThe method of using the F-test for comparing two classiﬁers in this section is somewhat loosely\\nbased on Looney [Looney, 1988], whereas it shall be noted that Looney recommends an adjusted\\nversion called F+ test.'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 40, 'page_label': '41'}, page_content='based on Looney [Looney, 1988], whereas it shall be noted that Looney recommends an adjusted\\nversion called F+ test.\\nIn the context of the F-test, our null hypothesis is again that there that there is no difference between\\nthe classiﬁcation accuracies:\\npi : H0 = p1 = p2 = ··· = pL.\\nLet {C1,...,C M}be a set of classiﬁers which have all been tested on the same dataset. If the M\\nclassiﬁers do not perform differently, then the F statistic is distributed according to an F distribution\\nwith (M −1) and (M −1) ×ndegrees of freedom, where nis the number of examples in the test\\nset. The calculation of the F statistic consists of several components, which are listed below (adopted\\nfrom [Looney, 1988]).\\nWe start by deﬁning ACCavg as the average of the accuracies of the different models\\nACCavg = 1\\nM\\nM∑\\nj=1\\nACCj.\\n41'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 41, 'page_label': '42'}, page_content='The sum of squares of the classiﬁers is then computed as\\nSSA = n\\nM∑\\nj=1\\n(Gj)2 −n·M ·ACCavg,\\nwhere Gj is the proportion of the nexamples classiﬁed correctly by classiﬁer j.\\nThe sum of squares for the objects is calculated as follows:\\nSSB = 1\\nM\\nn∑\\nj=1\\n(Mj)2 −M ·n·ACC2\\navg.\\nHere, Mj is the number of classiﬁers out of M that correctly classiﬁed object xj ∈Xn, where\\nXn = {x1,...xn}is the test dataset on which the classiﬁers are tested on.\\nFinally, we compute the total sum of squares,\\nSST = M ·n·ACCavg(1 −ACCavg),\\nso that we then can compute the sum of squares for the classiﬁcation–object interaction:\\nSSAB = SST −SSA −SSB.\\nTo compute the F statistic, we next compute the mean SSA and mean SSAB values:\\nMSA = SSA\\nM −1,\\nand\\nMSAB = SSAB\\n(M −1)(n−1).\\nFrom the MSA and MSAB, we can then calculate the F-value as\\nF = MSA\\nMSAB .\\nAfter computing the F-value, we can then look up the p-value from an F-distribution table for the'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 41, 'page_label': '42'}, page_content='F = MSA\\nMSAB .\\nAfter computing the F-value, we can then look up the p-value from an F-distribution table for the\\ncorresponding degrees of freedom or obtain it computationally from a cumulative F-distribution\\nfunction. In practice, if we successfully rejected the null hypothesis at a previously chosen signiﬁcance\\nthreshold, we could perform multiple post hoc pair-wise tests – for example, McNemar tests with a\\nBonferroni correction – to determine which pairs have different population proportions.\\nAn implementation of this F-test is available via MLxtend [Raschka, 2018].13\\n4.8 Comparing Algorithms\\nThe previously described statistical tests focused on model comparisons and thus do not take the\\nvariance of the training sets into account, which can be an issue especially if training sets are small\\nand learning algorithms are susceptible to perturbations in the training sets.\\nHowever, if we consider the comparison between sets of models where each set has been ﬁt to'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 41, 'page_label': '42'}, page_content='However, if we consider the comparison between sets of models where each set has been ﬁt to\\ndifferent training sets, we conceptually shift from a model compared to an algorithm comparison task.\\nThis is often desirable, though. For instance, assume we develop a new learning algorithm or want to\\ndecide which learning algorithm is best to ship with our new software (a trivial example would be an\\n13http://rasbt.github.io/mlxtend/user_guide/evaluate/ftest\\n42'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 42, 'page_label': '43'}, page_content='email program with a learning algorithm that learns how to ﬁlter spam based on the user’s decisions).\\nIn this case, we are want to ﬁnd out how different algorithms perform on datasets from a similar\\nproblem domain.\\nOne of the commonly used techniques for algorithm comparison is Thomas Dietterich’s 5x2-Fold\\nCross-Validation method (5x2cv for short) that was introduced in his paper \"Approximate statistical\\ntests for comparing supervised classiﬁcation learning algorithms\" [ Dietterich, 1998]. It is a nice\\npaper that discusses all the different testing scenarios (the different circumstances and applications\\nfor model evaluation, model selection, and algorithm selection) in the context of statistical tests. The\\nconclusions that can be drawn from empirical comparison on simulated datasets are summarized\\nbelow.\\n1. McNemar’s test:\\n• low false positive rate\\n• fast, only needs to be executed once\\n2. The difference in proportions test:'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 42, 'page_label': '43'}, page_content='below.\\n1. McNemar’s test:\\n• low false positive rate\\n• fast, only needs to be executed once\\n2. The difference in proportions test:\\n• high false positive rate (here, incorrectly detected a difference when there is none)\\n• cheap to compute, though\\n3. Resampled paired t-test:\\n• high false positive rate\\n• computationally very expensive\\n4. k-fold cross-validated t-test:\\n• somewhat elated false positive rate\\n• requires reﬁtting to training sets; ktimes more computations than McNemar’s test\\n5. 5x2cv paired t-test\\n• low false positive rate (similar to McNemar’s test)\\n• slightly more powerful than McNemar’s test; recommended if computational efﬁciency\\n(runtime) is not an issue (10 times more computations than McNemar’s test)\\nThe bottom line is that McNemar’s test is a good choice if the datasets are relatively large and/or the\\nmodel ﬁtting can only be conducted once. If the repeated ﬁtting of the models is possible, the 5x2cv'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 42, 'page_label': '43'}, page_content='model ﬁtting can only be conducted once. If the repeated ﬁtting of the models is possible, the 5x2cv\\ntest is a good choice as it also considers the effect of varying or resampled training sets on the model\\nﬁtting.\\nFor completeness, the next section will summarize the mechanics of the tests we have not covered\\nthus far.\\n4.9 Resampled Paired t-Test\\nResampled paired t-test procedure (also called k-hold-out paired t-test) is a popular method for\\ncomparing the performance of two models (classiﬁers or regressors); however, this method has many\\ndrawbacks and is not recommended to be used in practice as Dietterich [Dietterich, 1998] noted.\\nTo explain how this method works, let us consider two classiﬁers C1 and C2. Further, we have a\\nlabeled dataset S. In the common hold-out method, we typically split the dataset into 2 parts: a\\ntraining and a test set. In the resampled paired t-test procedure, we repeat this splitting procedure'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 42, 'page_label': '43'}, page_content='training and a test set. In the resampled paired t-test procedure, we repeat this splitting procedure\\n(with typically 2/3 training data and 1/3 test data) ktimes (usually 30 or more). In each iteration, we\\nﬁt both C1 and C2 on the same training set and evaluate these on the same test set. Then, we compute\\nthe difference in performance between C1 and C2 in each iteration so that we obtain kdifference\\nmeasures. Now, by making the assumption that these kdifferences were independently drawn and\\nfollow an approximately normal distribution, we can compute the following tstatistic with k−1\\ndegrees of freedom according to Student’st-test, under the null hypothesis that the models C1 and\\nC2 have equal performance:\\nt= ACCavg\\n√\\nk√∑k\\ni=1(ACCi −ACCavg)2/(k−1)\\n.\\n43'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 43, 'page_label': '44'}, page_content='Here, ACCi computes the difference between the model accuracies in the ith iteration ACCi =\\nACCi,C1 −ACCi,C2 and ACCavg represents the average difference between the classiﬁer perfor-\\nmances ACCavg = 1\\nk\\n∑k\\ni=1 ACCi.\\nOnce we computed the t statistic, we can calculate the p-value and compare it to our chosen\\nsigniﬁcance level, for example, α = 0 .05. If the p-value is smaller than α, we reject the null\\nhypothesis and accept that there is a signiﬁcant difference between the two models.\\nThe problem with this method, and the reason why it is not recommended to be used in practice, is\\nthat it violates the assumptions of Student’st-test, as the differences of the model performances are\\nnot normally distributed because the accuracies are not independent (since we compute them on the\\nsame test set). Also, the differences between the accuracies themselves are also not independent since\\nthe test sets overlap upon resampling. Hence, in practice, it is not recommended to use this test in'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 43, 'page_label': '44'}, page_content='the test sets overlap upon resampling. Hence, in practice, it is not recommended to use this test in\\npractice. However, for comparison studies, this test is implemented in MLxtend[Raschka, 2018].14.\\n4.10 k-fold Cross-validated Paired t-Test\\nSimilar to the resampled paired t-test, the k-fold cross-validated paired t-test is a statistical testing\\ntechnique that is very common in (older) literature. While it addresses some of the drawbacks of the\\nresampled paired t-test procedure, this method has still the problem that the training sets overlap and\\nis hence also not recommended to be used in practice [Dietterich, 1998].\\nAgain, for completeness, the method is outlined below. The procedure is basically equivalent to that\\nof the resampled paired t-test procedure except that we use k-fold cross validation instead of simple\\nresampling, such that if we compute the tvalue,\\nt= ACCavg\\n√\\nk√∑k\\ni=1(ACCi −ACCavg)2/(k−1)\\n,'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 43, 'page_label': '44'}, page_content='resampling, such that if we compute the tvalue,\\nt= ACCavg\\n√\\nk√∑k\\ni=1(ACCi −ACCavg)2/(k−1)\\n,\\nkis equal to the number of cross-validation rounds. Again, for comparison studies, I made this this\\ntesting procedure available through MLxtend [Raschka, 2018].15\\n4.11 Dietterich’s 5x2-Fold Cross-Validated Paired t-Test\\nThe 5x2cv paired t-test is a procedure for comparing the performance of two models (classiﬁers\\nor regressors) that was proposed by Dietterich (Dietterich, 1998) to address shortcomings in other\\nmethods such as the resampled paired t-test and the k-fold cross-validated paired t-test, which were\\noutlined in the previous two sections.\\nWhile the overall approach is similar to the previously described t-test variants, in the 5x2cv paired\\nt-test, we repeat the splitting (50% training and 50% test data) ﬁve times.\\nIn each of the 5 iterations, we ﬁt two classiﬁers C1 and C2 to the training split and evaluate their'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 43, 'page_label': '44'}, page_content='In each of the 5 iterations, we ﬁt two classiﬁers C1 and C2 to the training split and evaluate their\\nperformance on the test split. Then, we rotate the training and test sets (the training set becomes the\\ntest set and vice versa) compute the performance again, which results in two performance difference\\nmeasures:\\nACCA = ACCA,C1 −ACCA,C2 ,\\nand\\nACCB = ACCB,C1 −ACCB,C2 .\\nThen, we estimate the estimate mean and variance of the differences:\\nACCavg = (ACCA + ACCB)/2,\\nand\\n14http://rasbt.github.io/mlxtend/user_guide/evaluate/paired_ttest_resampled/\\n15http://rasbt.github.io/mlxtend/user_guide/evaluate/paired_ttest_kfold_cv/\\n44'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 44, 'page_label': '45'}, page_content='s2 = (ACCA −ACCavg)2 + (ACCB −ACC2\\navg)2.\\nThe variance of the difference is computed for the 5 iterations and then used to compute the tstatistic\\nas follows:\\nt= ACCA,1√\\n(1/5) ∑5\\ni=1 s2\\ni\\n,\\nwhere ACCA,1 is the ACCA obtained from the ﬁrst iteration.\\nThe tstatistic approximately follows as tdistribution with 5 degrees of freedom, under the null\\nhypothesis that the models C1 and C2 have equal performance. Using the tstatistic, the p-value can\\nthen be computed and compared with a previously chosen signiﬁcance level, for example, α= 0.05.\\nIf the p-value is smaller than α, we reject the null hypothesis and accept that there is a signiﬁcant\\ndifference in the two models. The 5x2cv paired t-test is available from MLxtend [Raschka, 2018].16\\n4.12 Alpaydin’s Combined 5x2cv F-test\\nThe 5x2cv combined F-test is a procedure for comparing the performance of models (classiﬁers\\nor regressors) that was proposed by Alpaydin [ Alpaydin, 1999] as a more robust alternative to'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 44, 'page_label': '45'}, page_content='or regressors) that was proposed by Alpaydin [ Alpaydin, 1999] as a more robust alternative to\\nDietterich’s 5x2cv pairedt-test procedure outlined in the previous section.\\nTo explain how this mechanics of this method, let us consider two classiﬁers 1 and 2 and re-use the\\nnotation from the previous section. The F statistic is then computed as follows:\\nf =\\n∑5\\ni=1\\n∑2\\nj=1(ACCi,j)2\\n2 ∑5\\ni=1 s2\\ni\\n,\\nwhich is approximately F distributed with 10 and 5 degrees of freedom. The combined 5x2cv F-test\\nis available from MLxtend [Raschka, 2018].17.\\n4.13 Effect size\\nWhile (unfortunately rarely done in practice), we may also want to consider effect sizes since large\\nsamples elevate p-values and can make everything seem statistically signiﬁcant. In other words,\\n\"theoretical signiﬁcance\" does not imply \"practical signiﬁcance.\" As effect size is a more of an\\nobjective topic that depends on the problem/task/question at hand, a detailed discussion is obviously\\nout of the scope of this article.'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 44, 'page_label': '45'}, page_content='objective topic that depends on the problem/task/question at hand, a detailed discussion is obviously\\nout of the scope of this article.\\n4.14 Nested Cross-Validation\\nIn practical applications, we usually never have the luxury of having a large (or, ideally inﬁnitely)\\nsized test set, which would provide us with an unbiased estimate of the true generalization error of a\\nmodel. Hence, we are always on a quest of ﬁnding \"better\" workaround for dealing with size-limited\\ndatasets: Reserving too much data for training results in unreliable estimates of the generalization\\nperformance, and setting aside too much data for testing results in too little data for training, which\\nhurts model performance.\\nAlmost always, we also do not know the ideal settings of the learning algorithm for a given problem or\\nproblem domain. Hence, we need to use an available training set for hyperparameter tuning and model'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 44, 'page_label': '45'}, page_content='problem domain. Hence, we need to use an available training set for hyperparameter tuning and model\\nselection. We established earlier that we could use k-fold cross-validation as a method for these tasks.\\nHowever, if we select the \"best hyperparameter settings\" based on the average k-fold performance or\\nthe *same* test set, we introduce a bias into the procedure, and our model performance estimates\\nwill not be unbiased anymore. Mainly, we can think of model selection as another training procedure,\\nand hence, we would need a decently-sized, independent test set that we have not seen before to get\\nan unbiased estimate of the models’ performance. Often, this is not affordable.\\n16http://rasbt.github.io/mlxtend/user_guide/evaluate/paired_ttest_5x2cv/\\n17http://rasbt.github.io/mlxtend/user_guide/evaluate/combined_ftest_5x2cv/\\n45'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 45, 'page_label': '46'}, page_content='In recent years, a technique called nested cross-validation has emerged as one of the popular or\\nsomewhat recommended methods for comparing machine learning algorithms; it was likely ﬁrst\\ndescribed by Iizuka [ Iizuka et al., 2003] and Varma and Simon [ Varma and Simon, 2006] when\\nworking with small datasets. The nested cross-validation procedure offers a workaround for small-\\ndataset situations that shows a low bias in practice where reserving data for independent test sets is\\nnot feasible.\\nVarma and Simon found that the nested cross-validation approach can reduce the bias, compared\\nto regular k-fold cross-validation when used for both hyperparameter tuning and evaluation, can\\nbe considerably be reduced. As the researchers state, \"A nested CV procedure provides an almost\\nunbiased estimate of the true error\" [Varma and Simon, 2006].\\nThe method of nested cross-validation is relatively straight-forward as it merely is a nesting of two'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 45, 'page_label': '46'}, page_content='The method of nested cross-validation is relatively straight-forward as it merely is a nesting of two\\nk-fold cross-validation loops: the inner loop is responsible for the model selection, and the outer loop\\nis responsible for estimating the generalization accuracy, as shown in Figure 22.\\nOriginal set\\nTraining foldsTe s t  f o l d\\nTraining foldValidation fold\\nOuter loop\\nInner loop\\nTrain with optimal parameters from the inner loop; then averageasanestimateof the generalization performance\\nT une parameters\\nFigure 22: Illustration of nested cross-validation.\\nNote that in this particular case, this is a 5x2 setup (5-fold cross-validation in the outer loop, and\\n2-fold cross-validation in the inner loop). However, this is not the same as Dietterich’s 5x2cv method,\\nwhich is an often confused scenario such that I want to highlight it here.\\nCode for using nested cross-validation with scikit-learn [ Pedregosa et al., 2011] can be found at'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 45, 'page_label': '46'}, page_content='Code for using nested cross-validation with scikit-learn [ Pedregosa et al., 2011] can be found at\\nhttps://github.com/rasbt/model-eval-article-supplementary/blob/master/code/\\nnested_cv_code.ipynb.\\n4.15 Conclusions\\nSince \"a picture is worth a thousand words,\" I want to conclude this series on model evaluation,\\nmodel selection, and algorithm selection with a diagram (Figure 23) that summarizes my personal\\nrecommendations based on the concepts and literature that was reviewed.\\nIt should be stressed that parametric tests for comparing model performances usually violate one or\\nmore independent assumptions (the models are not independent because the same training set was\\nused, and the estimated generalization performances are not independent because the same test set\\nwas used.). In an ideal world, we would have access to the data generating distribution or at least an\\n46'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 46, 'page_label': '47'}, page_content='Performance\\nestimation\\nModel selection\\n(hyperparameter optimization)\\nand performance estimation\\nLarge dataset\\n▪ 2-way holdout method \\n        (train/test split)\\n▪ Conﬁdence interval via \\n        normal approximation\\nSmall dataset\\n▪ 3-way holdout method\\n        (train/validation/test split)\\n▪ (Repeated) k-fold cross-validation\\n        without independent test set\\n▪ Leave-one-out cross-validation\\n        without independent test set\\n▪ Conﬁdence interval via \\n        0.632(+) bootstrap\\nModel & algorithm \\ncomparison\\n▪ Multiple independent \\n        training sets + test sets \\n        (algorithm comparison, AC)\\n▪ McNemar test \\n        (model comparison, MC)\\n▪ Cochran’s Q + McNemar test \\n(MC)\\n▪ Combined 5x2cv F test (AC)\\n▪ Nested cross-validation (AC)\\nLarge dataset\\nSmall dataset\\nLarge dataset\\nSmall dataset\\n▪ (Repeated) k-fold cross-validation\\n        with independent test set\\n▪ Leave-one-out cross-validation\\n        with independent test set\\nThis work by Sebastian Raschka is licensed under a'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 46, 'page_label': '47'}, page_content='▪ Leave-one-out cross-validation\\n        with independent test set\\nThis work by Sebastian Raschka is licensed under a\\nCreative Commons Attribution 4.0 International License.\\nFigure 23: A recommended subset of techniques to be used to address different aspects of model\\nevaluation in the context of small and large datasets. The abbreviation \"MC\" stands for \"Model\\nComparison,\" and \"AC\" stands for \"Algorithm Comparison,\" to distinguish these two tasks.\\nalmost inﬁnite pool of new data. However, in most practical applications, the size of the dataset is\\nlimited; hence, we can use one of the statistical tests discussed in this article as a heuristic to aid our\\ndecision making.\\nNote that the recommendations I listed in the ﬁgure above are suggestions and depend on the problem\\nat hand. For instance, large test datasets (where \"large\" is relative but might refer to thousands or\\nmillions of data records), can provide reliable estimates of the generalization performance, whereas'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 46, 'page_label': '47'}, page_content='millions of data records), can provide reliable estimates of the generalization performance, whereas\\nusing a single training and test set when only a few data records are available can be problematic for\\nseveral reasons discussed throughout Section 2 and Section 3. If the dataset is very small, it might\\nnot be feasible to set aside data for testing, and in such cases, we can use k-fold cross-validation\\nwith a large kor Leave-one-out cross-validation as a workaround for evaluating the generalization\\nperformance. However, using these procedures, we have to bear in mind that we then do not\\ncompare between models but different algorithms that produce different models on the training\\nfolds. Nonetheless, the average performance over the different test folds can serve as an estimate for\\n47'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 47, 'page_label': '48'}, page_content='the generalization performance (Section 3) discussed the various implications for the bias and the\\nvariance of this estimate as a function of the number of folds).\\nFor model comparisons, we usually do not have multiple independent test sets to evaluate the models\\non, so we can again resort to cross-validation procedures such as k-fold cross-validation, the 5x2cv\\nmethod, or nested cross-validation. As Gael Varoquaux [Varoquaux, 2017] writes:\\nCross-validation is not a silver bullet. However, it is the best tool available, because\\nit is the only non-parametric method to test for model generalization.\\n4.16 Acknowledgments\\nI would like to thank Simon Opstrup Drue for carefully reading the manuscript and providing useful\\nsuggestions.\\nReferences\\n[Alpaydin, 1999] Alpaydin, E. (1999). Combined 5x2cv F test for comparing supervised classiﬁca-\\ntion learning algorithms. Neural Computation, 11(8):1885–1892.'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 47, 'page_label': '48'}, page_content='tion learning algorithms. Neural Computation, 11(8):1885–1892.\\n[Bengio and Grandvalet, 2004] Bengio, Y . and Grandvalet, Y . (2004). No unbiased estimator of the\\nvariance of k-fold cross-validation. Journal of Machine Learning Research, 5(Sep):1089–1105.\\n[Bonferroni, 1936] Bonferroni, C. (1936). Teoria statistica delle classi e calcolo delle probabilita.\\nPubblicazioni del R Istituto Superiore di Scienze Economiche e Commericiali di Firenze, 8:3–62.\\n[Breiman et al., 1984] Breiman, L., Friedman, J., Stone, C. J., and Olshen, R. A. (1984). Classiﬁca-\\ntion and regression trees. CRC press.\\n[Cochran, 1950] Cochran, W. G. (1950). The comparison of percentages in matched samples.\\nBiometrika, 37(3/4):256–266.\\n[Dietterich, 1998] Dietterich, T. G. (1998). Approximate statistical tests for comparing supervised\\nclassiﬁcation learning algorithms. Neural computation, 10(7):1895–1923.\\n[Dunn, 1961] Dunn, O. J. (1961). Multiple comparisons among means. Journal of the American'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 47, 'page_label': '48'}, page_content='[Dunn, 1961] Dunn, O. J. (1961). Multiple comparisons among means. Journal of the American\\nstatistical association, 56(293):52–64.\\n[Edwards, 1948] Edwards, A. L. (1948). Note on the “correction for continuity” in testing the\\nsigniﬁcance of the difference between correlated proportions. Psychometrika, 13(3):185–187.\\n[Efron, 1981] Efron, B. (1981). Nonparametric standard errors and conﬁdence intervals. Canadian\\nJournal of Statistics, 9(2):139–158.\\n[Efron, 1983] Efron, B. (1983). Estimating the error rate of a prediction rule: improvement on\\ncross-validation. Journal of the American Statistical Association, 78(382):316–331.\\n[Efron, 1992] Efron, B. (1992). Bootstrap methods: another look at the Jackknife. In Breakthroughs\\nin Statistics, pages 569–593. Springer.\\n[Efron and Tibshirani, 1997] Efron, B. and Tibshirani, R. (1997). Improvements on cross-validation:\\nthe .632+ bootstrap method. Journal of the American Statistical Association, 92(438):548–560.'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 47, 'page_label': '48'}, page_content='the .632+ bootstrap method. Journal of the American Statistical Association, 92(438):548–560.\\n[Efron and Tibshirani, 1994] Efron, B. and Tibshirani, R. J. (1994). An Introduction to the Bootstrap.\\nCRC press.\\n[Fleiss et al., 2013] Fleiss, J. L., Levin, B., and Paik, M. C. (2013). Statistical Methods for Rates\\nand Proportions. John Wiley & Sons.\\n[Hastie et al., 2009] Hastie, T., Tibshirani, R., and Friedman, J. H. (2009). In The Elements of\\nStatistical Learning: Data Mining, Inference, and Prediction. Springer, New York.\\n[Hawkins et al., 2003] Hawkins, D. M., Basak, S. C., and Mills, D. (2003). Assessing model ﬁt by\\ncross-validation. Journal of Chemical Information and Computer Sciences, 43(2):579–586.\\n[Iizuka et al., 2003] Iizuka, N., Oka, M., Yamada-Okabe, H., Nishida, M., Maeda, Y ., Mori, N.,\\nTakao, T., Tamesa, T., Tangoku, A., Tabuchi, H., et al. (2003). Oligonucleotide microarray for\\nprediction of early intrahepatic recurrence of hepatocellular carcinoma after curative resection.'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 47, 'page_label': '48'}, page_content='prediction of early intrahepatic recurrence of hepatocellular carcinoma after curative resection.\\nThe lancet, 361(9361):923–929.\\n48'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 48, 'page_label': '49'}, page_content='[James et al., 2013] James, G., Witten, D., Hastie, T., and Tibshirani, R. (2013). In An Introduction\\nto Statistical Learning: With Applications in R. Springer, New York.\\n[Kim, 2009] Kim, J.-H. (2009). Estimating classiﬁcation error rate: Repeated cross-validation,\\nrepeated hold-out and bootstrap. Computational Statistics & Data Analysis, 53(11):3735–3745.\\n[Kohavi, 1995] Kohavi, R. (1995). A study of cross-validation and bootstrap for accuracy estimation\\nand model selection. International Joint Conference on Artiﬁcial Intelligence, 14(12):1137–1143.\\n[Kuncheva, 2004] Kuncheva, L. I. (2004). Combining Pattern Classiﬁers: Methods and Algorithms.\\nJohn Wiley & Sons.\\n[Looney, 1988] Looney, S. W. (1988). A statistical technique for comparing the accuracies of several\\nclassiﬁers. Pattern Recognition Letters, 8(1):5–9.\\n[McNemar, 1947] McNemar, Q. (1947). Note on the sampling error of the difference between\\ncorrelated proportions or percentages. Psychometrika, 12(2):153–157.'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 48, 'page_label': '49'}, page_content='correlated proportions or percentages. Psychometrika, 12(2):153–157.\\n[Molinaro et al., 2005] Molinaro, A. M., Simon, R., and Pfeiffer, R. M. (2005). Prediction error\\nestimation: a comparison of resampling methods. Bioinformatics, 21(15):3301–3307.\\n[Pedregosa et al., 2011] Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V ., Thirion, B., Grisel,\\nO., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V ., et al. (2011). Scikit-learn: Machine\\nlearning in python. Journal of Machine Learning Research, 12(Oct):2825–2830.\\n[Perneger, 1998] Perneger, T. V . (1998). What’s wrong with bonferroni adjustments. Bmj,\\n316(7139):1236–1238.\\n[Raschka, 2018] Raschka, S. (2018). Mlxtend: Providing machine learning and data science utilities\\nand extensions to python’s scientiﬁc computing stack. The Journal of Open Source Software ,\\n3(24).\\n[Refaeilzadeh et al., 2007] Refaeilzadeh, P., Tang, L., and Liu, H. (2007). On comparison of feature'),\n",
       " Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 48, 'page_label': '49'}, page_content='3(24).\\n[Refaeilzadeh et al., 2007] Refaeilzadeh, P., Tang, L., and Liu, H. (2007). On comparison of feature\\nselection algorithms. In Proceedings of AAAI Workshop on Evaluation Methods for Machine\\nLearning II, pages 34–39.\\n[Rothman, 1990] Rothman, K. J. (1990). No adjustments are needed for multiple comparisons.\\nEpidemiology, pages 43–46.\\n[Tan et al., 2005] Tan, P.-N., Steinbach, M., and Kumar, V . (2005).In Introduction to Data Mining.\\nPearson Addison Wesley, Boston.\\n[Varma and Simon, 2006] Varma, S. and Simon, R. (2006). Bias in error estimation when using\\ncross-validation for model selection. BMC bioinformatics, 7(1):91.\\n[Varoquaux, 2017] Varoquaux, G. (2017). Cross-validation failure: small sample sizes lead to large\\nerror bars. Neuroimage.\\n[Westfall et al., 2010] Westfall, P. H., Troendle, J. F., and Pennello, G. (2010). Multiple McNemar\\ntests. Biometrics, 66(4):1185–1191.\\n49')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3399d81f-fe11-4506-b7a5-a7518507b2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "text_splitter = TokenTextSplitter(chunk_size=10, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8b0bdee7-c252-4c0d-9aeb-ae8cf690ae6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'pdf_lists/model_evaluation_selection_ML.pdf', 'page': 5, 'page_label': '6'}, page_content=' can have different meanings: A hypothesis could be the')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[420]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b536f78f-975c-435c-947a-791eebc599c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'pdf_lists/model_evaluation_selection_ML.pdf',\n",
       " 'page': 5,\n",
       " 'page_label': '6'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[5].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8fc16672-5953-48e4-8e53-10fabe81c82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1500,\n",
    "    chunk_overlap = 150\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "59179900-d42c-43f2-98e0-04b8916992b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cfafa151-b87f-4f2a-a6f7-dfb196afb4c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bea35c-679c-4ea0-b848-3c77a8d49998",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Embedding and Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "947f83df-4529-43c3-9260-10eafcb0cbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "persist_directory = 'chroma_db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8c6a8e86-1239-43a0-a0f9-f828aa3a778a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "790eaa41-b30b-4f7b-bab5-d12f9c9cadee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118\n"
     ]
    }
   ],
   "source": [
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "691350f7-c9c5-491a-9653-e61e66daccf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"main advantages and disadvantages\"\n",
    "docs = vectordb.similarity_search(question, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8cf69392-de66-41cc-8717-1fb07a4160ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "936bf8b9-f549-4d8b-b4eb-bbe45019d545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' the main advantages and disadvantages\\nof each technique with'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[10].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "537a21ff-c06f-47eb-92f8-58c5b710253b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rizdi\\AppData\\Local\\Temp\\ipykernel_7908\\3711397106.py:1: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n"
     ]
    }
   ],
   "source": [
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ec541465-cb1e-40f3-aeea-44ab63992dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what methods used in estimating the performance of machine learning?\"\n",
    "docs = vectordb.similarity_search(question, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "80a9040a-3d3f-477a-bc9d-697eb46a15ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'page': 10, 'page_label': '11', 'source': 'pdf_lists/model_evaluation_selection_ML.pdf'}, page_content='labeled data for model evaluation. Using the holdout method, we split our dataset into two parts: A\\ntraining and a test set. First, we provide the training data to a supervised learning algorithm. The\\nlearning algorithm builds a model from the training set of labeled observations. Then, we evaluate the\\npredictive performance of the model on an independent test set that shall represent new, unseen data.\\nAlso, we brieﬂy introduced the normal approximation, which requires us to make certain assumptions\\nthat allow us to compute conﬁdence intervals for modeling the uncertainty of our performance\\nestimate based on a single test set, which we have to take with a grain of salt.\\nThis section introduces some of the advanced techniques for model evaluation. We will start by\\ndiscussing techniques for estimating the uncertainty of our estimated model performance as well\\nas the model’s variance and stability. And after getting these basics under our belt, we will look at\\ncross-validation techniques for model selection in the next article in this series. As we remember from\\nSection 1, there are three related, yet different tasks or reasons why we care about model evaluation:\\n1. We want to estimate the generalization accuracy, the predictive performance of a model on\\nfuture (unseen) data.\\n2. We want to increase the predictive performance by tweaking the learning algorithm and\\nselecting the best-performing model from a given hypothesis space.')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1e3222c8-aba9-40a2-843d-d5f9d1df57db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'page': 24, 'page_label': '25', 'source': 'pdf_lists/model_evaluation_selection_ML.pdf'}, page_content='validation as a crossing over of training and validation stages in successive rounds. Here, the main\\nidea behind cross-validation is that each sample in our dataset has the opportunity of being tested.\\nk-fold cross-validation is a special case of cross-validation where we iterate over a dataset setktimes.\\nIn each round, we split the dataset into kparts: one part is used for validation, and the remaining\\nk−1 parts are merged into a training subset for model evaluation as shown in Figure 13 , which\\nillustrates the process of 5-fold cross-validation.\\n1st\\n2nd\\n3rd\\n4th\\n5th\\nK Iterations (K-Folds)\\nValidation  \\nFold\\nTraining  \\nFold\\nLearning  \\nAlgorithm\\n Hyperparameter  \\nValues\\nModel\\nTraining Fold Data\\nTraining Fold Labels\\nPrediction\\nPerformance\\nModel\\nValidation  \\nFold Data\\nValidation  \\nFold Labels\\nPerformance\\nPerformance\\nPerformance\\nPerformance\\nPerformance\\n1\\n2\\n3\\n4\\n5\\nPerformance  \\n1 \\n5 ∑5\\ni =1\\nPerformancei=\\nA\\nB C\\nFigure 13: Illustration of the k-fold cross-validation procedure.\\nJust as in the \"two-way\" holdout method (Section 1.5), we use a learning algorithm with ﬁxed\\nhyperparameter settings to ﬁt models to the training folds in each iteration when we use the k-fold\\ncross-validation method for model evaluation. In 5-fold cross-validation, this procedure will result\\nin ﬁve different models ﬁtted; these models were ﬁt to distinct yet partly overlapping training sets\\nand evaluated on non-overlapping validation sets. Eventually, we compute the cross-validation')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594835ce-a053-4886-8013-78adda6f97dc",
   "metadata": {},
   "source": [
    "#### Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0108e2a8-77be-4796-82c4-6275fb4ec500",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rizdi\\AppData\\Local\\Temp\\ipykernel_2772\\490945677.py:4: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectordb = Chroma(\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "persist_directory = 'chroma_db'\n",
    "vectordb = Chroma(\n",
    "    persist_directory=persist_directory,\n",
    "    embedding_function=embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14b078a8-f5f9-4a06-ae97-80b3e3ab2f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118\n"
     ]
    }
   ],
   "source": [
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8285c5d6-cdd8-4a54-ad1a-77608d02aadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"\"\"The Amanita phalloides has a large and imposing epigeous (aboveground) fruiting body (basidiocarp).\"\"\",\n",
    "    \"\"\"A mushroom with a large fruiting body is the Amanita phalloides. Some varieties are all-white.\"\"\",\n",
    "    \"\"\"A. phalloides, a.k.a Death Cap, is one of the most poisonous of all known mushrooms.\"\"\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc107a40-827a-4c03-9111-e7a265ca046c",
   "metadata": {},
   "outputs": [],
   "source": [
    "smalldb = Chroma.from_texts(texts, embedding=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f88e9a41-c94f-403b-8c81-eea2faffff5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='The Amanita phalloides has a large and imposing epigeous (aboveground) fruiting body (basidiocarp).'),\n",
       " Document(metadata={}, page_content='A mushroom with a large fruiting body is the Amanita phalloides. Some varieties are all-white.')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Tell me about all-white mushrooms with large fruiting bodies\"\n",
    "\n",
    "smalldb.similarity_search(question, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "598d2a61-6c40-4fd8-adce-60e0b9410e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='The Amanita phalloides has a large and imposing epigeous (aboveground) fruiting body (basidiocarp).'),\n",
       " Document(metadata={}, page_content='A mushroom with a large fruiting body is the Amanita phalloides. Some varieties are all-white.')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smalldb.max_marginal_relevance_search(question,k=2, fetch_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e404e1-0c18-47f6-b9b9-79f7fe21c379",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Addressing Diversity: Maximum marginal relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51a735f9-8754-4c3e-be4d-f5e2608b12de",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what are reasons that three-way holdout is preferred over k-fold cross validation?\"\n",
    "docs_ss = vectordb.similarity_search(question,k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e903da91-73ae-43b6-ba5b-d5c20cfc020e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. We want to estimate the generalization accuracy, the predictive performance of a model on\\nfuture '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_ss[0].page_content[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2854336f-bddb-4fd2-940e-84751792661b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'labeled data for model evaluation. Using the holdout method, we split our dataset into two parts: A\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_ss[1].page_content[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "12aa91d2-90d5-435f-9920-869992ef3e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Comparison when MMR is applied\n",
    "docs_mmr = vectordb.max_marginal_relevance_search(question, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1a438b5-8806-4fd8-9e67-a2ac3b4d32e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. We want to estimate the generalization accuracy, the predictive performance of a model on\\nfuture '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_ss[0].page_content[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d7b6e78-4b70-46b1-91e6-656352ff6c36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'labeled data for model evaluation. Using the holdout method, we split our dataset into two parts: A\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_ss[1].page_content[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1190945f-ee60-4e5e-82cd-77a72ce403a5",
   "metadata": {},
   "source": [
    "#### Addressing Specificity: working with metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf1f8cdd-29f3-4aaa-8aec-4f11b92df1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what is the drawback of decreasing the size of the test set?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a7ae8a8-25fa-48df-80b5-0a5358f988fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = vectordb.similarity_search(\n",
    "    question,\n",
    "    k=3,\n",
    "    filter={'source': 'pdf_lists/model_evaluation_selection_ML.pdf'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b463765-40ba-4d62-9f30-96a2e6193570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page': 10, 'page_label': '11', 'source': 'pdf_lists/model_evaluation_selection_ML.pdf'}\n",
      "{'page': 22, 'page_label': '23', 'source': 'pdf_lists/model_evaluation_selection_ML.pdf'}\n",
      "{'page': 15, 'page_label': '16', 'source': 'pdf_lists/model_evaluation_selection_ML.pdf'}\n"
     ]
    }
   ],
   "source": [
    "for d in docs:\n",
    "    print(d.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9762467b-8e04-40d1-a865-200ad65caf18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec71ab2-9780-4c7b-9f75-166a9020304f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "778beede-c004-4e07-adbd-1d9df10dc084",
   "metadata": {},
   "source": [
    "#### Addressing Specificity: working with metadata using self-query retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f304a86f-821b-4c7a-b919-0d6c583c7510",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"source\",\n",
    "        description=\"The source of the chunk is from, should be `pdf_lists/model_evaluation_selection_ML.pdf`\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"page\",\n",
    "        description=\"The page from the PDF\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "22bf2e13-7b8a-478d-aacd-8d6caabab38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.2\n"
     ]
    }
   ],
   "source": [
    "from lark import Lark\n",
    "\n",
    "import lark\n",
    "print(lark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ace47f-d4a6-44b4-a6f0-020abab69f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_content_description = \"Paper PDF\"\n",
    "llm = model\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm,\n",
    "    vectordb,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50046a33-26ad-4dc7-b4ea-0a372030bdce",
   "metadata": {},
   "source": [
    "#### Additional Tricks : compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dd298048-4be6-48ef-a68a-03ee67cbb6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9b930341-bc86-4f17-84f1-a9d9154982b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_docs(docs):\n",
    "    print(f\"\\n{'-' * 100}\\n\".join([f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "239607fb-34ec-4ecd-a971-c4a2ade450bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap our vectorstore\n",
    "compressor = LLMChainExtractor.from_llm(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c1db3215-6e2a-4025-b9c7-f837f71d9e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vectordb.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a74fdba1-3f7d-4c37-97c7-f9225ee1ba50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rizdi\\AppData\\Local\\Temp\\ipykernel_2772\\2147165383.py:2: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  compressed_docs = compression_retriever.get_relevant_documents(question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "The extracted relevant part of the context is:\n",
      "\n",
      "\"We will start by discussing techniques for estimating the uncertainty of our estimated model performance as well as the model’s variance and stability. And after getting these basics under our belt, we will look at cross-validation techniques for model selection in the next article in this series.\"\n",
      "\n",
      "However, upon closer inspection, this part doesn't seem to directly address the question about decreasing the size of the test set.\n",
      "\n",
      "A more relevant extracted part is:\n",
      "\n",
      "\"Also, we brieﬂy introduced the normal approximation, which requires us to make certain assumptions that allow us to compute conﬁdence intervals for modeling the uncertainty of our performance estimate based on a single test set, which we have to take with a grain of salt.\"\n",
      "\n",
      "This part does mention the use of a single test set, but it doesn't specifically discuss decreasing its size.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "Decreasing the size of the test set will result in higher pessimistic bias and variance - the sensitivity of a model towards the data is partitioned.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "NO_OUTPUT.\n"
     ]
    }
   ],
   "source": [
    "question = \"what is the drawback of decreasing the size of the test set?\"\n",
    "compressed_docs = compression_retriever.get_relevant_documents(question)\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2627247-7030-4bba-93da-b5d31e18dd5b",
   "metadata": {},
   "source": [
    "#### Other Retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "510855d2-fe00-48c2-bd24-e089b18aff90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import SVMRetriever\n",
    "from langchain.retrievers import TFIDFRetriever\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9908ad2a-271a-4968-8fc7-70a9213c3253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PDF\n",
    "loader = PyPDFLoader(\"pdf_lists/model_evaluation_selection_ML.pdf\")\n",
    "pages = loader.load()\n",
    "all_page_text=[p.page_content for p in pages]\n",
    "joined_page_text=\" \".join(all_page_text)\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1500,chunk_overlap = 150)\n",
    "splits = text_splitter.split_text(joined_page_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4fdf8f85-005f-42bc-b88d-79ef0814a60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve\n",
    "svm_retriever = SVMRetriever.from_texts(splits,embedding)\n",
    "tfidf_retriever = TFIDFRetriever.from_texts(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ef4a88a2-3f03-4588-8af6-b16685a934d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={}, page_content='performances (samples may overlap between these training subsets). Looking at the plot above, we\\ncan see two distinct trends. First, the resubstitution accuracy (training set) declines as the number of\\ntraining samples grows. Second, we observe an improving generalization accuracy (test set) with\\nan increasing training set size. These trends can likely be attributed to a reduction in overﬁtting. If\\n4https://github.com/rasbt/model-eval-article-supplementary/blob/master/code/resampling-and-kfold.ipynb\\n5http://yann.lecun.com/exdb/mnist\\n12 Low Variance\\n(Precise)\\nHigh Variance\\n(Not Precise)\\nLow Bias\\n(Accurate)\\nHigh Bias\\n(Not Accurate)\\nFigure 3: Illustration of bias and variance.\\nFigure 4: Learning curves of softmax classiﬁers ﬁt to MNIST.\\nthe training set is small, the algorithm is more likely picking up noise in the training set so that the\\nmodel fails to generalize well to data that it has not seen before. This observation also explains the\\npessimistic bias of the holdout method: A training algorithm may beneﬁt from more training data,\\ndata that was withheld for testing. Thus, after we evaluated a model, we may want to run the learning\\nalgorithm once again on the complete dataset before we use it in a real-world application.\\nNow, that we established the point of pessimistic biases for disproportionally large test sets, we may\\nask whether it is a good idea to decrease the size of the test set. Decreasing the size of the test set')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"what is the drawback of decreasing the size of the test set?\"\n",
    "docs_svm=svm_retriever.get_relevant_documents(question)\n",
    "docs_svm[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4397aaf0-bf96-4d7c-bc1b-5c245619512c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={}, page_content='ask whether it is a good idea to decrease the size of the test set. Decreasing the size of the test set\\nbrings up another problem: It may result in a substantial variance of a model’s performance estimate.\\nThe reason is that it depends on which instances end up in training set, and which particular instances\\n13 end up in test set. Keeping in mind that each time we resample a dataset, we alter the statistics of the\\ndistribution of the sample. Most supervised learning algorithms for classiﬁcation and regression as\\nwell as the performance estimates operate under the assumption that a dataset is representative of the\\npopulation that this dataset sample has been drawn from. As discussed in Section 1.4, stratiﬁcation\\nhelps with keeping the sample proportions intact upon splitting a dataset. However, the change in the\\nunderlying sample statistics along the features axes is still a problem that becomes more pronounced\\nif we work with small datasets, which is illustrated in Figure 5.\\nDataset \\nDistributionSample 1Sample 2Sample 3\\nTrain\\n(70%)\\nTest \\n(30%)\\nTrain\\n(70%)\\nTest \\n(30%)\\nn=1000n=100\\nReal World \\nDistribution\\nResampling\\nFigure 5: Repeated subsampling from a two-dimensional Gaussian distribution.\\n14 2.3 Repeated Holdout Validation\\nOne way to obtain a more robust performance estimate that is less variant to how we split the data\\ninto training and test sets is to repeat the holdout method ktimes with different random seeds and\\ncompute the average performance over these krepetitions:')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_tfidf=tfidf_retriever.get_relevant_documents(question)\n",
    "docs_tfidf[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35992be-94b6-4f50-a62e-2cc667f285d1",
   "metadata": {},
   "source": [
    "#### Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4826e4ba-634a-45dd-8797-f6a3728c5662",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3dfc7ebc-26e7-4750-888b-5c4dd19a7612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page': 10, 'page_label': '11', 'source': 'pdf_lists/model_evaluation_selection_ML.pdf'}\n",
      "{'page': 22, 'page_label': '23', 'source': 'pdf_lists/model_evaluation_selection_ML.pdf'}\n",
      "{'page': 15, 'page_label': '16', 'source': 'pdf_lists/model_evaluation_selection_ML.pdf'}\n"
     ]
    }
   ],
   "source": [
    "for d in docs:\n",
    "    print(d.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3bee476c-4644-4be5-9d74-c0a96092d430",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f60d922f-a36a-49b3-bff9-609d792dccc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rizdi\\AppData\\Local\\Temp\\ipykernel_2772\\4094420968.py:1: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa_chain({\"query\": question})\n"
     ]
    }
   ],
   "source": [
    "result = qa_chain({\"query\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6cd2ae70-fb4f-4b13-8915-a909161a92d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to the text, the drawback of decreasing the size of the test set is that it can lead to a higher pessimistic bias and increased variance in the model. The text states: \"the smaller the dataset, the higher the pessimistic bias and the variance – the sensitivity of a model towards the data is partitioned.\"'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723a1ca4-3a61-4db5-bf0a-ac9043f289df",
   "metadata": {},
   "source": [
    "#### With Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "60a8fcc8-38bc-44df-a27e-ebb1462d0178",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Build prompt\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0e14c45a-b0e5-427a-a8d5-3ebca5472ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "229326a8-ba99-48a8-b33f-5b8a85563da0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know, but it appears that the text discusses different methods for evaluating and comparing machine learning algorithms, including resampling techniques such as the three-way holdout method and McNemar's test. \\n\\nIt also mentions the importance of considering the sample size and potential biases in model performance. The author seems to be discussing how to estimate generalization accuracy, increase predictive performance, and identify the best-suited algorithm for a problem.\\n\\nThanks for asking!\""
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What are some main highlights of resampling that the author covered?\"\n",
    "\n",
    "result = qa_chain({\"query\": question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "34c03729-95eb-489c-be3b-ce4cc495f60d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'page': 3, 'page_label': '4', 'source': 'pdf_lists/model_evaluation_selection_ML.pdf'}, page_content='More often than not, we want to compare different algorithms to each other, oftentimes in terms of\\npredictive and computational performance. Let us summarize the main points why we evaluate the\\npredictive performance of a model:\\n1. We want to estimate the generalization performance, the predictive performance of our\\nmodel on future (unseen) data.\\n2. We want to increase the predictive performance by tweaking the learning algorithm and\\nselecting the best performing model from a given hypothesis space.\\n3. We want to identify the machine learning algorithm that is best-suited for the problem at\\nhand; thus, we want to compare different algorithms, selecting the best-performing one as\\nwell as the best performing model from the algorithm’s hypothesis space.\\nAlthough these three sub-tasks listed above have all in common that we want to estimate the\\nperformance of a model, they all require different approaches. We will discuss some of the different\\nmethods for tackling these sub-tasks in this article.\\nOf course, we want to estimate the future performance of a model as accurately as possible. However,\\nwe shall note that biased performance estimates are perfectly okay in model selection and algorithm\\nselection if the bias affects all models equally. If we rank different models or algorithms against each\\nother in order to select the best-performing one, we only need to know their \"relative\" performance.')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"source_documents\"][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cd3463-aafd-4fb4-9d8d-ef1c43fb8b58",
   "metadata": {},
   "source": [
    "#### Retrieval QA with Map-Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "84411c6c-412f-41b6-996a-84e962cdeb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### With mapreduce method\n",
    "qa_chain_mr = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    chain_type=\"map_reduce\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5705d55c-068b-461e-a189-3e16d6fd2982",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\github_repos\\OpenAI\\ollama_project\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1651 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Based on the provided content, I don't know any information about resampling or its main highlights from the text.\""
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = qa_chain_mr({\"query\": question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "57d60c02-2fa7-4f5c-bed7-f131161d25b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### With refine method\n",
    "qa_chain_mr = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    chain_type=\"refine\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8ea15d58-77bd-416e-8dfd-fd98462dfad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Based on the provided context about machine learning model selection, resampling, and hyperparameter tuning, I can refine the original answer.\\n\\nThe author of the original response did not provide any relevant information on resampling in the context of machine learning or model selection. However, considering the new context:\\n\\nResampling techniques are widely used in machine learning to evaluate the performance of models and algorithms. Some common resampling techniques include:\\n\\n1. **K-Fold Cross-Validation**: This technique involves dividing the available data into k folds, training a model on k-1 folds, and evaluating its performance on the remaining fold. The process is repeated k times, with each fold serving as the test set once.\\n2. **Leave-One-Out Cross-Validation (LOOCV)**: Similar to K-Fold CV, but instead of dividing the data into k folds, only one data point is left out at a time for testing, while the remaining points are used for training and validation.\\n3. **Stratified Sampling**: This technique involves sampling the data in a way that preserves the same proportion of each class (if applicable) as seen in the original data.\\n\\nThese resampling techniques can help estimate the generalization performance of a model, identify hyperparameter tuning opportunities, and select the best-performing model from a given hypothesis space.\\n\\nIn addition to these methods, the author also introduced McNemar's test, which is a statistical test used to compare the performance of two models with paired data. The test can help determine whether one model performs significantly better than another by comparing their confusion matrices. This is particularly useful when dealing with paired data, such as in this example where we are comparing Model 1 and Model 2.\\n\\nThe main highlights of resampling that the author covered include:\\n\\n* K-Fold Cross-Validation: a widely used technique for evaluating model performance\\n* Leave-One-Out Cross-Validation (LOOCV): a method for testing model performance on individual data points\\n* Stratified Sampling: a technique for preserving class proportions in sampling\\n* McNemar's test: a statistical test for comparing the performance of two models with paired data\""
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = qa_chain_mr({\"query\": question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff07d8d5-5579-467c-bfb7-2db9cea72918",
   "metadata": {},
   "source": [
    "#### Chatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8e90d3d0-4cda-4044-bcbb-e7239f62c24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9b92c03c-054a-41a2-b673-a742f72e7b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How repeated holdout validation provides a better estimate of model's performance compared to a single train/test split method?\"\n",
    "result = qa({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "511057db-9830-4e85-b89b-01e7fffb42d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know how repeated holdout validation provides a better estimate of a model's performance compared to a single train/test split method. The text does not provide an explanation or analysis of this comparison, and I couldn't find any information that directly addresses this question in the provided context.\""
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "03003238-f63f-49a8-9d21-ef9896cc252c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_db(file, chain_type, k):\n",
    "    # load documents\n",
    "    loader = PyPDFLoader(file)\n",
    "    documents = loader.load()\n",
    "    # split documents\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "    # define embedding\n",
    "    # create vector database from data\n",
    "    db = DocArrayInMemorySearch.from_documents(docs, embeddings)\n",
    "    # define retriever\n",
    "    retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": k})\n",
    "    # create a chatbot chain. Memory is managed externally.\n",
    "    qa = ConversationalRetrievalChain.from_llm(\n",
    "        llm=model, \n",
    "        chain_type=chain_type, \n",
    "        retriever=retriever, \n",
    "        return_source_documents=True,\n",
    "        return_generated_question=True,\n",
    "    )\n",
    "    return qa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1afb8978-3533-475f-9cf9-6c4c952bfa39",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'panel'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[79], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpanel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpn\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mparam\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcbfs\u001b[39;00m(param\u001b[38;5;241m.\u001b[39mParameterized):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'panel'"
     ]
    }
   ],
   "source": [
    "import panel as pn\n",
    "import param\n",
    "\n",
    "class cbfs(param.Parameterized):\n",
    "    chat_history = param.List([])\n",
    "    answer = param.String(\"\")\n",
    "    db_query  = param.String(\"\")\n",
    "    db_response = param.List([])\n",
    "    \n",
    "    def __init__(self,  **params):\n",
    "        super(cbfs, self).__init__( **params)\n",
    "        self.panels = []\n",
    "        self.loaded_file = \"pdf_lists/model_evaluation_selection_ML.pdf\"\n",
    "        self.qa = load_db(self.loaded_file,\"stuff\", 4)\n",
    "    \n",
    "    def call_load_db(self, count):\n",
    "        if count == 0 or file_input.value is None:  # init or no file specified :\n",
    "            return pn.pane.Markdown(f\"Loaded File: {self.loaded_file}\")\n",
    "        else:\n",
    "            file_input.save(\"temp.pdf\")  # local copy\n",
    "            self.loaded_file = file_input.filename\n",
    "            button_load.button_style=\"outline\"\n",
    "            self.qa = load_db(\"temp.pdf\", \"stuff\", 4)\n",
    "            button_load.button_style=\"solid\"\n",
    "        self.clr_history()\n",
    "        return pn.pane.Markdown(f\"Loaded File: {self.loaded_file}\")\n",
    "\n",
    "    def convchain(self, query):\n",
    "        if not query:\n",
    "            return pn.WidgetBox(pn.Row('User:', pn.pane.Markdown(\"\", width=600)), scroll=True)\n",
    "        result = self.qa({\"question\": query, \"chat_history\": self.chat_history})\n",
    "        self.chat_history.extend([(query, result[\"answer\"])])\n",
    "        self.db_query = result[\"generated_question\"]\n",
    "        self.db_response = result[\"source_documents\"]\n",
    "        self.answer = result['answer'] \n",
    "        self.panels.extend([\n",
    "            pn.Row('User:', pn.pane.Markdown(query, width=600)),\n",
    "            pn.Row('ChatBot:', pn.pane.Markdown(self.answer, width=600, style={'background-color': '#F6F6F6'}))\n",
    "        ])\n",
    "        inp.value = ''  #clears loading indicator when cleared\n",
    "        return pn.WidgetBox(*self.panels,scroll=True)\n",
    "\n",
    "    @param.depends('db_query ', )\n",
    "    def get_lquest(self):\n",
    "        if not self.db_query :\n",
    "            return pn.Column(\n",
    "                pn.Row(pn.pane.Markdown(f\"Last question to DB:\", styles={'background-color': '#F6F6F6'})),\n",
    "                pn.Row(pn.pane.Str(\"no DB accesses so far\"))\n",
    "            )\n",
    "        return pn.Column(\n",
    "            pn.Row(pn.pane.Markdown(f\"DB query:\", styles={'background-color': '#F6F6F6'})),\n",
    "            pn.pane.Str(self.db_query )\n",
    "        )\n",
    "\n",
    "    @param.depends('db_response', )\n",
    "    def get_sources(self):\n",
    "        if not self.db_response:\n",
    "            return \n",
    "        rlist=[pn.Row(pn.pane.Markdown(f\"Result of DB lookup:\", styles={'background-color': '#F6F6F6'}))]\n",
    "        for doc in self.db_response:\n",
    "            rlist.append(pn.Row(pn.pane.Str(doc)))\n",
    "        return pn.WidgetBox(*rlist, width=600, scroll=True)\n",
    "\n",
    "    @param.depends('convchain', 'clr_history') \n",
    "    def get_chats(self):\n",
    "        if not self.chat_history:\n",
    "            return pn.WidgetBox(pn.Row(pn.pane.Str(\"No History Yet\")), width=600, scroll=True)\n",
    "        rlist=[pn.Row(pn.pane.Markdown(f\"Current Chat History variable\", styles={'background-color': '#F6F6F6'}))]\n",
    "        for exchange in self.chat_history:\n",
    "            rlist.append(pn.Row(pn.pane.Str(exchange)))\n",
    "        return pn.WidgetBox(*rlist, width=600, scroll=True)\n",
    "\n",
    "    def clr_history(self,count=0):\n",
    "        self.chat_history = []\n",
    "        return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0f33ac-1b1e-46e6-9b03-fe478f3e1442",
   "metadata": {},
   "outputs": [],
   "source": [
    "cb = cbfs()\n",
    "\n",
    "file_input = pn.widgets.FileInput(accept='.pdf')\n",
    "button_load = pn.widgets.Button(name=\"Load DB\", button_type='primary')\n",
    "button_clearhistory = pn.widgets.Button(name=\"Clear History\", button_type='warning')\n",
    "button_clearhistory.on_click(cb.clr_history)\n",
    "inp = pn.widgets.TextInput( placeholder='Enter text here…')\n",
    "\n",
    "bound_button_load = pn.bind(cb.call_load_db, button_load.param.clicks)\n",
    "conversation = pn.bind(cb.convchain, inp) \n",
    "\n",
    "jpg_pane = pn.pane.Image( './img/convchain.jpg')\n",
    "\n",
    "tab1 = pn.Column(\n",
    "    pn.Row(inp),\n",
    "    pn.layout.Divider(),\n",
    "    pn.panel(conversation,  loading_indicator=True, height=300),\n",
    "    pn.layout.Divider(),\n",
    ")\n",
    "tab2= pn.Column(\n",
    "    pn.panel(cb.get_lquest),\n",
    "    pn.layout.Divider(),\n",
    "    pn.panel(cb.get_sources ),\n",
    ")\n",
    "tab3= pn.Column(\n",
    "    pn.panel(cb.get_chats),\n",
    "    pn.layout.Divider(),\n",
    ")\n",
    "tab4=pn.Column(\n",
    "    pn.Row( file_input, button_load, bound_button_load),\n",
    "    pn.Row( button_clearhistory, pn.pane.Markdown(\"Clears chat history. Can use to start a new topic\" )),\n",
    "    pn.layout.Divider(),\n",
    "    pn.Row(jpg_pane.clone(width=400))\n",
    ")\n",
    "dashboard = pn.Column(\n",
    "    pn.Row(pn.pane.Markdown('# ChatWithYourData_Bot')),\n",
    "    pn.Tabs(('Conversation', tab1), ('Database', tab2), ('Chat History', tab3),('Configure', tab4))\n",
    ")\n",
    "dashboard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama_project",
   "language": "python",
   "name": "ollama_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
